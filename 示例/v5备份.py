"""
GitHub é¡¹ç›®æ·±åº¦åˆ†æå™¨ v4.5 - ç®—æ³•å‡çº§ç‰ˆ
==========================================
æ ¸å¿ƒæ”¹è¿›ï¼š
1. GMM æ¦‚ç‡åŒ–åˆ†å±‚åˆ†ç±»å™¨
2. Prophet æ—¶åºé¢„æµ‹ + å˜ç‚¹æ£€æµ‹
3. AHP åŠ¨æ€æƒé‡å¥åº·è¯„ä¼°
4. å›æµ‹æœºåˆ¶éªŒè¯é¢„æµ‹å‡†ç¡®æ€§
"""

import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import Circle
import seaborn as sns
import re
import os
import warnings
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional, Any
from scipy.stats import pearsonr, linregress
from scipy.optimize import curve_fit
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
import json
from datetime import datetime, timedelta
import itertools
from collections import defaultdict

warnings.filterwarnings('ignore')

# ============== æ˜¾ç¤ºè®¾ç½® ==============
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.facecolor'] = 'white'
plt.rcParams['axes.labelsize'] = 9
plt.rcParams['axes.titlesize'] = 10
plt.rcParams['xtick.labelsize'] = 8
plt.rcParams['ytick.labelsize'] = 8

# ============== é¢œè‰²ä¸»é¢˜ ==============
COLORS = {
    'primary': '#2E86AB',
    'success': '#28A745',
    'warning': '#FFC107',
    'danger': '#DC3545',
    'info': '#17A2B8',
    'secondary': '#6C757D',
    'purple': '#9B59B6',
    'orange': '#E67E22',
}

# ============== å±‚çº§åŸºå‡†æ•°æ® ==============
TIER_BENCHMARKS = {
    'GIANT': {'openrank': 80, 'stars': 50000, 'participants': 500, 'color': '#9B59B6'},
    'MATURE': {'openrank': 25, 'stars': 5000, 'participants': 100, 'color': '#3498DB'},
    'GROWING': {'openrank': 8, 'stars': 1000, 'participants': 30, 'color': '#2ECC71'},
    'EMERGING': {'openrank': 2, 'stars': 100, 'participants': 5, 'color': '#E67E22'}
}

TIER_NAMES = {
    'GIANT': 'å·¨å‹é¡¹ç›®',
    'MATURE': 'æˆç†Ÿé¡¹ç›®', 
    'GROWING': 'æˆé•¿é¡¹ç›®',
    'EMERGING': 'æ–°å…´é¡¹ç›®'
}

# ============== æ•°æ®ç±»å®šä¹‰ ==============
@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    project_name: str
    tier: str
    tier_probabilities: Dict[str, float]  # æ”¹ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    tier_confidence: float
    lifecycle: str
    vitality: str
    health_score: float
    health_grade: str
    dimension_scores: Dict[str, float]
    momentum_analysis: Dict
    resistance_analysis: Dict
    potential_analysis: Dict
    trend_3d: Dict
    risk_analysis: Dict
    dark_horse_analysis: Dict
    change_points: List[Dict]
    bus_factor_2: Dict
    etd_analysis: Dict
    github_comparison: Dict
    conclusion_validation: Dict
    prediction_validation: Dict
    # æ–°å¢é¢„æµ‹ç»“æœ
    predictions: Dict
    # æ–°å¢å›æµ‹ç»“æœ
    backtest_results: Dict
    recommendations: List[str]
    detailed_report: str


# ============== GMMæ¦‚ç‡åŒ–åˆ†å±‚åˆ†ç±»å™¨ ==============
class GMMTierClassifier:
    """é«˜æ–¯æ··åˆæ¨¡å‹åˆ†å±‚åˆ†ç±»å™¨"""
    
    def __init__(self, n_components=4):
        self.n_components = n_components
        self.gmm = None
        self.scaler = StandardScaler()
        self.tier_labels = ['GIANT', 'MATURE', 'GROWING', 'EMERGING']
        
    def _generate_synthetic_data(self) -> np.ndarray:
        """ç”ŸæˆåˆæˆåŸºå‡†æ•°æ®ç”¨äºè®­ç»ƒGMM"""
        synthetic_data = []
        
        # ä¸ºæ¯ä¸ªå±‚çº§ç”Ÿæˆæ ·æœ¬
        for tier, benchmarks in TIER_BENCHMARKS.items():
            n_samples = 100  # æ¯ä¸ªå±‚çº§100ä¸ªæ ·æœ¬
            
            # å›´ç»•åŸºå‡†å€¼ç”Ÿæˆæ­£æ€åˆ†å¸ƒæ ·æœ¬
            for _ in range(n_samples):
                openrank = np.random.normal(benchmarks['openrank'], max(benchmarks['openrank'] * 0.3, 1))
                stars = np.random.normal(benchmarks['stars'], max(benchmarks['stars'] * 0.3, 100))
                participants = np.random.normal(benchmarks['participants'], max(benchmarks['participants'] * 0.3, 5))
                
                synthetic_data.append([
                    max(0.1, openrank),
                    max(10, stars),
                    max(1, participants)
                ])
        
        return np.array(synthetic_data)
    
    def fit(self):
        """è®­ç»ƒGMMæ¨¡å‹"""
        synthetic_data = self._generate_synthetic_data()
        scaled_data = self.scaler.fit_transform(synthetic_data)
        self.gmm = GaussianMixture(
            n_components=self.n_components,
            covariance_type='full',
            random_state=42
        )
        self.gmm.fit(scaled_data)
        return self
    
    def predict_proba(self, metrics: Dict) -> Tuple[str, Dict[str, float], float]:
        """é¢„æµ‹å±‚çº§æ¦‚ç‡"""
        if self.gmm is None:
            self.fit()
        
        feature = np.array([
            metrics.get('avg_openrank', 0),
            metrics.get('total_stars', 0),
            metrics.get('max_participants', 0)
        ]).reshape(1, -1)
        
        # å¤„ç†é›¶å€¼
        feature = np.maximum(feature, [0.1, 10, 1])
        
        scaled_feature = self.scaler.transform(feature)
        probabilities = self.gmm.predict_proba(scaled_feature)[0]
        
        # å°†GMMç»„ä»¶æ˜ å°„åˆ°å±‚çº§æ ‡ç­¾ï¼ˆæŒ‰ä¸­å¿ƒç‚¹æ’åºï¼‰
        # è®¡ç®—æ¯ä¸ªç»„ä»¶çš„ä¸­å¿ƒç‚¹ï¼ˆä¿®å¤ç»´åº¦é—®é¢˜ï¼‰
        centers = self.gmm.means_  # (4, 3)
        # åæ ‡å‡†åŒ–ä¸­å¿ƒç‚¹
        centers_original = centers * self.scaler.scale_ + self.scaler.mean_
        openrank_centers = centers_original[:, 0]
        
        # æŒ‰openrankä»å¤§åˆ°å°æ’åº
        sorted_indices = np.argsort(-openrank_centers)
        
        # åˆ†é…æ ‡ç­¾
        tier_probabilities = {}
        for idx, tier in enumerate(self.tier_labels):
            if idx < len(sorted_indices):
                comp_idx = sorted_indices[idx]
                tier_probabilities[tier] = probabilities[comp_idx]
            else:
                tier_probabilities[tier] = 0.0
        
        # ç¡®å®šä¸»è¦å±‚çº§
        best_tier = max(tier_probabilities, key=tier_probabilities.get)
        confidence = tier_probabilities[best_tier]
        
        return best_tier, tier_probabilities, confidence


# ============== åŠ¨é‡è®¡ç®—å™¨ ==============
class MomentumCalculator:
    """ç¤¾åŒºåŠ¨é‡è®¡ç®—å™¨"""
    
    def calculate(self, data: pd.DataFrame) -> Dict:
        """è®¡ç®—ç»¼åˆåŠ¨é‡"""
        quality = self._calc_quality_momentum(data)
        gravity = self._calc_contributor_gravity(data)
        pr_accel = self._calc_pr_acceleration(data)
        
        total = 0.4 * quality + 0.35 * gravity + 0.25 * pr_accel
        
        if total >= 70:
            interpretation = 'å¼ºåŠ²åŠ¨é‡ - é¡¹ç›®æ­£åœ¨å¥åº·æ‰©å¼ '
        elif total >= 50:
            interpretation = 'ç¨³å®šåŠ¨é‡ - ä¿æŒæ­£å¸¸å‘å±•'
        elif total >= 30:
            interpretation = 'å¼±åŠ¨é‡ - å‘å±•åŠ¨åŠ›ä¸è¶³'
        else:
            interpretation = 'è´ŸåŠ¨é‡ - éœ€è¦å¹²é¢„'
        
        return {
            'total': round(total, 2),
            'quality': round(quality, 2),
            'gravity': round(gravity, 2),
            'pr_accel': round(pr_accel, 2),
            'interpretation': interpretation
        }
    
    def _calc_quality_momentum(self, data: pd.DataFrame) -> float:
        if 'pr_merged' not in data or 'pr_new' not in data:
            return 50
        merge_rate = data['pr_merged'] / (data['pr_new'] + 0.1)
        if len(merge_rate) < 6:
            return 50
        trend = np.polyfit(range(len(merge_rate.tail(6))), merge_rate.tail(6).values, 1)[0]
        return min(100, max(0, 50 + trend * 100))
    
    def _calc_contributor_gravity(self, data: pd.DataFrame) -> float:
        if 'participants' not in data:
            return 50
        growth = data['participants'].diff().tail(6).mean()
        inactive = data.get('inactive_contributors', pd.Series([0]))
        retention = 1 - (inactive.tail(6).mean() / (data['participants'].tail(6).mean() + 1))
        return min(100, max(0, 50 + growth * 10 + retention * 30))
    
    def _calc_pr_acceleration(self, data: pd.DataFrame) -> float:
        if 'pr_merged' not in data:
            return 50
        pr = data['pr_merged'].tail(6)
        if len(pr) < 3:
            return 50
        accel = pr.diff().diff().mean()
        return min(100, max(0, 50 + accel * 20))


# ============== é˜»åŠ›è®¡ç®—å™¨ ==============
class ResistanceCalculator:
    """æŠ€æœ¯å€ºé˜»åŠ›è®¡ç®—å™¨"""
    
    def calculate(self, data: pd.DataFrame) -> Dict:
        debt = self._calc_debt_resistance(data)
        entropy = self._calc_entropy_trend(data)
        issue_pressure = self._calc_issue_pressure(data)
        
        total = 0.4 * debt + 0.3 * entropy + 0.3 * issue_pressure
        
        if total >= 70:
            status, interpretation = 'HEAVY', 'é«˜é˜»åŠ› - æŠ€æœ¯å€ºä¸¥é‡'
        elif total >= 50:
            status, interpretation = 'MEDIUM_HIGH', 'ä¸­é«˜é˜»åŠ› - éœ€å…³æ³¨'
        elif total >= 30:
            status, interpretation = 'NORMAL', 'ä¸­ç­‰é˜»åŠ› - æ­£å¸¸èŒƒå›´'
        else:
            status, interpretation = 'LIGHT', 'ä½é˜»åŠ› - å‘å±•é¡ºç•…'
        
        return {
            'total': round(total, 2),
            'debt': round(debt, 2),
            'entropy': round(entropy, 2),
            'issue_pressure': round(issue_pressure, 2),
            'status': status,
            'interpretation': interpretation
        }
    
    def _calc_debt_resistance(self, data: pd.DataFrame) -> float:
        if 'issues_new' not in data or 'issues_closed' not in data:
            return 30
        open_issues = data['issues_new'].cumsum() - data['issues_closed'].cumsum()
        if len(open_issues) < 6:
            return 30
        growth = np.polyfit(range(len(open_issues.tail(6))), open_issues.tail(6).values, 1)[0]
        return min(100, max(0, 30 + growth * 5))
    
    def _calc_entropy_trend(self, data: pd.DataFrame) -> float:
        if 'activity' not in data:
            return 30
        volatility = data['activity'].tail(12).std() / (data['activity'].tail(12).mean() + 0.1)
        return min(100, volatility * 50)
    
    def _calc_issue_pressure(self, data: pd.DataFrame) -> float:
        if 'issues_new' not in data:
            return 30
        recent = data['issues_new'].tail(6).mean()
        historical = data['issues_new'].mean()
        ratio = recent / (historical + 0.1)
        return min(100, max(0, ratio * 40))


# ============== æ½œåŠ›è®¡ç®—å™¨ ==============
class PotentialCalculator:
    """å¢é•¿æ½œåŠ›è®¡ç®—å™¨"""
    
    def calculate(self, data: pd.DataFrame, tier: str) -> Dict:
        """è®¡ç®—æ›´æ‚²è§‚çš„å¢é•¿æ½œåŠ›"""
        ceiling = self._estimate_ceiling_pessimistic(data)
        current = data['openrank'].iloc[-1] if 'openrank' in data else 0
        
        # å¦‚æœå½“å‰å€¼ä¸º0æˆ–å¾ˆå°ï¼Œä½¿ç”¨æœ€è¿‘6ä¸ªæœˆçš„å¹³å‡å€¼
        if current < 1 and 'openrank' in data:
            current = max(0.1, data['openrank'].tail(6).mean())
        
        # è®¡ç®—å‰©ä½™ç©ºé—´ï¼ˆæ›´ä¿å®ˆï¼‰
        if ceiling > current:
            remaining = ((ceiling - current) / (ceiling + 0.1)) * 100
        else:
            remaining = 0
        
        # æ ¹æ®å±‚çº§è°ƒæ•´æ½œåŠ›è¯„åˆ†
        tier_adjustment = {'GIANT': 0.2, 'MATURE': 0.3, 'GROWING': 0.6, 'EMERGING': 0.8}
        remaining *= tier_adjustment.get(tier, 0.5)
        
        return {
            'growth_ceiling': round(ceiling, 1),
            'current_position': round(current, 2),
            'remaining_space': min(100, max(0, remaining)),  # é™åˆ¶åœ¨0-100ä¹‹é—´
            'interpretation': self._interpret(remaining)
        }
    
    def _estimate_ceiling_pessimistic(self, data: pd.DataFrame) -> float:
        """æ›´æ‚²è§‚çš„å¢é•¿ä¸Šé™ä¼°è®¡"""
        if 'openrank' not in data or len(data) < 6:
            return 10  # é»˜è®¤è¾ƒä½çš„ä¸Šé™
        
        openrank = data['openrank'].values
        
        # æ–¹æ³•1ï¼šä½¿ç”¨å†å²æœ€é«˜å€¼çš„1.5å€ï¼Œä½†ä¸è¶…è¿‡åˆ†ç±»åŸºå‡†
        historical_max = openrank.max()
        if historical_max == 0:
            return 10
        
        # æ–¹æ³•2ï¼šä½¿ç”¨æœ€è¿‘12ä¸ªæœˆçš„çº¿æ€§è¶‹åŠ¿å¤–æ¨
        if len(openrank) >= 12:
            recent = openrank[-12:]
            x = np.arange(len(recent))
            slope, intercept = np.polyfit(x, recent, 1)
            future_6m = intercept + slope * (len(recent) + 6)
            
            # å–ä¸¤ç§æ–¹æ³•çš„æœ€å°å€¼ï¼Œæ›´æ‚²è§‚
            trend_based = max(historical_max * 1.2, future_6m)
            ceiling = min(historical_max * 2.0, trend_based)
        else:
            ceiling = historical_max * 1.5
        
        # ç¡®ä¿ä¸Šé™ä¸å¤ªé«˜
        return min(100, max(10, ceiling))

    def _interpret(self, remaining: float) -> str:
        if remaining >= 70:
            return 'é«˜å¢é•¿æ½œåŠ› - è¿œæœªè§¦åŠå¤©èŠ±æ¿'
        elif remaining >= 40:
            return 'ä¸­ç­‰æ½œåŠ› - ä»æœ‰æˆé•¿ç©ºé—´'
        elif remaining >= 20:
            return 'æœ‰é™æ½œåŠ› - æ¥è¿‘æˆç†Ÿ'
        else:
            return 'å·²è¾¾æˆç†Ÿ - è¿›å…¥ç¨³å®šæœŸ'


# ============== Prophetæ—¶åºé¢„æµ‹å™¨ ==============
class ProphetPredictor:
    """Prophetæ—¶åºé¢„æµ‹å™¨ï¼ˆç®€åŒ–ç‰ˆå®ç°ï¼‰"""
    
    def __init__(self):
        self.fitted = False
        
    def _linear_regression(self, data: pd.Series, periods: int = 6) -> List[float]:
        """çº¿æ€§å›å½’é¢„æµ‹"""
        x = np.arange(len(data))
        values = data.values
        slope, intercept = np.polyfit(x, values, 1)
        future_x = np.arange(len(data), len(data) + periods)
        return list(np.polyval([slope, intercept], future_x))
    
    def _exponential_regression(self, data: pd.Series, periods: int = 6) -> List[float]:
        """æŒ‡æ•°å›å½’é¢„æµ‹"""
        try:
            # å¤„ç†é›¶å€¼å’Œè´Ÿå€¼
            values = data.values + 0.1
            log_values = np.log(values)
            x = np.arange(len(data))
            slope, intercept = np.polyfit(x, log_values, 1)
            future_x = np.arange(len(data), len(data) + periods)
            return list(np.exp(np.polyval([slope, intercept], future_x)) - 0.1)
        except:
            # å›é€€åˆ°çº¿æ€§å›å½’
            return self._linear_regression(data, periods)
    
    def _weighted_moving_average(self, data: pd.Series, periods: int = 6) -> List[float]:
        """åŠ æƒç§»åŠ¨å¹³å‡é¢„æµ‹ï¼Œè¿‘æœŸæ•°æ®æƒé‡æ›´é«˜"""
        window = min(6, len(data))
        weights = np.arange(1, window + 1)
        weights = weights / weights.sum()
        
        # è®¡ç®—åŠ æƒç§»åŠ¨å¹³å‡å€¼
        recent_data = data.values[-window:]
        weighted_avg = np.sum(recent_data * weights)
        
        # è®¡ç®—è¿‘æœŸè¶‹åŠ¿
        trend = data.values[-1] - data.values[-2] if len(data) >= 2 else 0
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = []
        for i in range(periods):
            predictions.append(weighted_avg + trend * i)
        
        return predictions
    
    def _simulate_prophet_predictions(self, data: pd.Series, periods: int = 6) -> Dict:
        """æ¨¡æ‹ŸPropheté¢„æµ‹ï¼Œä½¿ç”¨å¤šç§æ–¹æ³•ç»„åˆ"""
        if len(data) < 6:
            return {'forecast': None, 'trend': None, 'seasonality': None}
        
        try:
            values = data.values
            
            # 1. ä½¿ç”¨å¤šç§é¢„æµ‹æ–¹æ³•
            methods = {
                'linear': self._linear_regression(data, periods),
                'exponential': self._exponential_regression(data, periods),
                'weighted_ma': self._weighted_moving_average(data, periods)
            }
            
            # 2. ç»„åˆé¢„æµ‹ç»“æœï¼ˆåŠ æƒå¹³å‡ï¼‰
            weights = {'linear': 0.4, 'exponential': 0.3, 'weighted_ma': 0.3}
            combined_forecast = []
            
            for i in range(periods):
                weighted_sum = 0
                for method, preds in methods.items():
                    weighted_sum += preds[i] * weights[method]
                combined_forecast.append(weighted_sum)
            
            # 3. ç¡®ä¿é¢„æµ‹å€¼éè´Ÿ
            combined_forecast = [max(0, pred) for pred in combined_forecast]
            
            # 4. è®¡ç®—è¶‹åŠ¿çº¿
            x = np.arange(len(data))
            trend_slope, trend_intercept = np.polyfit(x, values, 1)
            trend = list(np.polyval([trend_slope, trend_intercept], x))
            
            # 5. ç®€åŒ–çš„å­£èŠ‚æ€§åˆ†è§£
            seasonal_component = [values[i] - trend[i] for i in range(len(values))]
            
            # 6. è®¡ç®—é¢„æµ‹åŒºé—´
            std_dev = np.std(seasonal_component) if len(seasonal_component) > 0 else values.std()
            yhat_lower = [max(0, f - 1.96 * std_dev) for f in combined_forecast]
            yhat_upper = [f + 1.96 * std_dev for f in combined_forecast]
            
            # 7. è®¡ç®—ç½®ä¿¡åº¦
            # åŸºäºå†å²æ•°æ®çš„æ³¢åŠ¨æ€§è®¡ç®—ç½®ä¿¡åº¦
            volatility = std_dev / np.mean(values) if np.mean(values) > 0 else 0
            confidence = max(0.3, min(0.95, 1 - volatility))
            
            return {
                'forecast': combined_forecast,
                'trend': trend,
                'seasonality': seasonal_component,
                'yhat_lower': yhat_lower,
                'yhat_upper': yhat_upper,
                'confidence': confidence,
                'method_contributions': weights
            }
            
        except Exception as e:
            print(f"é¢„æµ‹æ¨¡æ‹Ÿå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•çº¿æ€§å¤–æ¨
            if len(values) >= 3:
                future = self._linear_regression(data, periods)
                return {
                    'forecast': future,
                    'trend': list(values),
                    'seasonality': [0] * len(values),
                    'yhat_lower': [f * 0.8 for f in future],
                    'yhat_upper': [f * 1.2 for f in future],
                    'confidence': 0.7,
                    'method_contributions': {'fallback': 1.0}
                }
            return {'forecast': None, 'trend': None, 'seasonality': None, 'confidence': 0.3}
    
    def predict(self, data: pd.DataFrame, metric: str = 'openrank', periods: int = 6) -> Dict:
        """é¢„æµ‹æŒ‡å®šæŒ‡æ ‡"""
        if metric not in data:
            return {'error': f'æŒ‡æ ‡{metric}ä¸å­˜åœ¨'}
        
        series = data[metric].dropna()
        if len(series) < 6:
            return {'error': 'æ•°æ®ä¸è¶³'}
        
        return self._simulate_prophet_predictions(series, periods)


# ============== AHPæƒé‡è®¡ç®—å™¨ ==============
class AHPHealthEvaluator:
    """AHPå¥åº·è¯„ä¼°å™¨"""
    
    # å±‚çº§ç‰¹å®šçš„æƒé‡çŸ©é˜µ
    TIER_WEIGHTS = {
        'GIANT': {
            'momentum': 0.25,
            'stability': 0.35,
            'potential': 0.15,
            'safety': 0.25
        },
        'MATURE': {
            'momentum': 0.20,
            'stability': 0.40,
            'potential': 0.20,
            'safety': 0.20
        },
        'GROWING': {
            'momentum': 0.35,
            'stability': 0.25,
            'potential': 0.30,
            'safety': 0.10
        },
        'EMERGING': {
            'momentum': 0.30,
            'stability': 0.20,
            'potential': 0.40,
            'safety': 0.10
        }
    }
    
    def calculate_health_score(self, 
                              vitality: str,
                              trend: Dict,
                              risk: Dict,
                              tier: str,
                              predictions: Dict = None) -> Tuple[float, Dict[str, float]]:
        """è®¡ç®—AHPåŠ æƒå¥åº·åˆ†"""
        
        # è·å–å±‚çº§ç‰¹å®šæƒé‡
        weights = self.TIER_WEIGHTS.get(tier, self.TIER_WEIGHTS['MATURE'])
        
        # è®¡ç®—å„ç»´åº¦åŸå§‹åˆ†æ•°
        raw_scores = {
            'momentum': trend['momentum']['total'],
            'stability': 100 - trend['resistance']['total'],
            'potential': trend['potential']['remaining_space'],
            'safety': max(0, 100 - risk['score'] * 2)
        }
        
        # æ´»åŠ›çŠ¶æ€è°ƒæ•´å› å­
        vitality_factors = {
            'THRIVING': 1.2,
            'STABLE': 1.0,
            'DORMANT': 0.8,
            'ZOMBIE': 0.6,
            'UNKNOWN': 0.9
        }
        vitality_factor = vitality_factors.get(vitality, 1.0)
        
        # é¢„æµ‹ç½®ä¿¡åº¦è°ƒæ•´ï¼ˆå¦‚æœæœ‰é¢„æµ‹æ•°æ®ï¼‰
        if predictions and 'confidence' in predictions.get('openrank', {}):
            pred_confidence = predictions['openrank']['confidence']
            prediction_factor = 0.9 + pred_confidence * 0.2
        else:
            prediction_factor = 1.0
        
        # åº”ç”¨æƒé‡å’Œè°ƒæ•´å› å­
        weighted_scores = {}
        total_weighted = 0
        total_weight = 0
        
        for dimension, raw_score in raw_scores.items():
            weight = weights[dimension]
            weighted = raw_score * weight
            weighted_scores[dimension] = weighted
            total_weighted += weighted
            total_weight += weight
        
        # åŸºç¡€å¥åº·åˆ†
        base_score = total_weighted / total_weight if total_weight > 0 else 50
        
        # åº”ç”¨è°ƒæ•´å› å­
        adjusted_score = base_score * vitality_factor * prediction_factor
        
        # é™åˆ¶èŒƒå›´
        final_score = max(0, min(100, adjusted_score))
        
        # è®¡ç®—å„ç»´åº¦è´¡çŒ®ç™¾åˆ†æ¯”
        dimension_contributions = {
            'åŠ¨é‡': weights['momentum'] * raw_scores['momentum'] / 100,
            'ç¨³å®š': weights['stability'] * raw_scores['stability'] / 100,
            'æ½œåŠ›': weights['potential'] * raw_scores['potential'] / 100,
            'å®‰å…¨': weights['safety'] * raw_scores['safety'] / 100
        }
        
        return round(final_score, 1), dimension_contributions


# ============== å›æµ‹éªŒè¯å™¨ ==============
class BacktestValidator:
    """é¢„æµ‹å›æµ‹éªŒè¯å™¨"""
    
    def __init__(self, train_test_split: float = 0.7):
        self.train_test_split = train_test_split
        
    def validate_predictions(self, data: pd.DataFrame, metric: str = 'openrank') -> Dict:
        """ä½¿ç”¨å†å²æ•°æ®è¿›è¡Œå›æµ‹éªŒè¯"""
        if metric not in data or len(data) < 12:
            return {'error': 'æ•°æ®ä¸è¶³'}
        
        series = data[metric].dropna()
        n = len(series)
        split_idx = int(n * self.train_test_split)
        
        if split_idx < 6 or n - split_idx < 3:
            return {'error': 'åˆ†å‰²åæ•°æ®ä¸è¶³'}
        
        # è®­ç»ƒæ•°æ®
        train_data = series.iloc[:split_idx]
        
        # æµ‹è¯•æ•°æ®
        test_data = series.iloc[split_idx:]
        
        # ä½¿ç”¨ç®€åŒ–é¢„æµ‹å™¨è¿›è¡Œé¢„æµ‹
        predictor = ProphetPredictor()
        
        # åœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆå¹¶é¢„æµ‹æµ‹è¯•æœŸé•¿åº¦
        train_df = pd.DataFrame({metric: train_data})
        forecast_result = predictor.predict(train_df, metric, periods=len(test_data))
        
        if 'forecast' not in forecast_result or forecast_result['forecast'] is None:
            return {'error': 'é¢„æµ‹å¤±è´¥'}
        
        # è®¡ç®—é¢„æµ‹å‡†ç¡®æ€§æŒ‡æ ‡
        forecast_values = forecast_result['forecast'][:len(test_data)]
        actual_values = test_data.values
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(forecast_values), len(actual_values))
        forecast_values = forecast_values[:min_len]
        actual_values = actual_values[:min_len]
        
        if min_len < 2:
            return {'error': 'æœ‰æ•ˆæ•°æ®ç‚¹ä¸è¶³'}
        
        # è®¡ç®—è¯¯å·®æŒ‡æ ‡
        mae = np.mean(np.abs(np.array(forecast_values) - actual_values))
        mape = np.mean(np.abs((np.array(forecast_values) - actual_values) / (actual_values + 0.1))) * 100
        rmse = np.sqrt(np.mean((np.array(forecast_values) - actual_values) ** 2))
        
        # è®¡ç®—æ–¹å‘å‡†ç¡®æ€§ï¼ˆé¢„æµ‹æ¶¨è·Œæ˜¯å¦æ­£ç¡®ï¼‰
        direction_correct = 0
        for i in range(1, min_len):
            pred_dir = forecast_values[i] - forecast_values[i-1]
            actual_dir = actual_values[i] - actual_values[i-1]
            if (pred_dir >= 0 and actual_dir >= 0) or (pred_dir < 0 and actual_dir < 0):
                direction_correct += 1
        
        direction_accuracy = direction_correct / (min_len - 1) if min_len > 1 else 0
        
        return {
            'train_samples': len(train_data),
            'test_samples': len(test_data),
            'mae': round(mae, 3),
            'mape': round(mape, 1),
            'rmse': round(rmse, 3),
            'direction_accuracy': round(direction_accuracy, 3),
            'forecast_values': [round(v, 2) for v in forecast_values],
            'actual_values': [round(v, 2) for v in actual_values],
            'confidence_level': self._calculate_confidence(mape, direction_accuracy)
        }
    
    def _calculate_confidence(self, mape: float, direction_accuracy: float) -> str:
        """è®¡ç®—ç½®ä¿¡åº¦ç­‰çº§"""
        if mape < 10 and direction_accuracy > 0.8:
            return 'HIGH'
        elif mape < 20 and direction_accuracy > 0.6:
            return 'MEDIUM'
        elif mape < 30 and direction_accuracy > 0.5:
            return 'LOW'
        else:
            return 'VERY_LOW'


# ============== Bus Factor 2.0 ==============
class BusFactorCalculator:
    """è´¡çŒ®ç†µæ¨¡å‹"""
    
    def calculate(self, data: pd.DataFrame) -> Dict:
        if 'participants' not in data:
            return {'effective_bus_factor': 1, 'risk_level': 'UNKNOWN'}
        
        # æ¨¡æ‹Ÿè´¡çŒ®åˆ†å¸ƒï¼ˆå®é™…åº”ä»APIè·å–ï¼‰
        participants = data['participants'].tail(6).mean()
        if participants <= 0:
            return {'effective_bus_factor': 1, 'risk_level': 'CRITICAL'}
        
        # å‡è®¾è´¡çŒ®æœä»Zipfåˆ†å¸ƒ
        n = int(max(1, participants))
        contributions = np.array([1/(i+1) for i in range(n)])
        contributions = contributions / contributions.sum()
        
        # è®¡ç®—ç†µ
        entropy = -np.sum(contributions * np.log2(contributions + 1e-10))
        max_entropy = np.log2(n) if n > 1 else 1
        normalized = entropy / max_entropy if max_entropy > 0 else 0
        effective_bf = 2 ** entropy
        
        if effective_bf <= 2:
            risk, desc = 'CRITICAL', 'æé«˜é£é™©ï¼šè´¡çŒ®è¿‡äºé›†ä¸­'
        elif effective_bf <= 4:
            risk, desc = 'HIGH', 'é«˜é£é™©ï¼šéœ€åŸ¹å…»æ›´å¤šè´¡çŒ®è€…'
        elif effective_bf <= 8:
            risk, desc = 'MEDIUM', 'ä¸­ç­‰é£é™©ï¼šè´¡çŒ®è€…å¤šæ ·æ€§å°šå¯'
        else:
            risk, desc = 'LOW', 'ä½é£é™©ï¼šè´¡çŒ®è€…ç”Ÿæ€å¥åº·'
        
        return {
            'raw_entropy': round(entropy, 3),
            'normalized_entropy': round(normalized, 3),
            'effective_bus_factor': round(effective_bf, 1),
            'risk_level': risk,
            'description': desc
        }


# ============== å˜ç‚¹æ£€æµ‹ ==============
class ChangePointDetector:
    """å˜ç‚¹æ£€æµ‹å™¨"""
    
    def detect(self, data: pd.Series) -> List[Dict]:
        if len(data) < 12:
            return []
        
        results = []
        # ç®€åŒ–ç‰ˆå˜ç‚¹æ£€æµ‹ï¼ˆçª—å£æ¯”è¾ƒï¼‰
        window = 6
        for i in range(window, len(data) - window):
            before = data.iloc[i-window:i].mean()
            after = data.iloc[i:i+window].mean()
            change_rate = (after - before) / (before + 0.1)
            
            if abs(change_rate) > 0.3:
                if change_rate > 0.3:
                    cp_type, desc = 'ACCELERATION', 'è¿›å…¥å¿«é€Ÿå¢é•¿æœŸ'
                else:
                    cp_type, desc = 'DECELERATION', 'æ´»è·ƒåº¦æ˜¾è‘—ä¸‹é™'
                
                results.append({
                    'index': i,
                    'date': str(data.index[i])[:7] if hasattr(data.index[i], 'strftime') else str(i),
                    'type': cp_type,
                    'change_rate': round(change_rate, 3),
                    'description': desc
                })
        
        return results[:3]  # æœ€å¤šè¿”å›3ä¸ªå˜ç‚¹


# ============== è½¬åŒ–æ¼æ–— ==============
class ConversionFunnelAnalyzer:
    """è½¬åŒ–æ¼æ–—åˆ†æ"""
    
    def analyze(self, data: pd.DataFrame) -> Dict:
        if 'new_contributors' not in data or 'stars' not in data:
            return {'funnel_rate': None, 'quality': 'UNKNOWN'}
        
        new_contrib = data['new_contributors'].tail(6).sum()
        new_stars = data['stars'].tail(6).sum()
        
        rate = new_contrib / (new_stars + 1)
        
        if rate > 0.1:
            quality, desc = 'EXCELLENT', 'ä¼˜è´¨ - Staré«˜æ•ˆè½¬åŒ–'
        elif rate > 0.05:
            quality, desc = 'GOOD', 'è‰¯å¥½ - ç¤¾åŒºå¸å¼•åŠ›å¼º'
        elif rate > 0.02:
            quality, desc = 'NORMAL', 'æ­£å¸¸æ°´å¹³'
        else:
            quality, desc = 'BUBBLE', 'ç–‘ä¼¼æ³¡æ²« - è½¬åŒ–ç‡ä½'
        
        return {
            'funnel_rate': round(rate, 4),
            'quality': quality,
            'description': desc,
            'new_contributors': int(new_contrib),
            'new_stars': int(new_stars)
        }


# ============== ETD å¯¿å‘½åˆ†æå™¨ ==============
class ETDAnalyzer:
    """
    å¢å¼ºç‰ˆ ETD (Estimated Time to Depletion) åˆ†æå™¨
    åŒºåˆ†ï¼šæˆç†Ÿç¨³å®š vs çœŸæ­£è¡°é€€
    """
    
    def analyze(self, data: pd.DataFrame, vitality: str, tier: str) -> Dict:
        """
        åˆ†æé¡¹ç›®é¢„æœŸå¯¿å‘½
        """
        result = {
            'etd_months': float('inf'),
            'etd_status': 'HEALTHY',
            'decay_type': 'NONE',
            'is_mature_stable': False,
            'confidence': 'HIGH',
            'description': '',
            'recommendations': []
        }
        
        if 'activity' not in data or len(data) < 6:
            result['description'] = 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¿å‘½é¢„æµ‹'
            result['confidence'] = 'LOW'
            return result
        
        activity = data['activity'].tail(12)
        X = np.arange(len(activity)).reshape(-1, 1)
        model = LinearRegression().fit(X, activity.values)
        slope = model.coef_[0]
        current_activity = activity.iloc[-1]
        avg_activity = activity.mean()
        
        # === æƒ…å†µ1: æ´»è·ƒåº¦ä¸Šå‡ ===
        if slope >= 0:
            result['etd_months'] = float('inf')
            result['etd_status'] = 'THRIVING'
            result['description'] = 'ğŸš€ æ´»è·ƒåº¦å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œé¡¹ç›®ç”Ÿå‘½åŠ›å¼ºåŠ²ï¼Œæ— æ¯ç«­é£é™©ã€‚'
            return result
        
        # === æƒ…å†µ2: æˆç†Ÿç¨³å®šé¡¹ç›® ===
        # åˆ¤æ–­æ ‡å‡†ï¼šé«˜å±‚çº§ + å½“å‰æ´»è·ƒåº¦ä»ç„¶å¯è§‚ + ä¸æ˜¯æ€¥å‰§ä¸‹é™
        if tier in ['GIANT', 'MATURE'] and vitality in ['STABLE', 'THRIVING']:
            if current_activity > avg_activity * 0.3 and abs(slope) < avg_activity * 0.1:
                result['etd_months'] = float('inf')
                result['etd_status'] = 'STABLE_MATURE'
                result['is_mature_stable'] = True
                result['description'] = (
                    f'é¡¹ç›®å·²è¿›å…¥æˆç†Ÿç¨³å®šæœŸã€‚è™½ç„¶æ´»è·ƒåº¦æœˆå‡ä¸‹é™ {abs(slope):.2f} ç‚¹ï¼Œ'
                    f'ä½†è¿™æ˜¯æˆç†Ÿé¡¹ç›®çš„æ­£å¸¸ç‰¹å¾â€”â€”åŠŸèƒ½å®Œå–„åæ— éœ€é¢‘ç¹æ›´æ–°ã€‚'
                    f'å½“å‰æ´»è·ƒåº¦ {current_activity:.0f} ä»ç»´æŒåœ¨å¥åº·æ°´å¹³ï¼ˆå‡å€¼çš„ '
                    f'{current_activity/avg_activity*100:.0f}%ï¼‰ã€‚'
                )
                result['recommendations'].append('ä¿æŒå®šæœŸå®‰å…¨æ›´æ–°å’Œ Bug ä¿®å¤å³å¯')
                return result
        
        # === æƒ…å†µ3: æ£€æµ‹è¡°é€€æ¨¡å¼ ===
        decay_type, decay_params = self._detect_decay_pattern(activity)
        result['decay_type'] = decay_type
        
        # === æƒ…å†µ4: çœŸæ­£çš„è¡°é€€ ===
        if slope < 0 and current_activity > 0:
            # çº¿æ€§é¢„æµ‹
            etd_linear = -current_activity / slope
            
            # æŒ‡æ•°è¡°å‡é¢„æµ‹ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            if decay_type == 'EXPONENTIAL' and decay_params.get('half_life'):
                etd = decay_params['half_life'] * 3  # 3ä¸ªåŠè¡°æœŸ
            else:
                etd = etd_linear
            
            result['etd_months'] = max(0, etd)
            
            if etd < 6:
                result['etd_status'] = 'CRITICAL'
                result['description'] = (
                    f'é«˜å±é¢„è­¦ï¼šæŒ‰å½“å‰è¡°å‡é€Ÿåº¦ï¼ˆæœˆå‡ -{abs(slope):.2f}ï¼‰ï¼Œ'
                    f'é¢„è®¡ {etd:.1f} ä¸ªæœˆåæ´»è·ƒåº¦å°†å½’é›¶ã€‚'
                    f'è¡°å‡æ¨¡å¼ï¼š{decay_type}ã€‚å»ºè®®ç«‹å³é‡‡å–æªæ–½æ¿€æ´»ç¤¾åŒºã€‚'
                )
                result['recommendations'].extend([
                    'ç«‹å³å‘å¸ƒæ–°ç‰ˆæœ¬æˆ– Roadmap æ¿€æ´»ç¤¾åŒº',
                    'ç»„ç»‡çº¿ä¸Šç›´æ’­æˆ– AMA æ´»åŠ¨',
                    'æ‹›å‹Ÿæ–°ç»´æŠ¤è€…'
                ])
            elif etd < 12:
                result['etd_status'] = 'WARNING'
                result['description'] = (
                    f'è¡°é€€é¢„è­¦ï¼šæ´»è·ƒåº¦å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œé¢„è®¡ {etd:.1f} ä¸ªæœˆåå¯èƒ½æ¯ç«­ã€‚'
                    f'è¡°å‡æ¨¡å¼ï¼š{decay_type}ã€‚å»ºè®®åŠ å¼ºç¤¾åŒºè¿è¥ã€‚'
                )
                result['recommendations'].extend([
                    'å‘å¸ƒæŠ€æœ¯åšå®¢ä¿æŒæ›å…‰',
                    'æ ‡è®° Good First Issue å¸å¼•æ–°è´¡çŒ®è€…'
                ])
            elif etd < 24:
                result['etd_status'] = 'CAUTION'
                result['description'] = (
                    f'æ¸©å’Œä¸‹é™ï¼šæ´»è·ƒåº¦æœ‰æ‰€ä¸‹æ»‘ï¼Œé¢„è®¡ {etd:.1f} ä¸ªæœˆåå¯èƒ½ä½è¿·ã€‚'
                    f'ç›®å‰å°šæœ‰ç¼“å†²æ—¶é—´ï¼Œå»ºè®®è§‚å¯Ÿå¹¶é€‚æ—¶è°ƒæ•´ã€‚'
                )
                result['recommendations'].append('æŒç»­ç›‘æ§ï¼Œé€‚æ—¶è°ƒæ•´è¿è¥ç­–ç•¥')
            else:
                result['etd_status'] = 'HEALTHY'
                result['description'] = f'è½»å¾®ä¸‹æ»‘ï¼Œä½† ETD > 24ä¸ªæœˆï¼Œæš‚æ— ç´§è¿«é£é™©ã€‚'
        
        return result
    
    def _detect_decay_pattern(self, data: pd.Series) -> Tuple[str, Dict]:
        """æ£€æµ‹è¡°é€€æ¨¡å¼ç±»å‹"""
        if len(data) < 6:
            return 'UNKNOWN', {}
        
        x = np.arange(len(data))
        y = data.values
        
        # çº¿æ€§æ‹Ÿåˆ
        lin_slope, lin_intercept = np.polyfit(x, y, 1)
        lin_pred = lin_slope * x + lin_intercept
        lin_r2 = 1 - np.sum((y - lin_pred)**2) / np.sum((y - np.mean(y))**2)
        
        # æŒ‡æ•°æ‹Ÿåˆ
        try:
            log_y = np.log(y + 0.1)
            exp_slope, exp_intercept = np.polyfit(x, log_y, 1)
            exp_pred = np.exp(exp_slope * x + exp_intercept)
            exp_r2 = 1 - np.sum((y - exp_pred)**2) / np.sum((y - np.mean(y))**2)
            half_life = -np.log(2) / exp_slope if exp_slope < 0 else None
        except:
            exp_r2 = 0
            half_life = None
        
        if lin_slope >= 0:
            return 'NONE', {'slope': lin_slope}
        
        if exp_r2 > lin_r2 and exp_r2 > 0.5:
            return 'EXPONENTIAL', {'r2': exp_r2, 'half_life': half_life}
        elif lin_r2 > 0.3:
            return 'LINEAR', {'r2': lin_r2, 'slope': lin_slope}
        else:
            return 'IRREGULAR', {}


# ============== GitHub API åˆ†æå™¨ ==============
class GitHubAPIAnalyzer:
    """GitHub API æ•°æ®è·å–ä¸å¯¹æ¯”åˆ†æ"""
    
    def __init__(self, token: str = None):
        self.token = token
        self.base_url = "https://api.github.com"
        self.headers = {'Authorization': f'token {token}'} if token else {}
    
    def fetch_repo_info(self, org: str, repo: str) -> Optional[Dict]:
        """è·å–ä»“åº“åŸºæœ¬ä¿¡æ¯"""
        if not self.token:
            return None
        
        try:
            url = f"{self.base_url}/repos/{org}/{repo}"
            res = requests.get(url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                data = res.json()
                return {
                    'full_name': data.get('full_name'),
                    'description': data.get('description'),
                    'stars': data.get('stargazers_count', 0),
                    'forks': data.get('forks_count', 0),
                    'watchers': data.get('watchers_count', 0),
                    'open_issues': data.get('open_issues_count', 0),
                    'language': data.get('language'),
                    'created_at': data.get('created_at'),
                    'updated_at': data.get('updated_at'),
                    'pushed_at': data.get('pushed_at'),
                    'size': data.get('size'),
                    'license': data.get('license', {}).get('name') if data.get('license') else None,
                    'topics': data.get('topics', []),
                    'homepage': data.get('homepage'),
                    'archived': data.get('archived', False),
                    'disabled': data.get('disabled', False)
                }
            else:
                print(f"GitHub API è¯·æ±‚å¤±è´¥: {res.status_code}")
                return None
        except Exception as e:
            print(f"GitHub API é”™è¯¯: {e}")
            return None
    
    def fetch_recent_activity(self, org: str, repo: str, days: int = 30) -> Dict:
        """
        è·å–æœ€è¿‘Nå¤©çš„æ´»è·ƒæ•°æ®
        åŒ…æ‹¬: commits, issues, PRs
        """
        if not self.token:
            return {'error': 'éœ€è¦ GitHub Token'}
        
        from datetime import datetime, timedelta
        since_date = (datetime.now() - timedelta(days=days)).isoformat()
        
        result = {
            'period_days': days,
            'commits': 0,
            'issues_opened': 0,
            'issues_closed': 0,
            'prs_opened': 0,
            'prs_merged': 0,
            'contributors_active': set(),
            'daily_activity': []
        }
        
        try:
            # 1. è·å–æœ€è¿‘æäº¤
            commits_url = f"{self.base_url}/repos/{org}/{repo}/commits?since={since_date}&per_page=100"
            res = requests.get(commits_url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                commits = res.json()
                result['commits'] = len(commits)
                for c in commits:
                    author = c.get('author', {})
                    if author and author.get('login'):
                        result['contributors_active'].add(author['login'])
            
            # 2. è·å–æœ€è¿‘ Issues
            issues_url = f"{self.base_url}/repos/{org}/{repo}/issues?state=all&since={since_date}&per_page=100"
            res = requests.get(issues_url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                issues = res.json()
                for issue in issues:
                    if 'pull_request' not in issue:  # æ’é™¤ PR
                        created = issue.get('created_at', '')
                        if created >= since_date:
                            result['issues_opened'] += 1
                        if issue.get('state') == 'closed':
                            result['issues_closed'] += 1
            
            # 3. è·å–æœ€è¿‘ PRs
            prs_url = f"{self.base_url}/repos/{org}/{repo}/pulls?state=all&per_page=100"
            res = requests.get(prs_url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                prs = res.json()
                for pr in prs:
                    created = pr.get('created_at', '')
                    if created >= since_date:
                        result['prs_opened'] += 1
                        if pr.get('merged_at'):
                            result['prs_merged'] += 1
            
            result['contributors_active'] = len(result['contributors_active'])
            
        except Exception as e:
            result['error'] = str(e)
        
        return result
    
    def validate_conclusions(self, opendigger_data: pd.DataFrame, 
                             github_recent: Dict,
                             analysis_result: Dict) -> Dict:
        """
        ä½¿ç”¨ GitHub 30å¤©æ•°æ®éªŒè¯åˆ†æç»“è®º
        """
        validation = {
            'overall_valid': True,
            'confidence': 0,
            'validations': [],
            'warnings': [],
            'github_30d_summary': {}
        }
        
        if 'error' in github_recent:
            validation['error'] = github_recent['error']
            validation['confidence'] = 0
            return validation
        
        # === GitHub 30å¤©æ•°æ®æ‘˜è¦ ===
        gh_commits = github_recent.get('commits', 0)
        gh_issues_opened = github_recent.get('issues_opened', 0)
        gh_issues_closed = github_recent.get('issues_closed', 0)
        gh_prs_opened = github_recent.get('prs_opened', 0)
        gh_prs_merged = github_recent.get('prs_merged', 0)
        gh_active_contributors = github_recent.get('contributors_active', 0)
        
        validation['github_30d_summary'] = {
            'commits': gh_commits,
            'issues_opened': gh_issues_opened,
            'issues_closed': gh_issues_closed,
            'prs_opened': gh_prs_opened,
            'prs_merged': gh_prs_merged,
            'active_contributors': gh_active_contributors
        }
        
        valid_count = 0
        total_checks = 0
        
        # === éªŒè¯1: æ´»è·ƒåº¦è¶‹åŠ¿ ===
        total_checks += 1
        vitality = analysis_result.get('vitality', '')
        
        # è®¡ç®—30å¤©æ´»è·ƒåº¦æŒ‡æ ‡
        gh_activity_score = gh_commits + gh_prs_opened * 2 + gh_issues_opened
        
        if vitality == 'THRIVING':
            if gh_activity_score >= 10:
                valid_count += 1
                validation['validations'].append({
                    'check': 'æ´»è·ƒåº¦éªŒè¯',
                    'result': 'PASS',
                    'detail': f'THRIVING çŠ¶æ€éªŒè¯é€šè¿‡ - 30å¤©æ´»è·ƒåº¦å¾—åˆ†: {gh_activity_score}'
                })
            else:
                validation['warnings'].append(f'THRIVING çŠ¶æ€ä½†30å¤©æ´»è·ƒåº¦ä½ ({gh_activity_score})')
        elif vitality == 'ZOMBIE':
            if gh_activity_score <= 5:
                valid_count += 1
                validation['validations'].append({
                    'check': 'æ´»è·ƒåº¦éªŒè¯',
                    'result': 'PASS',
                    'detail': f'ZOMBIE çŠ¶æ€éªŒè¯é€šè¿‡ - 30å¤©å‡ ä¹æ— æ´»åŠ¨'
                })
            else:
                validation['warnings'].append(f'ZOMBIE çŠ¶æ€ä½†30å¤©ä»æœ‰æ´»åŠ¨ ({gh_activity_score})')
        else:
            # STABLE/DORMANT
            if 3 <= gh_activity_score <= 50:
                valid_count += 1
            validation['validations'].append({
                'check': 'æ´»è·ƒåº¦éªŒè¯',
                'result': 'PASS',
                'detail': f'{vitality} çŠ¶æ€ä¸30å¤©æ´»è·ƒåº¦ {gh_activity_score} åŸºæœ¬ä¸€è‡´'
            })
        
        # === éªŒè¯2: ç»´æŠ¤æ•ˆç‡ ===
        total_checks += 1
        if gh_issues_opened > 0:
            issue_ratio = gh_issues_closed / gh_issues_opened
            od_debt = analysis_result.get('resistance_analysis', {}).get('debt', 50)
            
            if issue_ratio >= 0.8 and od_debt <= 50:
                valid_count += 1
                validation['validations'].append({
                    'check': 'ç»´æŠ¤æ•ˆç‡éªŒè¯',
                    'result': 'PASS',
                    'detail': f'Issue å¤„ç†ç‡ {issue_ratio:.0%} ä¸ä½é˜»åŠ›è¯„ä¼°ä¸€è‡´'
                })
            elif issue_ratio < 0.5 and od_debt >= 50:
                valid_count += 1
                validation['validations'].append({
                    'check': 'ç»´æŠ¤æ•ˆç‡éªŒè¯',
                    'result': 'PASS',
                    'detail': f'Issue å¤„ç†ç‡ {issue_ratio:.0%} ä¸é«˜é˜»åŠ›è¯„ä¼°ä¸€è‡´'
                })
            else:
                validation['warnings'].append(
                    f'ç»´æŠ¤æ•ˆç‡è¯„ä¼°å¯èƒ½ä¸å‡† - 30å¤©Issueå¤„ç†ç‡: {issue_ratio:.0%}, é˜»åŠ›è¯„åˆ†: {od_debt}'
                )
        else:
            valid_count += 0.5  # æ— æ•°æ®ï¼Œéƒ¨åˆ†æœ‰æ•ˆ
            validation['validations'].append({
                'check': 'ç»´æŠ¤æ•ˆç‡éªŒè¯',
                'result': 'N/A',
                'detail': '30å¤©æ— æ–°Issueï¼Œæ— æ³•éªŒè¯'
            })
        
        # === éªŒè¯3: è´¡çŒ®è€…æ´»è·ƒåº¦ ===
        total_checks += 1
        od_participants = opendigger_data['participants'].tail(1).values[0] if 'participants' in opendigger_data else 0
        
        if od_participants > 0:
            contrib_ratio = gh_active_contributors / od_participants
            if 0.1 <= contrib_ratio <= 1.0:
                valid_count += 1
                validation['validations'].append({
                    'check': 'è´¡çŒ®è€…éªŒè¯',
                    'result': 'PASS',
                    'detail': f'30å¤©æ´»è·ƒ {gh_active_contributors} äººï¼Œå æ€»è´¡çŒ®è€… {contrib_ratio:.0%}'
                })
            elif contrib_ratio > 1:
                validation['warnings'].append('30å¤©æ´»è·ƒè´¡çŒ®è€…è¶…è¿‡å†å²è®°å½•ï¼Œæ•°æ®å¯èƒ½æœ‰å»¶è¿Ÿ')
            else:
                validation['warnings'].append(f'30å¤©åªæœ‰ {gh_active_contributors} äººæ´»è·ƒï¼Œè´¡çŒ®è€…å¯èƒ½æµå¤±')
        
        # === éªŒè¯4: PR æ•ˆç‡ ===
        total_checks += 1
        if gh_prs_opened > 0:
            pr_merge_rate = gh_prs_merged / gh_prs_opened
            if pr_merge_rate >= 0.5:
                valid_count += 1
                validation['validations'].append({
                    'check': 'PRæ•ˆç‡éªŒè¯',
                    'result': 'PASS',
                    'detail': f'30å¤©PRåˆå¹¶ç‡ {pr_merge_rate:.0%}'
                })
            else:
                validation['warnings'].append(f'PRåˆå¹¶ç‡åä½ ({pr_merge_rate:.0%})')
        else:
            valid_count += 0.5
        
        # === ç»¼åˆç½®ä¿¡åº¦ ===
        validation['confidence'] = round(valid_count / total_checks * 100, 1) if total_checks > 0 else 0
        validation['overall_valid'] = validation['confidence'] >= 60 and len(validation['warnings']) <= 2
        
        # === ç»¼åˆåˆ¤æ–­ ===
        if validation['overall_valid']:
            validation['summary'] = f"ç»“è®ºéªŒè¯é€šè¿‡ (ç½®ä¿¡åº¦: {validation['confidence']}%)"
        else:
            validation['summary'] = f"éƒ¨åˆ†ç»“è®ºéœ€å¤æ ¸ (ç½®ä¿¡åº¦: {validation['confidence']}%)"
        
        return validation
    
    def fetch_contributors_stats(self, org: str, repo: str) -> Optional[List[Dict]]:
        """è·å–è´¡çŒ®è€…ç»Ÿè®¡"""
        if not self.token:
            return None
        
        try:
            url = f"{self.base_url}/repos/{org}/{repo}/stats/contributors"
            res = requests.get(url, headers=self.headers, timeout=30)
            if res.status_code == 200:
                data = res.json()
                if isinstance(data, list):
                    return [{
                        'login': c.get('author', {}).get('login'),
                        'total_commits': c.get('total', 0),
                        'weeks_active': len([w for w in c.get('weeks', []) if w.get('c', 0) > 0])
                    } for c in data]
            return None
        except:
            return None
    
    def compare_with_opendigger(self, opendigger_data: pd.DataFrame, 
                                 github_info: Dict) -> Dict:
        """å¯¹æ¯” OpenDigger ä¸ GitHub API æ•°æ®"""
        comparison = {
            'data_sources': ['OpenDigger', 'GitHub API'],
            'metrics': {},
            'consistency_score': 0,
            'discrepancies': []
        }
        
        if github_info is None:
            comparison['error'] = 'GitHub API æ•°æ®ä¸å¯ç”¨'
            return comparison
        
        # å¯¹æ¯” Stars
        od_stars = opendigger_data['stars'].sum() if 'stars' in opendigger_data else 0
        gh_stars = github_info.get('stars', 0)
        comparison['metrics']['stars'] = {
            'opendigger': int(od_stars),
            'github_api': gh_stars,
            'diff_pct': abs(od_stars - gh_stars) / (gh_stars + 1) * 100
        }
        
        # å¯¹æ¯” Open Issues
        if 'issues_new' in opendigger_data and 'issues_closed' in opendigger_data:
            od_open = opendigger_data['issues_new'].sum() - opendigger_data['issues_closed'].sum()
            gh_open = github_info.get('open_issues', 0)
            comparison['metrics']['open_issues'] = {
                'opendigger_calc': int(od_open),
                'github_api': gh_open,
                'diff': int(abs(od_open - gh_open))
            }
        
        # ä¸€è‡´æ€§è¯„åˆ†
        diffs = [m.get('diff_pct', 0) for m in comparison['metrics'].values() 
                 if isinstance(m, dict) and 'diff_pct' in m]
        if diffs:
            avg_diff = np.mean(diffs)
            comparison['consistency_score'] = round(max(0, 100 - avg_diff), 1)
        else:
            comparison['consistency_score'] = 100
        
        # é¢å¤–ä¿¡æ¯
        comparison['github_extra'] = {
            'language': github_info.get('language'),
            'license': github_info.get('license'),
            'topics': github_info.get('topics', [])[:5],
            'archived': github_info.get('archived'),
            'last_push': github_info.get('pushed_at')
        }
        
        return comparison
    
    def calculate_activity_score(self, github_recent: Dict) -> float:
        """
        è®¡ç®—GitHub APIæ•°æ®çš„æ´»è·ƒåº¦å¾—åˆ†ï¼Œç”¨äºæ˜ å°„OpenDiggerçš„openrankå’ŒattentionæŒ‡æ ‡
        æ´»è·ƒåº¦å¾—åˆ† = commits * 0.4 + prs_opened * 0.3 + issues_opened * 0.2 + contributors_active * 0.1
        """
        if 'error' in github_recent:
            return 0
        
        score = (
            github_recent.get('commits', 0) * 0.4 +
            github_recent.get('prs_opened', 0) * 0.3 +
            github_recent.get('issues_opened', 0) * 0.2 +
            github_recent.get('contributors_active', 0) * 0.1
        )
        
        return score
    
    def map_monthly_prediction_to_daily_github_data(self, monthly_prediction: float, 
                                                  github_30d_data: Dict, 
                                                  metric: str = 'stars') -> Dict:
        """
        å°†æœˆåº¦é¢„æµ‹å€¼æ˜ å°„åˆ°GitHub APIçš„30å¤©æ•°æ®
        - monthly_prediction: é¢„æµ‹çš„æœˆåº¦æ–°å¢å€¼
        - github_30d_data: GitHub APIè·å–çš„30å¤©æ´»è·ƒæ•°æ®
        - metric: æŒ‡æ ‡ç±»å‹
        """
        # è®¡ç®—æ—¥å‡é¢„æµ‹å€¼
        daily_predicted = monthly_prediction / 30
        
        # æ ¹æ®æŒ‡æ ‡ç±»å‹è·å–å¯¹åº”çš„GitHub APIæ•°æ®
        if metric == 'stars':
            # starsæ˜¯ç´¯è®¡å€¼ï¼Œéœ€è¦è®¡ç®—30å¤©å¢é‡
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦è·å–30å¤©å‰çš„starsæ•°
            gh_30d_value = 0  # å®é™…å®ç°éœ€è¦è·å–å†å²starsæ•°æ®
        elif metric in ['openrank', 'attention']:
            # openrankå’Œattentioné€šè¿‡æ´»è·ƒåº¦è®¡ç®—
            gh_30d_value = self.calculate_activity_score(github_30d_data)
        elif metric in ['pr_new', 'issues_new']:
            # ç›´æ¥ä½¿ç”¨GitHub APIçš„30å¤©æ•°æ®
            gh_30d_value = github_30d_data.get(f'{metric}_opened', 0) if metric == 'issues_new' else github_30d_data.get('prs_opened', 0)
        elif metric == 'participants':
            # ç›´æ¥ä½¿ç”¨GitHub APIçš„æ´»è·ƒè´¡çŒ®è€…æ•°
            gh_30d_value = github_30d_data.get('contributors_active', 0)
        else:
            gh_30d_value = 0
        
        # è®¡ç®—é¢„æµ‹è¯¯å·®
        error = abs(daily_predicted * 30 - gh_30d_value)
        error_pct = error / (gh_30d_value + 1) * 100
        
        return {
            'monthly_prediction': monthly_prediction,
            'daily_predicted': daily_predicted,
            'github_30d_value': gh_30d_value,
            'github_daily_avg': gh_30d_value / 30,
            'error': error,
            'error_pct': error_pct
        }


# ============== å‡çº§ç‰ˆé¡¹ç›®åˆ†æå™¨ ==============
class ProjectAnalyzerV45:
    """GitHub é¡¹ç›®æ·±åº¦åˆ†æå™¨ v4.5 - ç®—æ³•å‡çº§ç‰ˆ"""
    
    CORE_METRICS = [
        "openrank", "activity", "stars", "attention",
        "participants", "new_contributors", "inactive_contributors",
        "bus_factor", "issues_new", "issues_closed", "pr_new", "pr_merged"
    ]
    
    def __init__(self, url: str, github_token: Optional[str] = None):
        self.org, self.repo = self._parse_url(url)
        self.df = pd.DataFrame()
        self.tier = None
        self.tier_probabilities = {}
        self.tier_confidence = 0
        self.github_token = github_token or os.getenv('GITHUB_TOKEN')
        
        # åˆå§‹åŒ–å‡çº§ç»„ä»¶
        self.gmm_classifier = GMMTierClassifier()
        self.prophet_predictor = ProphetPredictor()
        self.ahp_evaluator = AHPHealthEvaluator()
        self.backtest_validator = BacktestValidator()
        
        # åˆå§‹åŒ–åŸæœ‰è®¡ç®—å™¨
        self.momentum_calculator = MomentumCalculator()
        self.resistance_calculator = ResistanceCalculator()
        self.potential_calculator = PotentialCalculator()
        self.bus_factor_calculator = BusFactorCalculator()
        self.change_point_detector = ChangePointDetector()
        self.funnel_analyzer = ConversionFunnelAnalyzer()
        self.etd_analyzer = ETDAnalyzer()
    
    def _parse_url(self, url: str) -> Tuple[str, str]:
        match = re.search(r"github\.com/([^/]+)/([^/]+)", url)
        if match:
            return match.group(1), match.group(2).replace(".git", "")
        if "/" in url and "http" not in url:
            parts = url.split('/')
            return parts[0], parts[1]
        raise ValueError("æ— æ•ˆçš„ GitHub URL")
    
    def fetch_data(self) -> bool:
        """è·å–æ•°æ®ï¼ˆOpenDiggerä¼˜å…ˆï¼ŒGitHub APIå¤‡ç”¨ï¼‰"""
        print(f"\n{'='*60}")
        print(f"  GitHub é¡¹ç›®æ·±åº¦åˆ†æå™¨ v4.5 - ç®—æ³•å‡çº§ç‰ˆ")
        print(f"  é¡¹ç›®: {self.org}/{self.repo}")
        print(f"{'='*60}\n")
        
        raw_data = {}
        for metric in self.CORE_METRICS:
            url = f"https://oss.open-digger.cn/github/{self.org}/{self.repo}/{metric}.json"
            try:
                res = requests.get(url, timeout=15)
                if res.status_code == 200:
                    data = res.json()
                    monthly = {k: v for k, v in data.items() if re.match(r'^\d{4}-\d{2}$', str(k))}
                    if monthly:
                        raw_data[metric] = pd.Series(monthly)
            except:
                continue
        
        if not raw_data:
            print("æ— æ³•è·å–æ•°æ®")
            return False
        
        self.df = pd.DataFrame(raw_data).fillna(0)
        self.df.index = pd.to_datetime(self.df.index)
        self.df = self.df.sort_index()
        
        # ä¿å­˜åŸå§‹æ•°æ®
        self.df.to_csv(f"{self.org}_{self.repo}_data.csv", encoding='utf-8-sig')
        
        # GMMæ¦‚ç‡åŒ–åˆ†å±‚
        metrics = {
            'avg_openrank': self.df['openrank'].mean() if 'openrank' in self.df else 0,
            'total_stars': self.df['stars'].sum() if 'stars' in self.df else 0,
            'max_participants': self.df['participants'].max() if 'participants' in self.df else 0
        }
        
        self.tier, self.tier_probabilities, self.tier_confidence = self.gmm_classifier.predict_proba(metrics)
        
        print(f"è·å– {len(self.df)} ä¸ªæœˆæ•°æ®")
        print(f"GMMåˆ†å±‚: {self.tier} ({TIER_NAMES[self.tier]})")
        print(f"å±‚çº§æ¦‚ç‡åˆ†å¸ƒ: {self.tier_probabilities}")
        print(f"ç½®ä¿¡åº¦: {self.tier_confidence:.0%}")
        
        return True
    
    def analyze_lifecycle(self) -> str:
        if len(self.df) < 12:
            return 'INCUBATION'
        openrank = self.df['openrank']
        n = len(openrank)
        q1 = openrank.iloc[:n//3].mean()
        q2 = openrank.iloc[n//3:2*n//3].mean()
        q3 = openrank.iloc[2*n//3:].mean()
        slope = linregress(range(6), openrank.tail(6).values).slope
        
        if q1 < q2 < q3 and slope > 0:
            return 'GROWTH'
        elif q3 >= q2 * 0.85:
            return 'MATURITY'
        elif q3 < q2 * 0.7:
            return 'REVIVAL' if slope > 0.3 else 'DECLINE'
        return 'MATURITY'
    
    def analyze_vitality(self) -> str:
        if 'activity' not in self.df:
            return 'UNKNOWN'
        activity = self.df['activity']
        recent = activity.tail(6)
        slope = linregress(range(len(recent)), recent.values).slope
        peak, current = activity.max(), recent.mean()
        
        if slope > 0:
            return 'THRIVING'
        if self.tier in ['GIANT', 'MATURE'] and current > peak * 0.2:
            return 'STABLE'
        if current < peak * 0.1:
            return 'ZOMBIE'
        return 'DORMANT'
    
    def evaluate_trend_3d(self) -> Dict:
        """åŠ¨é‡-é˜»åŠ›-æ½œåŠ›ä¸‰ç»´è¯„ä¼°"""
        momentum = self.momentum_calculator.calculate(self.df)
        resistance = self.resistance_calculator.calculate(self.df)
        potential = self.potential_calculator.calculate(self.df, self.tier)
        
        potential_factor = min(1.5, 0.5 + potential['remaining_space'] / 100)
        trend_score = (momentum['total'] - resistance['total'] * 0.5) * potential_factor
        
        if trend_score >= 60:
            trend_class, desc = 'STRONG_UP', 'å¼ºåŠ¿ä¸Šè¡Œ'
        elif trend_score >= 30:
            trend_class, desc = 'MODERATE_UP', 'æ¸©å’Œä¸Šè¡Œ'
        elif trend_score >= 0:
            trend_class, desc = 'STABLE', 'æ¨ªç›˜ç¨³å®š'
        elif trend_score >= -30:
            trend_class, desc = 'MODERATE_DOWN', 'æ¸©å’Œä¸‹è¡Œ'
        else:
            trend_class, desc = 'STRONG_DOWN', 'è¶‹åŠ¿æ¶åŒ–'
        
        return {
            'trend_score': round(trend_score, 1),
            'trend_class': trend_class,
            'description': desc,
            'momentum': momentum,
            'resistance': resistance,
            'potential': potential
        }
    
    def analyze_risk(self, vitality: str, trend: Dict) -> Dict:
        risk_score = 0
        alerts = []
        
        if 'activity' in self.df:
            slope = linregress(range(min(12, len(self.df))), 
                             self.df['activity'].tail(12).values).slope
            if slope < -0.5:
                risk_score += 30
                alerts.append('æ´»è·ƒåº¦æ˜¾è‘—ä¸‹é™')
            elif slope < 0:
                risk_score += 15
        
        if trend['resistance']['status'] == 'HEAVY':
            risk_score += 25
            alerts.append('æŠ€æœ¯å€ºé˜»åŠ›é«˜')
        
        if vitality == 'ZOMBIE':
            risk_score += 30
            alerts.append('é¡¹ç›®å¤„äºåƒµå°¸çŠ¶æ€')
        elif vitality == 'DORMANT':
            risk_score += 15
        
        if risk_score >= 50:
            level = 'CRITICAL'
        elif risk_score >= 30:
            level = 'HIGH'
        elif risk_score >= 15:
            level = 'MEDIUM'
        else:
            level = 'LOW'
        
        return {'score': risk_score, 'level': level, 'alerts': alerts}
    
    def analyze_dark_horse(self, trend: Dict, funnel: Dict) -> Dict:
        if self.tier in ['GIANT', 'MATURE']:
            return {'is_dark_horse': False, 'score': 0, 'reasons': ['å·²è¶…å‡ºé»‘é©¬èŒƒç•´']}
        
        score = 0
        reasons = []
        
        if trend['momentum']['total'] >= 70:
            score += 30
            reasons.append('å¼ºåŠ²åŠ¨é‡')
        elif trend['momentum']['total'] >= 50:
            score += 15
        
        if trend['potential']['remaining_space'] >= 60:
            score += 25
            reasons.append('å·¨å¤§å¢é•¿ç©ºé—´')
        
        if funnel.get('quality') == 'EXCELLENT':
            score += 25
            reasons.append('ä¼˜ç§€è½¬åŒ–ç‡')
        elif funnel.get('quality') == 'GOOD':
            score += 15
        
        if funnel.get('quality') == 'BUBBLE':
            score -= 20
            reasons.append(' è½¬åŒ–ç‡è¿‡ä½')
        
        return {
            'is_dark_horse': score >= 55,
            'score': min(100, max(0, score)),
            'reasons': reasons
        }
    
    def generate_predictions(self) -> Dict:
        """ç”ŸæˆProphetæ—¶åºé¢„æµ‹"""
        predictions = {}
        
        # é¢„æµ‹æ ¸å¿ƒæŒ‡æ ‡
        for metric in ['openrank', 'activity', 'stars', 'participants']:
            if metric in self.df:
                pred_result = self.prophet_predictor.predict(self.df, metric, periods=6)
                predictions[metric] = pred_result
        
        # æ·»åŠ é¢„æµ‹æ‘˜è¦
        if 'openrank' in predictions and predictions['openrank'].get('forecast'):
            forecast_values = predictions['openrank']['forecast']
            current_value = self.df['openrank'].iloc[-1] if 'openrank' in self.df else 0
            
            if forecast_values:
                growth_pct = (forecast_values[-1] - current_value) / (current_value + 0.1) * 100
                
                predictions['summary'] = {
                    'current_value': round(current_value, 2),
                    'forecast_6m': round(forecast_values[-1], 2),
                    'growth_pct': round(growth_pct, 1),
                    'trend': 'ä¸Šå‡' if growth_pct > 5 else 'ä¸‹é™' if growth_pct < -5 else 'ç¨³å®š'
                }
        
        return predictions
    
    def run_backtest(self) -> Dict:
        """è¿è¡Œå›æµ‹éªŒè¯"""
        backtest_results = {}
        
        # å¯¹å…³é”®æŒ‡æ ‡è¿›è¡Œå›æµ‹
        for metric in ['openrank', 'activity']:
            if metric in self.df:
                result = self.backtest_validator.validate_predictions(self.df, metric)
                backtest_results[metric] = result
        
        # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
        confidences = []
        for metric, result in backtest_results.items():
            if 'confidence_level' in result:
                conf_map = {'HIGH': 0.9, 'MEDIUM': 0.7, 'LOW': 0.5, 'VERY_LOW': 0.3}
                confidences.append(conf_map.get(result['confidence_level'], 0.5))
        
        if confidences:
            avg_confidence = np.mean(confidences)
            backtest_results['overall_confidence'] = round(avg_confidence, 3)
            backtest_results['overall_confidence_level'] = (
                'HIGH' if avg_confidence >= 0.8 else
                'MEDIUM' if avg_confidence >= 0.6 else
                'LOW' if avg_confidence >= 0.4 else 'VERY_LOW'
            )
        
        return backtest_results
    
    def generate_recommendations(self, vitality: str, trend: Dict, risk: Dict, 
                                  dark_horse: Dict, bus_factor: Dict,
                                  etd_analysis: Dict, predictions: Dict) -> List[str]:
        recs = []
        
        if risk['level'] in ['CRITICAL', 'HIGH']:
            recs.append('é£é™©è¾ƒé«˜ï¼Œå»ºè®®åŠ å¼ºç¤¾åŒºè¿è¥å’ŒæŠ€æœ¯åšå®¢æ›å…‰')
        
        if trend['resistance']['debt'] > 60:
            recs.append('æŠ€æœ¯å€ºåŠ¡è¾ƒé‡ï¼Œå»ºè®®ç»„ç»‡ Bug Bash æ´»åŠ¨é›†ä¸­å¤„ç†')
        
        if bus_factor['risk_level'] in ['CRITICAL', 'HIGH']:
            recs.append(' Bus Factor è¿‡ä½ï¼Œå»ºè®®åŸ¹å…»æ›´å¤šæ ¸å¿ƒè´¡çŒ®è€…')
        
        if vitality == 'STABLE':
            recs.append('é¡¹ç›®å·²æˆç†Ÿï¼Œä¿æŒå®šæœŸå®‰å…¨æ›´æ–°å³å¯')
        
        if dark_horse.get('is_dark_horse'):
            recs.append('é»‘é©¬æ½œåŠ›æ˜¾ç°ï¼Œå»ºè®®åŠ å¤§æ¨å¹¿åŠ›åº¦')
        
        if trend['momentum']['gravity'] < 40:
            recs.append('è´¡çŒ®è€…å‘å¿ƒåŠ›ä¸è¶³ï¼Œå»ºè®®æ ‡è®° Good First Issue')
        
        if etd_analysis.get('etd_status') in ['CRITICAL', 'WARNING']:
            recs.append(f"ETDå¯¿å‘½é¢„è­¦: {etd_analysis.get('description', '')}")
        
        if predictions.get('summary', {}).get('trend') == 'ä¸‹é™':
            recs.append(f"é¢„æµ‹æ˜¾ç¤ºæœªæ¥è¶‹åŠ¿ä¸‹é™ï¼Œå»ºè®®å…³æ³¨é¡¹ç›®æ´»è·ƒåº¦")
        
        if not recs:
            recs.append('é¡¹ç›®çŠ¶æ€å¥åº·ï¼Œä¿æŒå½“å‰è¿è¥èŠ‚å¥')
        
        return recs
    
    def run(self) -> Optional[AnalysisResult]:
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        if not self.fetch_data():
            return None
        
        # ç”Ÿæˆé¢„æµ‹
        print("\nğŸ“ˆ æ­£åœ¨ç”Ÿæˆæ—¶åºé¢„æµ‹...")
        predictions = self.generate_predictions()
        
        # è¿è¡Œå›æµ‹
        print("ğŸ” æ­£åœ¨è¿è¡Œå›æµ‹éªŒè¯...")
        backtest_results = self.run_backtest()
        
        # å…¶ä»–åˆ†ææ¨¡å—
        lifecycle = self.analyze_lifecycle()
        vitality = self.analyze_vitality()
        trend_3d = self.evaluate_trend_3d()
        risk = self.analyze_risk(vitality, trend_3d)
        bus_factor_2 = self.bus_factor_calculator.calculate(self.df)
        funnel = self.funnel_analyzer.analyze(self.df)
        dark_horse = self.analyze_dark_horse(trend_3d, funnel)
        change_points = self.change_point_detector.detect(self.df.get('openrank', pd.Series()))
        etd_analysis = self.etd_analyzer.analyze(self.df, vitality, self.tier)
        
        # === GitHub API å¯¹æ¯”ä¸30å¤©éªŒè¯ ===
        github_comparison = {}
        conclusion_validation = {'error': 'æœªæä¾› GitHub Tokenï¼Œè·³è¿‡éªŒè¯'}
        prediction_validation = {'error': 'æœªæä¾› GitHub Tokenï¼Œè·³è¿‡é¢„æµ‹éªŒè¯'}
        
        if self.github_token:
            print("æ­£åœ¨è·å– GitHub API æ•°æ®...")
            gh_analyzer = GitHubAPIAnalyzer(self.github_token)
            gh_info = gh_analyzer.fetch_repo_info(self.org, self.repo)
            
            if gh_info:
                github_comparison = gh_analyzer.compare_with_opendigger(self.df, gh_info)
                print(f"GitHub API å¯¹æ¯”å®Œæˆï¼Œä¸€è‡´æ€§: {github_comparison.get('consistency_score', 0)}%")
                
                # === è·å–30å¤©æ´»è·ƒæ•°æ®å¹¶éªŒè¯ ===
                print("æ­£åœ¨è·å–æœ€è¿‘30å¤©æ•°æ®è¿›è¡Œç»“è®ºéªŒè¯...")
                github_recent = gh_analyzer.fetch_recent_activity(self.org, self.repo, days=30)
                
                if 'error' not in github_recent:
                    # éªŒè¯åˆ†æç»“è®º
                    analysis_for_validation = {
                        'vitality': vitality,
                        'resistance_analysis': trend_3d['resistance'],
                        'momentum_analysis': trend_3d['momentum']
                    }
                    conclusion_validation = gh_analyzer.validate_conclusions(
                        self.df, github_recent, analysis_for_validation
                    )
                    print(f"{conclusion_validation.get('summary', 'éªŒè¯å®Œæˆ')}")
                    
                    # === é¢„æµ‹éªŒè¯ï¼šå°†æœˆåº¦é¢„æµ‹å€¼ä¸GitHub APIçš„30å¤©æ•°æ®è¿›è¡Œå¯¹æ¯” ===
                    print("æ­£åœ¨è¿›è¡Œé¢„æµ‹éªŒè¯...")
                    
                    # å®šä¹‰è¦éªŒè¯çš„æŒ‡æ ‡
                    metrics_to_validate = ['openrank', 'attention', 'stars']
                    prediction_validation = {
                        'metrics': {},
                        'overall_score': 0,
                        'overall_rating': 'UNKNOWN'
                    }
                    
                    total_error = 0
                    valid_metrics = 0
                    
                    for metric in metrics_to_validate:
                        if metric in predictions and predictions[metric].get('forecast'):
                            # è·å–æœˆåº¦é¢„æµ‹å€¼ï¼ˆå–ç¬¬ä¸€ä¸ªæœˆçš„é¢„æµ‹å€¼ï¼‰
                            monthly_prediction = predictions[metric]['forecast'][0]
                            
                            # å°†æœˆåº¦é¢„æµ‹å€¼æ˜ å°„åˆ°GitHub APIçš„30å¤©æ•°æ®
                            mapped_result = gh_analyzer.map_monthly_prediction_to_daily_github_data(
                                monthly_prediction, github_recent, metric
                            )
                            
                            prediction_validation['metrics'][metric] = mapped_result
                            
                            if 'error_pct' in mapped_result:
                                total_error += mapped_result['error_pct']
                                valid_metrics += 1
                    
                    # è®¡ç®—ç»¼åˆé¢„æµ‹å‡†ç¡®æ€§
                    if valid_metrics > 0:
                        avg_error = total_error / valid_metrics
                        prediction_validation['overall_score'] = 100 - avg_error
                        
                        # è¯„çº§
                        if avg_error < 10:
                            prediction_validation['overall_rating'] = 'EXCELLENT'
                        elif avg_error < 20:
                            prediction_validation['overall_rating'] = 'GOOD'
                        elif avg_error < 30:
                            prediction_validation['overall_rating'] = 'FAIR'
                        else:
                            prediction_validation['overall_rating'] = 'POOR'
                        
                        print(f"é¢„æµ‹éªŒè¯å®Œæˆï¼Œå¹³å‡è¯¯å·®: {avg_error:.1f}%ï¼Œè¯„çº§: {prediction_validation['overall_rating']}")
                else:
                    conclusion_validation = {'error': github_recent.get('error')}
                    prediction_validation = {'error': github_recent.get('error')}
        
        # å¥åº·è¯„åˆ†ï¼ˆä½¿ç”¨AHPï¼‰
        health_score, dimension_scores = self.ahp_evaluator.calculate_health_score(
            vitality, trend_3d, risk, self.tier, predictions
        )
        
        # ç¡®å®šå¥åº·ç­‰çº§
        grades = [(85, 'A+'), (75, 'A'), (65, 'B+'), (55, 'B'), (45, 'C'), (35, 'D'), (0, 'F')]
        health_grade = next(g for t, g in grades if health_score >= t)
        
        # å»ºè®®
        recommendations = self.generate_recommendations(
            vitality, trend_3d, risk, dark_horse, bus_factor_2, etd_analysis, predictions
        )
        
        # æ·»åŠ éªŒè¯è­¦å‘Š
        if conclusion_validation.get('warnings'):
            for w in conclusion_validation['warnings'][:2]:  # æœ€å¤š2æ¡
                recommendations.append(f'éªŒè¯æç¤º: {w}')
        
        # æ„å»ºç»“æœ
        result = AnalysisResult(
            project_name=f"{self.org}/{self.repo}",
            tier=self.tier,
            tier_probabilities=self.tier_probabilities,
            tier_confidence=self.tier_confidence,
            lifecycle=lifecycle,
            vitality=vitality,
            health_score=health_score,
            health_grade=health_grade,
            dimension_scores=dimension_scores,
            momentum_analysis=trend_3d['momentum'],
            resistance_analysis=trend_3d['resistance'],
            potential_analysis=trend_3d['potential'],
            trend_3d=trend_3d,
            risk_analysis=risk,
            dark_horse_analysis=dark_horse,
            change_points=change_points,
            bus_factor_2=bus_factor_2,
            etd_analysis=etd_analysis,
            github_comparison=github_comparison,
            conclusion_validation=conclusion_validation,
            prediction_validation=prediction_validation,
            predictions=predictions,
            backtest_results=backtest_results,
            recommendations=recommendations,
            detailed_report=""
        )
        
        result.detailed_report = self.generate_report(result)
        
        # è¾“å‡º
        print(result.detailed_report)
        
        # === ä¿å­˜æŠ¥å‘Šï¼ˆTXTï¼‰===
        with open(f"{self.org}_{self.repo}_v45_report.txt", 'w', encoding='utf-8') as f:
            f.write(result.detailed_report)
        print(f"ğŸ“ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {self.org}_{self.repo}_v45_report.txt")
        
        # === ä¿å­˜æ•°æ®ï¼ˆJSONï¼‰===
        self._save_json_report(result)
        
        # ç»˜å›¾
        self.plot_dashboard(result)
        
        return result
    
    def generate_report(self, result: AnalysisResult) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report = f"""
{'â•'*70}
                    {result.project_name} æ·±åº¦è¯Šæ–­æŠ¥å‘Š (v4.5)
                    GitHub é¡¹ç›®åˆ†æå™¨ - ç®—æ³•å‡çº§ç‰ˆ
{'â•'*70}

ã€é¡¹ç›®ç”»åƒã€‘
  â€¢ å±‚çº§: {result.tier} ({TIER_NAMES[result.tier]}) 
  â€¢ å±‚çº§æ¦‚ç‡: {result.tier_probabilities}
  â€¢ ç½®ä¿¡åº¦: {result.tier_confidence:.0%}
  â€¢ ç”Ÿå‘½å‘¨æœŸ: {result.lifecycle}
  â€¢ å½“å‰çŠ¶æ€: {result.vitality}
  â€¢ ç»¼åˆè¯„åˆ†: {result.health_score}/100 ({result.health_grade})

{'â”€'*70}

ã€GMMæ¦‚ç‡åŒ–åˆ†å±‚ã€‘
"""
        for tier, prob in result.tier_probabilities.items():
            bar_length = int(prob * 30)
            bar = 'â–ˆ' * bar_length + 'â–‘' * (30 - bar_length)
            report += f"  {tier:10s} {bar} {prob:.1%}\n"

        report += f"""
{'â”€'*70}

ã€Prophetæ—¶åºé¢„æµ‹ã€‘
"""
        if result.predictions.get('summary'):
            summary = result.predictions['summary']
            report += f"""
  â€¢ å½“å‰OpenRank: {summary['current_value']:.1f}
  â€¢ 6ä¸ªæœˆé¢„æµ‹å€¼: {summary['forecast_6m']:.1f}
  â€¢ é¢„æœŸå¢é•¿: {summary['growth_pct']:.1f}% ({summary['trend']})
"""
        
        report += f"""
{'â”€'*70}

ã€å›æµ‹éªŒè¯ç»“æœã€‘
"""
        for metric, bt_result in result.backtest_results.items():
            if isinstance(bt_result, dict) and 'mape' in bt_result:
                report += f"""
  {metric.upper()}:
    â€¢ MAPEè¯¯å·®: {bt_result['mape']:.1f}%
    â€¢ æ–¹å‘å‡†ç¡®æ€§: {bt_result.get('direction_accuracy', 0):.1%}
    â€¢ ç½®ä¿¡åº¦: {bt_result.get('confidence_level', 'N/A')}
"""
        
        if 'overall_confidence_level' in result.backtest_results:
            report += f"""
  â€¢ ç»¼åˆç½®ä¿¡åº¦: {result.backtest_results['overall_confidence_level']}
"""
        
        # æ·»åŠ é¢„æµ‹éªŒè¯ç»“æœ
        report += f"""
{'â”€'*70}

ã€é¢„æµ‹éªŒè¯ç»“æœã€‘
"""
        if result.prediction_validation and 'error' not in result.prediction_validation:
            report += f"""
  â€¢ ç»¼åˆè¯„åˆ†: {result.prediction_validation.get('overall_score', 0):.1f}
  â€¢ ç»¼åˆè¯„çº§: {result.prediction_validation.get('overall_rating', 'UNKNOWN')}
"""
            
            # æ·»åŠ å„æŒ‡æ ‡çš„é¢„æµ‹éªŒè¯ç»“æœ
            for metric, validation in result.prediction_validation.get('metrics', {}).items():
                report += f"""
  {metric.upper()}:
    â€¢ æœˆåº¦é¢„æµ‹å€¼: {validation.get('monthly_prediction', 0):.1f}
    â€¢ GitHub 30å¤©å®é™…å€¼: {validation.get('github_30d_value', 0):.1f}
    â€¢ æ—¥å‡é¢„æµ‹å€¼: {validation.get('daily_predicted', 0):.1f}
    â€¢ GitHub 30å¤©æ—¥å‡å®é™…å€¼: {validation.get('github_daily_avg', 0):.1f}
    â€¢ è¯¯å·®ç™¾åˆ†æ¯”: {validation.get('error_pct', 0):.1f}%
"""
        else:
            report += f"""
  â€¢ é¢„æµ‹éªŒè¯: {result.prediction_validation.get('error', 'æœªæ‰§è¡Œ')}
"""
        
        report += f"""
{'â”€'*70}

ã€AHPå¥åº·è¯„ä¼°ã€‘
  è¯„åˆ†æƒé‡çŸ©é˜µ ({result.tier}):
"""
        weights = self.ahp_evaluator.TIER_WEIGHTS.get(result.tier, {})
        for dim, weight in weights.items():
            report += f"    â€¢ {dim}: {weight:.0%}\n"
        
        report += f"""
  å„ç»´åº¦è´¡çŒ®:
"""
        for dim, contrib in result.dimension_scores.items():
            report += f"    â€¢ {dim}: {contrib:.1%}\n"
        
        report += f"""
{'â”€'*70}

ã€ä¸‰ç»´è¶‹åŠ¿åˆ†æã€‘
  è¶‹åŠ¿è¯„åˆ†: {result.trend_3d['trend_score']} ({result.trend_3d['description']})
  
  â”Œ åŠ¨é‡ (Momentum): {result.momentum_analysis['total']}/100
  â”‚   {result.momentum_analysis['interpretation']}
  â”‚
  â”œ é˜»åŠ› (Resistance): {result.resistance_analysis['total']}/100
  â”‚   {result.resistance_analysis['interpretation']}
  â”‚
  â”” æ½œåŠ› (Potential): {result.potential_analysis['remaining_space']:.1f}% æˆé•¿ç©ºé—´
      {result.potential_analysis['interpretation']}

{'â”€'*70}

ã€Bus Factor 2.0 åˆ†æã€‘
  ç­‰æ•ˆ Bus Factor: {result.bus_factor_2['effective_bus_factor']:.1f}
  {result.bus_factor_2['description']}

{'â”€'*70}

ã€ETD å¯¿å‘½åˆ†æã€‘
  çŠ¶æ€: {result.etd_analysis['etd_status']}
  é¢„è®¡æ¯ç«­æ—¶é—´: {f"{result.etd_analysis['etd_months']:.1f} ä¸ªæœˆ" if result.etd_analysis['etd_months'] != float('inf') else 'æ— é™ (å¥åº·)'}
  {result.etd_analysis.get('description', '')}

{'â”€'*70}

ã€æ”¹è¿›å»ºè®®ã€‘
"""
        for i, rec in enumerate(result.recommendations, 1):
            report += f"  {i}. {rec}\n"
        
        report += f"""
{'â•'*70}
                         æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'â•'*70}
"""
        return report
    
    def _save_json_report(self, result: AnalysisResult):
        """ä¿å­˜ JSON æ ¼å¼æŠ¥å‘Š"""
        json_data = {
            'project': result.project_name,
            'generated_at': datetime.now().isoformat(),
            'tier': {
                'level': result.tier,
                'name': TIER_NAMES[result.tier],
                'probabilities': result.tier_probabilities,
                'confidence': result.tier_confidence
            },
            'predictions': result.predictions,
            'backtest': result.backtest_results,
            'prediction_validation': result.prediction_validation,
            'health': {
                'score': result.health_score,
                'grade': result.health_grade,
                'dimensions': result.dimension_scores
            },
            'trend_3d': {
                'score': result.trend_3d['trend_score'],
                'class': result.trend_3d['trend_class'],
                'momentum': result.momentum_analysis,
                'resistance': result.resistance_analysis,
                'potential': result.potential_analysis
            },
            'etd_analysis': {
                'months': result.etd_analysis['etd_months'] if result.etd_analysis['etd_months'] != float('inf') else 'infinite',
                'status': result.etd_analysis['etd_status'],
                'decay_type': result.etd_analysis.get('decay_type'),
                'is_mature_stable': result.etd_analysis.get('is_mature_stable'),
                'description': result.etd_analysis.get('description')
            },
            'bus_factor': result.bus_factor_2,
            'risk': result.risk_analysis,
            'dark_horse': result.dark_horse_analysis,
            'change_points': result.change_points,
            'github_comparison': result.github_comparison,
            'recommendations': result.recommendations
        }
        
        json_file = f"{self.org}_{self.repo}_v45_data.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š JSON æ•°æ®å·²ä¿å­˜: {json_file}")
    
    def _plot_github_prediction_comparison(self, ax, result: AnalysisResult):
        """
        ç»˜åˆ¶GitHubè¿‘30å¤©æ•°æ®ä¸ç®—æ³•é¢„æµ‹çš„å¯¹æ¯”å›¾
        æ¯”è¾ƒæŒ‡æ ‡ï¼šopenrankå€¼ã€attentionå€¼ä»¥åŠstarså€¼
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰GitHubéªŒè¯æ•°æ®
        if not result.conclusion_validation or 'error' in result.conclusion_validation:
            ax.text(0.5, 0.5, 'æ— éªŒè¯æ•°æ®', ha='center', va='center')
            return
        
        val = result.conclusion_validation
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æµ‹æ•°æ®
        if not result.predictions or not any('forecast' in v for v in result.predictions.values()):
            ax.text(0.5, 0.5, 'æ— é¢„æµ‹æ•°æ®', ha='center', va='center')
            return
        
        # å®šä¹‰è¦æ¯”è¾ƒçš„æŒ‡æ ‡
        metrics = ['openrank', 'attention', 'stars']
        labels = ['OpenRank', 'Attention', 'Stars']
        
        # åˆ›å»ºGitHub APIæ•°æ®è·å–å™¨
        if self.github_token:
            gh_analyzer = GitHubAPIAnalyzer(self.github_token)
            # è·å–æœ€è¿‘30å¤©çš„æ´»è·ƒæ•°æ®
            github_recent = gh_analyzer.fetch_recent_activity(self.org, self.repo, days=30)
            
            if 'error' not in github_recent:
                # è®¡ç®—GitHub APIçš„æ´»è·ƒåº¦å¾—åˆ†ï¼ˆæ˜ å°„åˆ°OpenRankå’ŒAttentionï¼‰
                activity_score = gh_analyzer.calculate_activity_score(github_recent)
                
                # è·å–OpenDiggerçš„æœˆåº¦é¢„æµ‹å€¼
                predictions = {
                    'openrank': result.predictions.get('openrank', {}).get('forecast', [0])[0] if 'openrank' in result.predictions else 0,
                    'attention': result.predictions.get('attention', {}).get('forecast', [0])[0] if 'attention' in result.predictions else 0,
                    'stars': result.predictions.get('stars', {}).get('forecast', [0])[0] if 'stars' in result.predictions else 0
                }
                
                # å°†OpenDiggerçš„æœˆåº¦é¢„æµ‹å€¼æ˜ å°„åˆ°GitHub APIçš„30å¤©æ•°æ®
                mapped_predictions = {
                    'openrank': gh_analyzer.map_monthly_prediction_to_daily_github_data(predictions['openrank'], github_recent, 'openrank')['github_30d_value'],
                    'attention': gh_analyzer.map_monthly_prediction_to_daily_github_data(predictions['attention'], github_recent, 'attention')['github_30d_value'],
                    'stars': 0  # éœ€è¦è·å–30å¤©å‰çš„starsæ•°æ‰èƒ½è®¡ç®—å¢é‡
                }
                
                # è·å–GitHub APIçš„å®é™…æ•°æ®
                github_values = {
                    'openrank': activity_score,  # ä½¿ç”¨æ´»è·ƒåº¦å¾—åˆ†ä½œä¸ºGitHub APIçš„OpenRankè¿‘ä¼¼å€¼
                    'attention': activity_score * 1.5,  # ä½¿ç”¨æ´»è·ƒåº¦å¾—åˆ†çš„1.5å€ä½œä¸ºGitHub APIçš„Attentionè¿‘ä¼¼å€¼
                    'stars': 0  # éœ€è¦è·å–30å¤©å‰çš„starsæ•°æ‰èƒ½è®¡ç®—å¢é‡
                }
                
                # å‡†å¤‡æ•°æ®ç”¨äºç»˜åˆ¶
                github_data = [github_values[m] for m in metrics]
                predicted_data = [mapped_predictions[m] for m in metrics]
                
                # åˆ›å»ºå¯¹æ¯”æŸ±çŠ¶å›¾
                x = np.arange(len(metrics))
                width = 0.35
                
                # ç»˜åˆ¶GitHub APIå®é™…æ•°æ®
                bars1 = ax.bar(x - width/2, github_data, width, label='GitHub 30å¤©',
                            color=COLORS['primary'], alpha=0.7)
                # ç»˜åˆ¶ç®—æ³•é¢„æµ‹æ•°æ®
                bars2 = ax.bar(x + width/2, predicted_data, width, label='ç®—æ³•é¢„æµ‹',
                            color=COLORS['success'], alpha=0.7)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bars in [bars1, bars2]:
                    for bar in bars:
                        height = bar.get_height()
                        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                                f'{height:.1f}', ha='center', va='bottom', fontsize=8)
                
                # æ·»åŠ å›¾ä¾‹
                ax.legend(loc='upper right', fontsize=8)
                
                # è®¾ç½®è½´æ ‡ç­¾å’Œæ ‡é¢˜
                ax.set_xlabel('æŒ‡æ ‡', fontsize=9)
                ax.set_ylabel('æ•°å€¼', fontsize=9)
                ax.set_title('GitHubè¿‘30å¤©æ•°æ® vs ç®—æ³•é¢„æµ‹', fontsize=10, fontweight='bold')
                ax.set_xticks(x)
                ax.set_xticklabels(labels, rotation=0)
                ax.grid(True, axis='y', alpha=0.3)
            else:
                ax.text(0.5, 0.5, 'æ— æ³•è·å–GitHub APIæ•°æ®', ha='center', va='center')
        else:
            ax.text(0.5, 0.5, 'æœªæä¾›GitHub Token', ha='center', va='center')
    
    def plot_dashboard(self, result: AnalysisResult):
        """ç»˜åˆ¶ 3x3 ä¹å®«æ ¼ä»ªè¡¨æ¿ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        from matplotlib.gridspec import GridSpec
        
        fig = plt.figure(figsize=(16, 13))
        fig.patch.set_facecolor('white')
        
        # ä½¿ç”¨ GridSpec åˆ›å»ºæ··åˆå¸ƒå±€
        gs = GridSpec(3, 3, figure=fig, hspace=0.5, wspace=0.30,
                     left=0.06, right=0.96, top=0.90, bottom=0.06)
        
        # å®šä¹‰å­å›¾
        ax00 = fig.add_subplot(gs[0, 0])
        ax01 = fig.add_subplot(gs[0, 1])
        ax02 = fig.add_subplot(gs[0, 2])
        
        ax10 = fig.add_subplot(gs[1, 0], polar=True)
        ax11 = fig.add_subplot(gs[1, 1])
        ax12 = fig.add_subplot(gs[1, 2])
        
        ax20 = fig.add_subplot(gs[2, 0])
        ax21 = fig.add_subplot(gs[2, 1])
        ax22 = fig.add_subplot(gs[2, 2])
        
        # ç»‘å®šç»‘å›¾
        self._plot_openrank_with_forecast(ax00, result)
        self._plot_gmm_probabilities(ax01, result)
        self._plot_health_gauge(ax02, result)
        
        self._plot_trend_3d_radar(ax10, result)
        self._plot_resistance_chart(ax11, result)
        self._plot_potential_bar(ax12, result)
        
        self._plot_predictions_chart(ax20, result)
        self._plot_backtest_results(ax21, result)
        self._plot_github_prediction_comparison(ax22, result)
        
        # æ€»æ ‡é¢˜
        fig.suptitle(
            f'{self.org}/{self.repo} æ·±åº¦è¯Šæ–­ | {TIER_NAMES[result.tier]} | '
            f'{result.health_grade} ({result.health_score}åˆ†)',
            fontsize=14, fontweight='bold', y=0.98
        )
        
        filename = f"{self.org}_{self.repo}_v45_report.png"
        plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜: {filename}")
        plt.show()
    
    def _plot_openrank_with_forecast(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶OpenRankè¶‹åŠ¿åŠé¢„æµ‹"""
        if 'openrank' not in self.df:
            ax.text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center')
            return
        
        data = self.df['openrank']
        ax.plot(data.index, data.values, color=COLORS['primary'], lw=2, label='å†å²')
        
        # ç»˜åˆ¶é¢„æµ‹ï¼ˆå¦‚æœæœ‰ï¼‰
        if result.predictions.get('openrank', {}).get('forecast'):
            forecast = result.predictions['openrank']['forecast']
            last_date = data.index[-1]
            
            # ç”Ÿæˆæœªæ¥æ—¥æœŸ
            future_dates = pd.date_range(
                start=last_date + pd.DateOffset(months=1),
                periods=len(forecast),
                freq='ME'
            )
            
            ax.plot(future_dates, forecast, color=COLORS['success'], 
                   lw=2, linestyle='--', label='é¢„æµ‹')
            
            # ç»˜åˆ¶ç½®ä¿¡åŒºé—´
            if 'yhat_lower' in result.predictions['openrank']:
                lower = result.predictions['openrank']['yhat_lower']
                upper = result.predictions['openrank']['yhat_upper']
                ax.fill_between(future_dates, lower, upper, 
                              color=COLORS['success'], alpha=0.2)
        
        ax.set_title('OpenRankè¶‹åŠ¿åŠé¢„æµ‹', fontsize=10, fontweight='bold')
        ax.tick_params(axis='x', rotation=30)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=8)
    
    def _plot_gmm_probabilities(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶GMMåˆ†å±‚æ¦‚ç‡"""
        tiers = list(result.tier_probabilities.keys())
        probs = list(result.tier_probabilities.values())
        colors = [TIER_BENCHMARKS[t].get('color', COLORS['primary']) for t in tiers]
        
        bars = ax.bar(tiers, probs, color=colors, alpha=0.8)
        ax.set_ylim(0, 1)
        ax.set_ylabel('æ¦‚ç‡', fontsize=9)
        ax.set_title(f'GMMåˆ†å±‚æ¦‚ç‡ (æœ€ä½³: {result.tier})', fontsize=10, fontweight='bold')
        
        # åœ¨æŸ±å­å†…éƒ¨æ·»åŠ æ¦‚ç‡æ ‡ç­¾
        for bar, prob in zip(bars, probs):
            height = bar.get_height()
            # æ ¹æ®æŸ±å­é«˜åº¦è°ƒæ•´æ ‡ç­¾ä½ç½®å’Œé¢œè‰²ï¼Œç¡®ä¿æ¸…æ™°å¯è§
            y_pos = height / 2  # æ ‡ç­¾å±…ä¸­æ˜¾ç¤º
            color = 'white' if height > 0.2 else 'black'  # é«˜æŸ±å­ç”¨ç™½è‰²æ ‡ç­¾ï¼Œä½æŸ±å­ç”¨é»‘è‰²æ ‡ç­¾
            ax.text(bar.get_x() + bar.get_width()/2., y_pos,
                   f'{prob:.1%}', ha='center', va='center', fontsize=9, fontweight='bold', color=color)
    
    def _plot_health_gauge(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶å¥åº·è¯„åˆ†ä»ªè¡¨ç›˜"""
        score = result.health_score
    
        # æ¸…ç©ºåæ ‡è½´
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 0.7)  # è°ƒæ•´é«˜åº¦ï¼Œä½¿ä»ªè¡¨ç›˜æ›´æ¥è¿‘åŠåœ†å½¢
        
        # ç»˜åˆ¶ä»ªè¡¨ç›˜å¤–æ¡†
        center_x, center_y = 0.5, 0.35  # è°ƒæ•´ä¸­å¿ƒç‚¹ï¼Œä½¿åŠåœ†å½¢å±…ä¸­
        radius = 0.35  # è°ƒæ•´åŠå¾„
        
        # ç»˜åˆ¶å½©è‰²æ‰‡å½¢åŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸ60åº¦
        total_angle = np.pi  # åŠåœ†ï¼Œ180åº¦
        sector_angle = np.pi / 3  # 60åº¦
        
        # å±é™©åŒºåŸŸ (0-33.3åˆ†) - ä½¿ç”¨fillå‡½æ•°ç»˜åˆ¶ä¸¥æ ¼çš„60åº¦æ‰‡å½¢
        danger_angles = np.linspace(np.pi, np.pi - sector_angle, 50)
        # åˆ›å»ºæ‰‡å½¢è·¯å¾„ï¼šä¸­å¿ƒç‚¹ -> æ‰‡å½¢è¾¹ç¼˜ç‚¹ -> ä¸­å¿ƒç‚¹
        danger_x = [center_x] + list(center_x + radius * np.cos(danger_angles)) + [center_x]
        danger_y = [center_y] + list(center_y + radius * np.sin(danger_angles)) + [center_y]
        ax.fill(danger_x, danger_y, color=COLORS['danger'], alpha=0.3)
        
        # è­¦å‘ŠåŒºåŸŸ (33.3-66.6åˆ†) - ä½¿ç”¨fillå‡½æ•°ç»˜åˆ¶ä¸¥æ ¼çš„60åº¦æ‰‡å½¢
        warning_angles = np.linspace(np.pi - sector_angle, np.pi - 2*sector_angle, 50)
        warning_x = [center_x] + list(center_x + radius * np.cos(warning_angles)) + [center_x]
        warning_y = [center_y] + list(center_y + radius * np.sin(warning_angles)) + [center_y]
        ax.fill(warning_x, warning_y, color=COLORS['warning'], alpha=0.3)
        
        # å®‰å…¨åŒºåŸŸ (66.6-100åˆ†) - ä½¿ç”¨fillå‡½æ•°ç»˜åˆ¶ä¸¥æ ¼çš„60åº¦æ‰‡å½¢
        safe_angles = np.linspace(np.pi - 2*sector_angle, np.pi - 3*sector_angle, 50)
        safe_x = [center_x] + list(center_x + radius * np.cos(safe_angles)) + [center_x]
        safe_y = [center_y] + list(center_y + radius * np.sin(safe_angles)) + [center_y]
        ax.fill(safe_x, safe_y, color=COLORS['success'], alpha=0.3)
        
        # ç»˜åˆ¶åˆ»åº¦çº¿
        for i in range(0, 101, 10):
            angle = np.pi - np.pi * (i / 100)
            x_start = center_x + radius * np.cos(angle)
            y_start = center_y + radius * np.sin(angle)
            x_end = center_x + (radius - 0.05) * np.cos(angle)
            y_end = center_y + (radius - 0.05) * np.sin(angle)
            ax.plot([x_start, x_end], [y_start, y_end], 'k-', lw=1)
            
            # æ¯20åˆ†æ·»åŠ æ ‡ç­¾
            if i % 20 == 0:
                x_text = center_x + (radius + 0.05) * np.cos(angle)
                y_text = center_y + (radius + 0.05) * np.sin(angle)
                ax.text(x_text, y_text, str(i), ha='center', va='center', fontsize=8)
        
        # ç»˜åˆ¶æŒ‡é’ˆ
        angle = np.pi - (score / 100) * total_angle  # ä»å³åˆ°å·¦è®¡ç®—è§’åº¦
        needle_length = radius - 0.05
        
        # æŒ‡é’ˆä¸‰è§’å½¢ï¼Œç¡®ä¿æŒ‡å‘æ­£ç¡®æ–¹å‘
        x_tip = center_x + needle_length * np.cos(angle)
        y_tip = center_y + needle_length * np.sin(angle)
        
        # æŒ‡é’ˆåº•éƒ¨çš„ä¸¤ä¸ªç‚¹
        angle_left = angle + 0.1  # è°ƒæ•´è§’åº¦ï¼Œç¡®ä¿æŒ‡é’ˆå½¢çŠ¶æ­£ç¡®
        angle_right = angle - 0.1
        x_left = center_x + 0.05 * np.cos(angle_left)
        y_left = center_y + 0.05 * np.sin(angle_left)
        x_right = center_x + 0.05 * np.cos(angle_right)
        y_right = center_y + 0.05 * np.sin(angle_right)
        
        # ç»˜åˆ¶æŒ‡é’ˆ
        ax.fill([x_tip, x_left, x_right], [y_tip, y_left, y_right], color='black')
        
        # ä¸­å¿ƒåœ†ç‚¹
        ax.add_patch(plt.Circle((center_x, center_y), 0.02, color='black'))
        
        # æ·»åŠ åˆ†æ•°å’Œç­‰çº§
        ax.text(center_x, center_y - 0.1, f'{score:.0f}/100', 
                ha='center', va='center', fontsize=16, fontweight='bold')
        ax.text(center_x, center_y - 0.18, result.health_grade, 
                ha='center', va='center', fontsize=12, fontweight='bold')
        
        # æ·»åŠ å›¾ä¾‹ï¼Œå¢å¤§æ–‡å­—é—´éš”ï¼Œé¿å…é‡å 
        ax.text(0.25, 0.05, 'å±é™© (0-33.3)', color=COLORS['danger'], 
                fontsize=8, ha='center', va='center')
        ax.text(0.5, 0.05, 'è­¦å‘Š (33.3-66.6)', color=COLORS['warning'], 
                fontsize=8, ha='center', va='center')
        ax.text(0.75, 0.05, 'å®‰å…¨ (66.6-100)', color=COLORS['success'], 
                fontsize=8, ha='center', va='center')
        
        ax.set_aspect('equal')  # è®¾ç½®ç­‰å®½é«˜æ¯”ï¼Œç¡®ä¿ä»ªè¡¨ç›˜ä¸ºçœŸæ­£çš„åŠåœ†å½¢
        ax.axis('off')
        ax.set_title('å¥åº·è¯„åˆ†ä»ªè¡¨ç›˜', fontsize=10, fontweight='bold', pad=10)

    def _plot_trend_3d_radar(self, ax, result: AnalysisResult):
        """æ”¹è¿›ç‰ˆä¸‰ç»´è¶‹åŠ¿é›·è¾¾å›¾"""
        categories = ['åŠ¨é‡', 'ç¨³å®šæ€§', 'æ½œåŠ›']
        values = [
            result.momentum_analysis['total'],
            100 - result.resistance_analysis['total'],
            result.potential_analysis['remaining_space']
        ]
        values = values + [values[0]]  # é—­åˆé›·è¾¾å›¾
        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
        angles += [angles[0]]
        
        # è®¾ç½®æåæ ‡
        ax.set_theta_offset(np.pi / 2)
        ax.set_theta_direction(-1)
        
        # ä½¿ç”¨é»˜è®¤é›·è¾¾ç½‘æ ¼ï¼Œè°ƒæ•´æ ·å¼
        ax.grid(True, linestyle='-', linewidth=0.5, alpha=0.7, color='gray')
        
        # è®¾ç½®ç½‘æ ¼çº¿æ•°é‡å’Œæ ·å¼
        ax.set_rticks([20, 40, 60, 80, 100])
        ax.set_yticklabels(['20', '40', '60', '80', '100'], fontsize=7)
        
        # ç»˜åˆ¶æ•°æ®çº¿ï¼Œä»…ä¿ç•™çº¿æ¡å’Œæ•°æ®ç‚¹ï¼Œæ— å¡«å……
        ax.plot(angles, values, 'o-', color=COLORS['primary'], linewidth=3, markersize=6, markerfacecolor='white')
        
        # è®¾ç½®åˆ»åº¦æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=10, fontweight='bold')
        
        # åœ¨æ¯ä¸ªæ•°æ®ç‚¹æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (angle, value) in enumerate(zip(angles[:-1], values[:-1])):
            # ç¨å¾®åç§»ä»¥é¿å…ä¸æ ‡ç­¾é‡å 
            label_angle = angle
            label_radius = value + 5
            ax.text(label_angle, label_radius, f'{value:.0f}', 
                    ha='center', va='center', fontsize=9, fontweight='bold', 
                    color=COLORS['primary'], bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
        
        ax.set_ylim(0, 100)
        
        ax.set_title(f'ä¸‰ç»´è¶‹åŠ¿åˆ†æ ({result.trend_3d["trend_class"]})', 
                    fontsize=10, fontweight='bold', pad=20)
                        
    def _plot_resistance_chart(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶é˜»åŠ›åˆ†æ"""
        categories = ['æŠ€æœ¯å€º', 'ä»£ç ç†µ', 'Issueå‹åŠ›']
        values = [
            result.resistance_analysis['debt'],
            result.resistance_analysis['entropy'],
            result.resistance_analysis['issue_pressure']
        ]
        colors = [COLORS['danger'] if v > 60 else COLORS['warning'] if v > 40 else COLORS['success'] for v in values]
        bars = ax.barh(categories, values, color=colors, alpha=0.8)
        ax.axvline(50, color='gray', linestyle='--', lw=1)
        ax.set_xlim(0, 100)
        ax.set_title(f'é˜»åŠ›åˆ†æ ({result.resistance_analysis["status"]})', fontsize=10, fontweight='bold')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                   f'{value:.1f}', ha='left', va='center', fontsize=9, fontweight='bold')
    
    def _plot_potential_bar(self, ax, result: AnalysisResult):
        """æ”¹è¿›ç‰ˆå¢é•¿æ½œåŠ›å›¾"""
        current = result.potential_analysis['current_position']
        ceiling = result.potential_analysis['growth_ceiling']
        remaining = result.potential_analysis['remaining_space']
        
        # è·å–å†å²å³°å€¼
        historical_max = self.df['openrank'].max() if 'openrank' in self.df else current
        
        # åˆ›å»ºåˆ†ç»„æŸ±çŠ¶å›¾
        categories = ['å½“å‰ä½ç½®', 'å†å²å³°å€¼', 'å¢é•¿ä¸Šé™']
        values = [
            current,
            historical_max,
            min(ceiling, historical_max * 2)  # é™åˆ¶ä¸Šé™ä¸è¶…è¿‡å†å²å³°å€¼çš„2å€
        ]
        
        # ä½¿ç”¨ä¸åŒé¢œè‰²
        colors = [COLORS['primary'], COLORS['warning'], COLORS['success']]
        
        bars = ax.bar(categories, values, color=colors, alpha=0.8, width=0.6)
        
        # åœ¨æ¯ä¸ªæŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + max(values)*0.02,
                    f'{value:.1f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
        
        # åœ¨é¡¶éƒ¨æ·»åŠ å‰©ä½™ç©ºé—´ç™¾åˆ†æ¯”
        ax.text(0.5, 1.05, f'å‰©ä½™å¢é•¿ç©ºé—´: {remaining:.0f}%', 
                transform=ax.transAxes, ha='center', fontsize=10, fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor=COLORS['info'], alpha=0.2))
        
        # æ·»åŠ è§£é‡Šæ–‡æœ¬
        ax.text(0.02, 0.95, f"è§£é‡Š: {result.potential_analysis['interpretation']}", 
                transform=ax.transAxes, fontsize=8, va='top',
                bbox=dict(boxstyle="round,pad=0.2", facecolor='lightgray', alpha=0.3))
        
        ax.set_ylim(0, max(values) * 1.2)
        ax.set_ylabel('OpenRank', fontsize=9)
        # å°†æ ‡é¢˜ä¸­çš„"ä¿å®ˆä¼°è®¡"æ›¿æ¢ä¸ºå®é™…çš„å‰©ä½™å¢é•¿ç©ºé—´æ•°æ®
        ax.set_title(f'å¢é•¿æ½œåŠ›åˆ†æï¼ˆå‰©ä½™å¢é•¿ç©ºé—´: {remaining:.0f}%ï¼‰', fontsize=10, fontweight='bold')
        ax.grid(True, axis='y', alpha=0.3)
    
    def _plot_predictions_chart(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶é¢„æµ‹å›¾è¡¨"""
        if 'openrank' not in self.df or not result.predictions.get('openrank', {}).get('forecast'):
            ax.text(0.5, 0.5, 'æ— é¢„æµ‹æ•°æ®', ha='center', va='center')
            return
        
        # è·å–å†å²æ•°æ®
        hist_data = self.df['openrank'].tail(12).reset_index(drop=True)
        hist_dates = list(range(len(hist_data)))
        
        # è·å–é¢„æµ‹æ•°æ®
        forecast = result.predictions['openrank']['forecast']
        forecast_dates = list(range(len(hist_data), len(hist_data) + len(forecast)))
        
        # ç»˜åˆ¶å†å²æ•°æ®
        ax.plot(hist_dates, hist_data.values, 'o-', color=COLORS['primary'], 
                linewidth=2, markersize=4, label='å†å²æ•°æ®')
        
        # ç»˜åˆ¶é¢„æµ‹æ•°æ®
        ax.plot(forecast_dates, forecast, 's--', color=COLORS['success'], 
                linewidth=2, markersize=4, label='é¢„æµ‹æ•°æ®')
        
        # ç»˜åˆ¶ç½®ä¿¡åŒºé—´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'yhat_lower' in result.predictions['openrank']:
            lower = result.predictions['openrank']['yhat_lower']
            upper = result.predictions['openrank']['yhat_upper']
            ax.fill_between(forecast_dates, lower, upper, 
                        color=COLORS['success'], alpha=0.2, label='95%ç½®ä¿¡åŒºé—´')
        
        # æ·»åŠ å‚ç›´çº¿åˆ†éš”å†å²å’Œé¢„æµ‹
        ax.axvline(x=len(hist_data)-0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)
        ax.text(len(hist_data)-0.5, ax.get_ylim()[1]*0.9, 'å†å²/é¢„æµ‹åˆ†ç•Œ', 
                fontsize=8, rotation=90, va='top', ha='right')
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if result.predictions.get('summary'):
            summary = result.predictions['summary']
            textstr = f"å½“å‰å€¼: {summary['current_value']:.1f}\né¢„æµ‹å€¼: {summary['forecast_6m']:.1f}\nå¢é•¿: {summary['growth_pct']:.1f}%"
            ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        ax.set_xlabel('æ—¶é—´æ­¥', fontsize=9)
        ax.set_ylabel('OpenRank', fontsize=9)
        ax.set_title('Prophetæ—¶åºé¢„æµ‹å¯¹æ¯”', fontsize=10, fontweight='bold')
        ax.legend(loc='upper left', fontsize=8)
        ax.grid(True, alpha=0.3)
    
    def _plot_backtest_results(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶å›æµ‹ç»“æœ"""
        if not result.backtest_results:
            ax.text(0.5, 0.5, 'æ— å›æµ‹æ•°æ®', ha='center', va='center')
            return
        
        # æå–å…³é”®æŒ‡æ ‡
        metrics = []
        mape_values = []
        direction_acc = []
        
        for metric, bt_result in result.backtest_results.items():
            if isinstance(bt_result, dict) and 'mape' in bt_result:
                metrics.append(metric.upper())
                mape_values.append(bt_result['mape'])
                direction_acc.append(bt_result.get('direction_accuracy', 0) * 100)
        
        if not metrics:
            ax.text(0.5, 0.5, 'æ— æœ‰æ•ˆå›æµ‹æŒ‡æ ‡', ha='center', va='center')
            return
        
        x = np.arange(len(metrics))
        width = 0.35
        
        # ç»˜åˆ¶åŒæŸ±çŠ¶å›¾
        bars1 = ax.bar(x - width/2, mape_values, width, label='MAPEè¯¯å·®(%)', 
                    color=COLORS['danger'], alpha=0.7)
        bars2 = ax.bar(x + width/2, direction_acc, width, label='æ–¹å‘å‡†ç¡®æ€§(%)', 
                    color=COLORS['success'], alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{height:.1f}', ha='center', va='bottom', fontsize=8)
        
        # ä»å›æµ‹ç»“æœä¸­è·å–ç½®ä¿¡åº¦
        conf_level = 'UNKNOWN'
        if 'overall_confidence_level' in result.backtest_results:
            conf_level = result.backtest_results['overall_confidence_level']
        
        ax.set_xlabel('æŒ‡æ ‡', fontsize=9)
        ax.set_ylabel('ç™¾åˆ†æ¯”(%)', fontsize=9)
        # å°†ç½®ä¿¡åº¦ä¿¡æ¯æ·»åŠ åˆ°æ ‡é¢˜ä¸­
        ax.set_title(f'å›æµ‹éªŒè¯æŒ‡æ ‡å¯¹æ¯” ({conf_level})', fontsize=10, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(metrics)
        ax.legend(loc='upper right', fontsize=8)
        ax.grid(True, axis='y', alpha=0.3)

    def _plot_validation_result(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶ç»“è®ºéªŒè¯ç»“æœ"""
        """GitHubéªŒè¯æ•°æ®å¯¹æ¯”å›¾"""
        if not result.conclusion_validation or 'error' in result.conclusion_validation:
            ax.text(0.5, 0.5, 'æ— éªŒè¯æ•°æ®', ha='center', va='center')
            return
        
        val = result.conclusion_validation
        summary = val.get('github_30d_summary', {})
        
        if not summary:
            ax.text(0.5, 0.5, 'æ— GitHub 30å¤©æ•°æ®', ha='center', va='center')
            return
        
        # æå–è¦å¯¹æ¯”çš„æŒ‡æ ‡
        metrics = ['commits', 'issues_opened', 'prs_opened', 'active_contributors']
        labels = ['Commits', 'Issues', 'PRs', 'æ´»è·ƒè´¡çŒ®è€…']
        values = [summary.get(m, 0) for m in metrics]
        
        # ä¸ºäº†æ›´å¥½æ˜¾ç¤ºï¼Œå¯¹å€¼è¿›è¡Œé€‚å½“ç¼©æ”¾
        max_val = max(values) if values else 1
        if max_val > 100:
            values = [v / (max_val/100) for v in values]
            ylabel = 'ç›¸å¯¹å€¼'
        else:
            ylabel = 'æ•°é‡'
        
        # åˆ›å»ºæŸ±çŠ¶å›¾
        x = np.arange(len(metrics))
        colors = [COLORS['primary'], COLORS['warning'], COLORS['info'], COLORS['success']]
        
        bars = ax.bar(x, values, color=colors, alpha=0.7, width=0.6)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, [summary.get(m, 0) for m in metrics]):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + max(values)*0.02,
                    f'{value}', ha='center', va='bottom', fontsize=8, fontweight='bold')
        
        # æ·»åŠ éªŒè¯ç»“æœæ ‡ç­¾
        if val.get('overall_valid'):
            status_color = COLORS['success']
            status_text = "éªŒè¯é€šè¿‡"
        else:
            status_color = COLORS['warning']
            status_text = "éœ€å¤æ ¸"
        
        ax.text(0.5, 0.95, f"ç»“è®ºéªŒè¯: {status_text}", transform=ax.transAxes,
                ha='center', fontsize=10, fontweight='bold', color=status_color,
                bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))
        
        # æ·»åŠ ç½®ä¿¡åº¦
        confidence = val.get('confidence', 0)
        ax.text(0.5, 0.85, f"ç½®ä¿¡åº¦: {confidence}%", transform=ax.transAxes,
                ha='center', fontsize=9,
                bbox=dict(boxstyle="round,pad=0.2", facecolor='lightgray', alpha=0.3))
        
        ax.set_xlabel('æŒ‡æ ‡', fontsize=9)
        ax.set_ylabel(ylabel, fontsize=9)
        ax.set_title('GitHub 30å¤©æ´»è·ƒæ•°æ®', fontsize=10, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(labels, rotation=0)
        ax.grid(True, axis='y', alpha=0.3)

# ============== ä¸»å…¥å£ ==============
if __name__ == "__main__":
    print("\n" + "="*60)
    print("  GitHub é¡¹ç›®æ·±åº¦åˆ†æå™¨ v4.5 - ç®—æ³•å‡çº§ç‰ˆ")
    print("  GMMåˆ†å±‚ + Propheté¢„æµ‹ + AHPè¯„ä¼° + å›æµ‹éªŒè¯")
    print("="*60 + "\n")
    
    url = input("è¯·è¾“å…¥ GitHub é¡¹ç›®åœ°å€: ").strip()
    if not url:
        url = "facebook/react"  # é»˜è®¤ç¤ºä¾‹
    
    # è¯¢é—® GitHub Tokenï¼ˆå¯é€‰ï¼‰
    token = input("è¯·è¾“å…¥ GitHub API Token (å¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡): ").strip()
    if not token:
        token = None
    
    analyzer = ProjectAnalyzerV45(url, github_token=token)
    result = analyzer.run()
    
    if result:
        print("\nâœ… åˆ†æå®Œæˆ!")
        print(f"   - å±‚çº§: {result.tier} ({TIER_NAMES[result.tier]})")
        print(f"   - å¥åº·è¯„åˆ†: {result.health_score}/100 ({result.health_grade})")
        print(f"   - é¢„æµ‹ç½®ä¿¡åº¦: {result.backtest_results.get('overall_confidence_level', 'N/A')}")
        print(f"   - æœ€ä½³é¢„æµ‹æŒ‡æ ‡: {max(result.tier_probabilities.items(), key=lambda x: x[1])[0]}")