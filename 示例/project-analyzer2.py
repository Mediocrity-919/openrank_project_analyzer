"""
GitHub é¡¹ç›®æ·±åº¦åˆ†æå™¨ v6.0 - æ•°æ®æµé‡æ„ç‰ˆ
==========================================
æ ¸å¿ƒé‡æ„ï¼š
1. OpenDigger åªç”¨äºè¶‹åŠ¿åˆ†æï¼ˆæœˆå¢é‡ï¼‰
2. GitHub API ç”¨äºé”šå®šç°å®ï¼ˆç»å¯¹å¿«ç…§ï¼‰
3. Prophet åªé¢„æµ‹è¶‹åŠ¿æ–¹å‘ï¼Œä¸é¢„æµ‹ç»å¯¹å€¼
4. åˆ†ç¦»ç»“æ„ã€æ—¶é—´ã€æ´»è·ƒçŠ¶æ€ï¼Œæ¶ˆé™¤é€»è¾‘å†²çª
"""

import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
import re
import os
import warnings
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional, Any
from scipy.stats import pearsonr, linregress
from scipy.optimize import curve_fit
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
import json
from datetime import datetime, timedelta
import itertools
from collections import defaultdict

# ============== æ˜¾ç¤ºè®¾ç½® ==============
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.facecolor'] = 'white'
plt.rcParams['axes.labelsize'] = 9
plt.rcParams['axes.titlesize'] = 10
plt.rcParams['xtick.labelsize'] = 8
plt.rcParams['ytick.labelsize'] = 8

# ============== é¢œè‰²ä¸»é¢˜ ==============
COLORS = {
    'primary': '#2E86AB',
    'success': '#28A745',
    'warning': '#FFC107',
    'danger': '#DC3545',
    'info': '#17A2B8',
    'secondary': '#6C757D',
    'purple': '#9B59B6',
    'orange': '#E67E22',
}

# ============== å±‚çº§åŸºå‡†æ•°æ® ==============
TIER_BENCHMARKS = {
    'GIANT': {'openrank': 80, 'stars': 50000, 'participants': 500, 'color': '#9B59B6'},
    'MATURE': {'openrank': 25, 'stars': 5000, 'participants': 100, 'color': '#3498DB'},
    'GROWING': {'openrank': 8, 'stars': 1000, 'participants': 30, 'color': '#2ECC71'},
    'EMERGING': {'openrank': 2, 'stars': 100, 'participants': 5, 'color': '#E67E22'}
}

TIER_NAMES = {
    'GIANT': 'å·¨å‹é¡¹ç›®',
    'MATURE': 'æˆç†Ÿé¡¹ç›®', 
    'GROWING': 'æˆé•¿é¡¹ç›®',
    'EMERGING': 'æ–°å…´é¡¹ç›®'
}

# ============== æ•°æ®ç±»å®šä¹‰ ==============
@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    project_name: str
    # ä¸‰å±‚çŠ¶æ€åˆ†ç¦»
    structural_tier: str  # GMMåˆ†ç±»çš„ç»“æ„å±‚çº§
    temporal_state: str   # æ—¶é—´è¶‹åŠ¿çŠ¶æ€
    activity_state: str   # æ´»è·ƒçŠ¶æ€
    # æ¦‚ç‡å’Œç½®ä¿¡åº¦
    tier_probabilities: Dict[str, float]
    tier_confidence: float
    # å¥åº·è¯„åˆ†
    health_score: float
    health_grade: str
    dimension_scores: Dict[str, float]
    # è¶‹åŠ¿åˆ†æ
    trend_analysis: Dict
    # é£é™©è¯„ä¼°
    risk_analysis: Dict
    # ç‰¹æ®Šåˆ†æ
    bus_factor_2: Dict
    etd_analysis: Dict
    dark_horse_analysis: Dict
    change_points: List[Dict]
    # æ•°æ®éªŒè¯
    github_comparison: Dict
    conclusion_validation: Dict
    # é¢„æµ‹ç»“æœï¼ˆåªè¶‹åŠ¿ï¼‰
    trend_predictions: Dict
    backtest_results: Dict
    # å»ºè®®
    recommendations: List[str]
    detailed_report: str


# ============== æ ¸å¿ƒæ•°æ®æ‹†åˆ†å™¨ ==============
class DataReconciliation:
    """OpenDigger ä¸ GitHub æ•°æ®åè°ƒå™¨"""
    
    @staticmethod
    def split_od_trend_and_gh_snapshot(od_df: pd.DataFrame, gh_info: Dict, col: str) -> Dict:
        """
        OpenDiggerï¼šè¶‹åŠ¿ï¼ˆæœˆåº¦å˜åŒ–ï¼‰
        GitHub APIï¼šç°å®é”šç‚¹ï¼ˆå½“å‰å¿«ç…§ï¼‰
        
        è¿”å›å€¼ï¼š
        - monthlyï¼šæœˆåº¦å˜åŒ–
        - cumulativeï¼šç´¯ç§¯åˆ°å½“å‰ï¼ˆæ ¹æ®OpenDiggerï¼‰
        - github_snapshotï¼šGitHubå½“å‰å€¼
        - reconciliationï¼šä¸¤è€…ä¸€è‡´æ€§è¯„ä¼°
        """
        if col not in od_df.columns:
            return {
                "monthly": pd.Series([]),
                "cumulative": pd.Series([]),
                "github_snapshot": gh_info.get(col, 0),
                "reconciliation": "æ•°æ®ç¼ºå¤±"
            }
        
        # OpenDiggeræ•°æ®ï¼šæœˆåº¦å˜åŒ–
        monthly = od_df[col].dropna().astype(float)
        
        # ç´¯ç§¯å˜åŒ–ï¼ˆä»OpenDiggerçœ‹çš„æ€»å˜åŒ–ï¼‰
        cumulative = monthly.cumsum()
        
        # GitHubå¿«ç…§
        github_value = gh_info.get(col, None)
        
        # åè°ƒè¯„ä¼°
        if len(cumulative) > 0:
            od_current = cumulative.iloc[-1]
            reconciliation = {
                "od_current": float(od_current),
                "gh_current": float(github_value) if github_value else 0,
                "diff_pct": abs(od_current - (github_value or 0)) / ((github_value or 0) + 1) * 100
            }
        else:
            reconciliation = {"od_current": 0, "gh_current": 0, "diff_pct": 100}
        
        return {
            "monthly": monthly,
            "cumulative": cumulative,
            "github_snapshot": github_value,
            "reconciliation": reconciliation
        }
    
    @staticmethod
    def get_structural_metrics(od_df: pd.DataFrame, gh_info: Dict) -> Dict:
        """è·å–ç”¨äºç»“æ„å±‚çº§çš„æŒ‡æ ‡ï¼ˆåŸºäºGitHubå¿«ç…§ï¼‰"""
        # ä¼˜å…ˆä½¿ç”¨GitHubå¿«ç…§ï¼Œæ²¡æœ‰åˆ™ç”¨OpenDiggerç´¯ç§¯å€¼
        return {
            'avg_openrank': od_df['openrank'].mean() if 'openrank' in od_df.columns else 0,
            'total_stars': gh_info.get('stars', 0) or od_df['stars'].sum() if 'stars' in od_df.columns else 0,
            'max_participants': od_df['participants'].max() if 'participants' in od_df.columns else 0
        }


# ============== GMMæ¦‚ç‡åŒ–åˆ†å±‚åˆ†ç±»å™¨ ==============
class GMMTierClassifier:
    """é«˜æ–¯æ··åˆæ¨¡å‹åˆ†å±‚åˆ†ç±»å™¨"""
    
    def __init__(self, n_components=4):
        self.n_components = n_components
        self.gmm = None
        self.scaler = StandardScaler()
        self.tier_labels = ['GIANT', 'MATURE', 'GROWING', 'EMERGING']
        
    def _generate_synthetic_data(self) -> np.ndarray:
        """ç”ŸæˆåˆæˆåŸºå‡†æ•°æ®ç”¨äºè®­ç»ƒGMM"""
        synthetic_data = []
        
        for tier, benchmarks in TIER_BENCHMARKS.items():
            n_samples = 100
            
            for _ in range(n_samples):
                openrank = np.random.normal(benchmarks['openrank'], max(benchmarks['openrank'] * 0.3, 1))
                stars = np.random.normal(benchmarks['stars'], max(benchmarks['stars'] * 0.3, 100))
                participants = np.random.normal(benchmarks['participants'], max(benchmarks['participants'] * 0.3, 5))
                
                synthetic_data.append([
                    max(0.1, openrank),
                    max(10, stars),
                    max(1, participants)
                ])
        
        return np.array(synthetic_data)
    
    def fit(self):
        """è®­ç»ƒGMMæ¨¡å‹"""
        synthetic_data = self._generate_synthetic_data()
        scaled_data = self.scaler.fit_transform(synthetic_data)
        self.gmm = GaussianMixture(
            n_components=self.n_components,
            covariance_type='full',
            random_state=42
        )
        self.gmm.fit(scaled_data)
        return self
    
    def predict_proba(self, metrics: Dict) -> Tuple[str, Dict[str, float], float]:
        """é¢„æµ‹å±‚çº§æ¦‚ç‡"""
        if self.gmm is None:
            self.fit()
        
        feature = np.array([
            metrics.get('avg_openrank', 0),
            metrics.get('total_stars', 0),
            metrics.get('max_participants', 0)
        ]).reshape(1, -1)
        
        # å¤„ç†é›¶å€¼
        feature = np.maximum(feature, [0.1, 10, 1])
        
        scaled_feature = self.scaler.transform(feature)
        probabilities = self.gmm.predict_proba(scaled_feature)[0]
        
        # è®¡ç®—æ¯ä¸ªç»„ä»¶çš„ä¸­å¿ƒç‚¹
        centers = self.gmm.means_
        centers_original = centers * self.scaler.scale_ + self.scaler.mean_
        openrank_centers = centers_original[:, 0]
        
        # æŒ‰openrankä»å¤§åˆ°å°æ’åº
        sorted_indices = np.argsort(-openrank_centers)
        
        # åˆ†é…æ ‡ç­¾
        tier_probabilities = {}
        for idx, tier in enumerate(self.tier_labels):
            if idx < len(sorted_indices):
                comp_idx = sorted_indices[idx]
                tier_probabilities[tier] = probabilities[comp_idx]
            else:
                tier_probabilities[tier] = 0.0
        
        # ç¡®å®šä¸»è¦å±‚çº§
        best_tier = max(tier_probabilities, key=tier_probabilities.get)
        confidence = tier_probabilities[best_tier]
        
        return best_tier, tier_probabilities, confidence


# ============== çœŸå® Prophet é¢„æµ‹å™¨ ==============
class ProphetTrendPredictor:
    """Prophet è¶‹åŠ¿é¢„æµ‹å™¨ï¼ˆåªé¢„æµ‹æ–¹å‘ï¼‰"""
    
    def __init__(self):
        try:
            from prophet import Prophet
            self.Prophet = Prophet
            self.available = True
        except ImportError:
            print("âŒ æœªå®‰è£… Prophet åº“ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆè¶‹åŠ¿é¢„æµ‹")
            self.Prophet = None
            self.available = False
    
    def prophet_forecast_monthly_trend(self, series: pd.Series, periods: int = 6) -> Dict:
        """ä½¿ç”¨çœŸå® Prophet é¢„æµ‹æœˆåº¦è¶‹åŠ¿ï¼ˆåªæ–¹å‘ï¼‰"""
        if not self.available or len(series) < 12:
            # å›é€€åˆ°ç®€åŒ–é¢„æµ‹
            return self._simple_trend_forecast(series, periods)
        
        try:
            df = series.reset_index()
            df.columns = ['ds', 'y']
            df['ds'] = pd.to_datetime(df['ds'])
            
            model = self.Prophet(
                yearly_seasonality=True,
                weekly_seasonality=False,
                daily_seasonality=False,
                changepoint_prior_scale=0.05  # ä¿å®ˆ
            )
            model.fit(df)
            
            future = model.make_future_dataframe(periods=periods, freq='MS')
            forecast = model.predict(future)
            
            # æå–æœªæ¥é¢„æµ‹
            future_forecast = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(periods)
            
            # è¶‹åŠ¿æ–¹å‘åˆ†æ
            current = series.iloc[-1]
            future_values = future_forecast['yhat'].values
            future_avg = np.mean(future_values)
            
            direction = "ä¸Šå‡" if future_avg > current else "ä¸‹é™" if future_avg < current else "å¹³ç¨³"
            direction_confidence = min(0.9, np.mean((future_forecast['yhat_upper'] - future_forecast['yhat_lower']) / 
                                                    (future_forecast['yhat'].abs() + 0.1)))
            
            return {
                'forecast': future_values.tolist(),
                'yhat_lower': future_forecast['yhat_lower'].values.tolist(),
                'yhat_upper': future_forecast['yhat_upper'].values.tolist(),
                'direction': direction,
                'direction_confidence': round(direction_confidence, 3),
                'current_value': float(current),
                'future_avg': float(future_avg),
                'is_prophet': True
            }
        except Exception as e:
            print(f"Prophet é¢„æµ‹å¤±è´¥: {e}")
            return self._simple_trend_forecast(series, periods)
    
    def _simple_trend_forecast(self, series: pd.Series, periods: int = 6) -> Dict:
        """ç®€åŒ–è¶‹åŠ¿é¢„æµ‹ï¼ˆå½“ Prophet ä¸å¯ç”¨æ—¶ï¼‰"""
        if len(series) < 3:
            return {'error': 'æ•°æ®ä¸è¶³'}
        
        values = series.values
        # è®¡ç®—æœ€è¿‘3ä¸ªæœˆçš„è¶‹åŠ¿
        recent = values[-3:]
        x = np.arange(len(recent))
        slope, intercept = np.polyfit(x, recent, 1)
        
        # æœªæ¥é¢„æµ‹
        future = [max(0, intercept + slope * i) for i in range(periods)]
        
        # æ–¹å‘åˆ¤æ–­
        current = values[-1]
        future_avg = np.mean(future)
        direction = "ä¸Šå‡" if future_avg > current * 1.05 else "ä¸‹é™" if future_avg < current * 0.95 else "å¹³ç¨³"
        
        # ç®€å•ç½®ä¿¡åº¦
        volatility = np.std(recent) / (np.mean(recent) + 0.1)
        confidence = max(0.1, 1 - volatility)
        
        return {
            'forecast': future,
            'direction': direction,
            'direction_confidence': round(confidence, 3),
            'current_value': float(current),
            'future_avg': float(future_avg),
            'is_prophet': False
        }


# ============== æ—¶é—´çŠ¶æ€åˆ†æå™¨ ==============
class TemporalStateAnalyzer:
    """åˆ†æé¡¹ç›®æ—¶é—´è¶‹åŠ¿çŠ¶æ€"""
    
    def analyze(self, od_df: pd.DataFrame, metric: str = 'openrank') -> Dict:
        """åˆ†ææ—¶é—´è¶‹åŠ¿çŠ¶æ€"""
        if metric not in od_df.columns or len(od_df) < 6:
            return {'state': 'INSUFFICIENT_DATA', 'confidence': 0, 'reason': 'æ•°æ®ä¸è¶³'}
        
        series = od_df[metric]
        
        # 1. çŸ­æœŸè¶‹åŠ¿ï¼ˆæœ€è¿‘3ä¸ªæœˆï¼‰
        if len(series) >= 3:
            recent = series.tail(3).values
            short_term_slope = self._calculate_slope(recent)
        else:
            short_term_slope = 0
        
        # 2. ä¸­æœŸè¶‹åŠ¿ï¼ˆæœ€è¿‘6ä¸ªæœˆï¼‰
        if len(series) >= 6:
            mid_term = series.tail(6).values
            mid_term_slope = self._calculate_slope(mid_term)
        else:
            mid_term_slope = 0
        
        # 3. é•¿æœŸè¶‹åŠ¿ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰
        long_term_slope = self._calculate_slope(series.values)
        
        # 4. è¶‹åŠ¿ç¨³å®šæ€§
        volatility = series.tail(12).std() / (series.tail(12).mean() + 0.1) if len(series) >= 12 else 1
        
        # çŠ¶æ€åˆ¤æ–­
        if len(series) < 6:
            return {'state': 'INSUFFICIENT_DATA', 'confidence': 0, 'reason': 'æ•°æ®ä¸è¶³'}
        
        # ä½¿ç”¨åŠ æƒè¶‹åŠ¿ï¼šçŸ­æœŸæƒé‡0.5ï¼Œä¸­æœŸ0.3ï¼Œé•¿æœŸ0.2
        weighted_slope = short_term_slope * 0.5 + mid_term_slope * 0.3 + long_term_slope * 0.2
        
        # çŠ¶æ€åˆ†ç±»
        if weighted_slope > 0.1:
            state = 'GROWING'
            reason = f'åŠ æƒè¶‹åŠ¿æ–œç‡: {weighted_slope:.3f}'
        elif weighted_slope < -0.1:
            state = 'DECLINING'
            reason = f'åŠ æƒè¶‹åŠ¿æ–œç‡: {weighted_slope:.3f}'
        else:
            state = 'STABLE'
            reason = f'åŠ æƒè¶‹åŠ¿æ–œç‡: {weighted_slope:.3f}'
        
        # ç½®ä¿¡åº¦è®¡ç®—
        if volatility < 0.2:
            confidence = 0.9
        elif volatility < 0.5:
            confidence = 0.7
        else:
            confidence = 0.5
        
        return {
            'state': state,
            'confidence': round(confidence, 2),
            'reason': reason,
            'short_term_slope': round(short_term_slope, 4),
            'mid_term_slope': round(mid_term_slope, 4),
            'long_term_slope': round(long_term_slope, 4),
            'volatility': round(volatility, 3)
        }
    
    def _calculate_slope(self, values: np.ndarray) -> float:
        """è®¡ç®—è¶‹åŠ¿æ–œç‡"""
        if len(values) < 2:
            return 0
        
        x = np.arange(len(values))
        slope, _ = np.polyfit(x, values, 1)
        
        # æ ‡å‡†åŒ–æ–œç‡ï¼ˆé™¤ä»¥å‡å€¼ï¼‰
        mean_val = np.mean(values)
        if mean_val > 0:
            normalized_slope = slope / mean_val
        else:
            normalized_slope = slope
        
        return normalized_slope


# ============== æ´»è·ƒçŠ¶æ€åˆ†æå™¨ ==============
class ActivityStateAnalyzer:
    """åˆ†æé¡¹ç›®æ´»è·ƒçŠ¶æ€"""
    
    def analyze(self, od_df: pd.DataFrame, gh_recent: Dict = None) -> Dict:
        """åˆ†ææ´»è·ƒçŠ¶æ€"""
        # 1. åŸºäºOpenDiggerçš„æ´»è·ƒåº¦
        od_activity = self._analyze_od_activity(od_df)
        
        # 2. åŸºäºGitHubæœ€è¿‘30å¤©æ´»è·ƒåº¦ï¼ˆå¦‚æœæœ‰ï¼‰
        gh_activity = self._analyze_gh_activity(gh_recent) if gh_recent else None
        
        # 3. ç»¼åˆåˆ¤æ–­
        if gh_activity:
            # ä¸¤è€…ç»“åˆ
            if od_activity['state'] == 'THRIVING' and gh_activity['state'] == 'ACTIVE':
                state = 'THRIVING'
            elif od_activity['state'] == 'DORMANT' and gh_activity['state'] == 'INACTIVE':
                state = 'ZOMBIE'
            elif od_activity['state'] == 'STABLE' or gh_activity['state'] == 'ACTIVE':
                state = 'ACTIVE'
            else:
                state = 'DORMANT'
        else:
            state = od_activity['state']
        
        return {
            'state': state,
            'od_analysis': od_activity,
            'gh_analysis': gh_activity
        }
    
    def _analyze_od_activity(self, od_df: pd.DataFrame) -> Dict:
        """åˆ†æOpenDiggeræ´»è·ƒåº¦"""
        if 'activity' not in od_df.columns or len(od_df) < 3:
            return {'state': 'UNKNOWN', 'reason': 'æ•°æ®ä¸è¶³'}
        
        activity = od_df['activity']
        recent = activity.tail(3)
        avg_recent = recent.mean()
        historical_avg = activity.mean()
        
        if avg_recent > historical_avg * 1.2:
            state = 'THRIVING'
            reason = f'è¿‘æœŸæ´»è·ƒåº¦({avg_recent:.1f})é«˜äºå†å²({historical_avg:.1f})'
        elif avg_recent > historical_avg * 0.8:
            state = 'STABLE'
            reason = f'è¿‘æœŸæ´»è·ƒåº¦({avg_recent:.1f})æ¥è¿‘å†å²({historical_avg:.1f})'
        elif avg_recent > historical_avg * 0.3:
            state = 'DORMANT'
            reason = f'è¿‘æœŸæ´»è·ƒåº¦({avg_recent:.1f})ä½äºå†å²({historical_avg:.1f})'
        else:
            state = 'ZOMBIE'
            reason = f'è¿‘æœŸæ´»è·ƒåº¦({avg_recent:.1f})è¿œä½äºå†å²({historical_avg:.1f})'
        
        return {'state': state, 'reason': reason, 'avg_recent': float(avg_recent), 'avg_historical': float(historical_avg)}
    
    def _analyze_gh_activity(self, gh_recent: Dict) -> Dict:
        """åˆ†æGitHubæœ€è¿‘30å¤©æ´»è·ƒåº¦"""
        if 'error' in gh_recent:
            return None
        
        commits = gh_recent.get('commits', 0)
        prs = gh_recent.get('prs_opened', 0)
        issues = gh_recent.get('issues_opened', 0)
        
        total_activity = commits + prs * 2 + issues  # PRæƒé‡æ›´é«˜
        
        if total_activity >= 20:
            state = 'VERY_ACTIVE'
        elif total_activity >= 10:
            state = 'ACTIVE'
        elif total_activity >= 3:
            state = 'LOW_ACTIVITY'
        else:
            state = 'INACTIVE'
        
        return {
            'state': state,
            'total_activity': total_activity,
            'commits': commits,
            'prs': prs,
            'issues': issues
        }


# ============== æ–¹å‘æ€§å›æµ‹éªŒè¯å™¨ ==============
class DirectionalBacktestValidator:
    """æ–¹å‘æ€§å›æµ‹éªŒè¯å™¨ï¼ˆåªéªŒè¯è¶‹åŠ¿æ–¹å‘ï¼‰"""
    
    @staticmethod
    def directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—æ–¹å‘å‡†ç¡®æ€§"""
        if len(y_true) < 2 or len(y_pred) < 2:
            return 0.0
        
        # è®¡ç®—çœŸå®æ–¹å‘
        true_dir = np.sign(np.diff(y_true))
        
        # è®¡ç®—é¢„æµ‹æ–¹å‘
        pred_dir = np.sign(np.diff(y_pred))
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(true_dir), len(pred_dir))
        true_dir = true_dir[:min_len]
        pred_dir = pred_dir[:min_len]
        
        # è®¡ç®—åŒ¹é…ç‡
        matches = np.sum(true_dir == pred_dir)
        return matches / min_len
    
    def validate(self, data: pd.Series, test_ratio: float = 0.3) -> Dict:
        """å›æµ‹éªŒè¯"""
        if len(data) < 12:
            return {'error': 'æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘12ä¸ªæœˆæ•°æ®'}
        
        n_test = max(3, int(len(data) * test_ratio))
        n_train = len(data) - n_test
        
        if n_train < 6:
            return {'error': 'è®­ç»ƒæ•°æ®ä¸è¶³'}
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_data = data.iloc[:n_train]
        test_data = data.iloc[n_train:]
        
        # ä½¿ç”¨ç®€åŒ–é¢„æµ‹å™¨è¿›è¡Œé¢„æµ‹
        from sklearn.linear_model import LinearRegression
        model = LinearRegression()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X_train = np.arange(len(train_data)).reshape(-1, 1)
        y_train = train_data.values
        
        model.fit(X_train, y_train)
        
        # é¢„æµ‹æµ‹è¯•é›†
        X_test = np.arange(len(train_data), len(train_data) + len(test_data)).reshape(-1, 1)
        y_pred = model.predict(X_test)
        
        # è®¡ç®—æ–¹å‘å‡†ç¡®æ€§
        dir_acc = self.directional_accuracy(test_data.values, y_pred)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        if dir_acc > 0.8:
            confidence = 'HIGH'
        elif dir_acc > 0.6:
            confidence = 'MEDIUM'
        elif dir_acc > 0.5:
            confidence = 'LOW'
        else:
            confidence = 'VERY_LOW'
        
        return {
            'train_samples': n_train,
            'test_samples': n_test,
            'direction_accuracy': round(dir_acc, 3),
            'confidence': confidence,
            'actual_values': [float(v) for v in test_data.values],
            'predicted_values': [float(v) for v in y_pred]
        }


# ============== æ–°çš„è¶‹åŠ¿åˆ†æå™¨ ==============
class TrendAnalyzerV6:
    """æ–°ç‰ˆè¶‹åŠ¿åˆ†æå™¨ï¼ˆåˆ†ç¦»åŠ¨é‡ã€é˜»åŠ›ã€æ½œåŠ›ï¼‰"""
    
    def analyze(self, od_df: pd.DataFrame, tier: str, gh_info: Dict = None) -> Dict:
        """åˆ†æè¶‹åŠ¿"""
        # åŠ¨é‡åˆ†æï¼ˆåŸºäºæœˆåº¦å˜åŒ–ï¼‰
        momentum = self._analyze_momentum(od_df)
        
        # é˜»åŠ›åˆ†æï¼ˆåŸºäºé—®é¢˜ç§¯å‹ç­‰ï¼‰
        resistance = self._analyze_resistance(od_df)
        
        # æ½œåŠ›åˆ†æï¼ˆåŸºäºGitHubå¿«ç…§ä¸åŸºå‡†å¯¹æ¯”ï¼‰
        potential = self._analyze_potential(od_df, tier, gh_info)
        
        # ç»¼åˆè¶‹åŠ¿è¯„åˆ†
        trend_score = momentum['score'] * 0.4 - resistance['score'] * 0.3 + potential['score'] * 0.3
        
        if trend_score >= 60:
            trend_class, trend_desc = 'STRONG_UP', 'å¼ºçƒˆä¸Šå‡è¶‹åŠ¿'
        elif trend_score >= 30:
            trend_class, trend_desc = 'MODERATE_UP', 'æ¸©å’Œä¸Šå‡è¶‹åŠ¿'
        elif trend_score >= 0:
            trend_class, trend_desc = 'STABLE', 'è¶‹åŠ¿ç¨³å®š'
        elif trend_score >= -30:
            trend_class, trend_desc = 'MODERATE_DOWN', 'æ¸©å’Œä¸‹é™è¶‹åŠ¿'
        else:
            trend_class, trend_desc = 'STRONG_DOWN', 'å¼ºçƒˆä¸‹é™è¶‹åŠ¿'
        
        return {
            'trend_score': round(trend_score, 1),
            'trend_class': trend_class,
            'trend_description': trend_desc,
            'momentum': momentum,
            'resistance': resistance,
            'potential': potential
        }
    
    def _analyze_momentum(self, od_df: pd.DataFrame) -> Dict:
        """åŠ¨é‡åˆ†æï¼ˆåŸºäºæœˆåº¦å˜åŒ–è¶‹åŠ¿ï¼‰"""
        # å‚ä¸åº¦åŠ¨é‡
        if 'participants' in od_df.columns and len(od_df) >= 6:
            participants_trend = linregress(range(6), od_df['participants'].tail(6).values).slope
            participants_momentum = min(100, max(0, 50 + participants_trend * 50))
        else:
            participants_momentum = 50
        
        # æ´»è·ƒåº¦åŠ¨é‡
        if 'activity' in od_df.columns and len(od_df) >= 6:
            activity_trend = linregress(range(6), od_df['activity'].tail(6).values).slope
            activity_momentum = min(100, max(0, 50 + activity_trend * 30))
        else:
            activity_momentum = 50
        
        # è´¡çŒ®è€…åŠ¨é‡
        if 'new_contributors' in od_df.columns:
            recent_contributors = od_df['new_contributors'].tail(6).mean()
            historical_contributors = od_df['new_contributors'].mean()
            if historical_contributors > 0:
                contributor_ratio = recent_contributors / historical_contributors
                contributor_momentum = min(100, max(0, contributor_ratio * 50))
            else:
                contributor_momentum = recent_contributors * 10
        else:
            contributor_momentum = 50
        
        # ç»¼åˆåŠ¨é‡
        total = participants_momentum * 0.3 + activity_momentum * 0.4 + contributor_momentum * 0.3
        
        return {
            'score': round(total, 1),
            'participants_momentum': round(participants_momentum, 1),
            'activity_momentum': round(activity_momentum, 1),
            'contributor_momentum': round(contributor_momentum, 1),
            'description': self._momentum_description(total)
        }
    
    def _analyze_resistance(self, od_df: pd.DataFrame) -> Dict:
        """é˜»åŠ›åˆ†æ"""
        # Issueç§¯å‹
        if 'issues_new' in od_df.columns and 'issues_closed' in od_df.columns:
            recent_new = od_df['issues_new'].tail(6).sum()
            recent_closed = od_df['issues_closed'].tail(6).sum()
            if recent_new > 0:
                issue_ratio = recent_closed / recent_new
                issue_resistance = max(0, 100 - issue_ratio * 100)
            else:
                issue_resistance = 0
        else:
            issue_resistance = 50
        
        # PRåˆå¹¶æ•ˆç‡
        if 'pr_new' in od_df.columns and 'pr_merged' in od_df.columns:
            recent_pr_new = od_df['pr_new'].tail(6).sum()
            recent_pr_merged = od_df['pr_merged'].tail(6).sum()
            if recent_pr_new > 0:
                pr_ratio = recent_pr_merged / recent_pr_new
                pr_resistance = max(0, 100 - pr_ratio * 100)
            else:
                pr_resistance = 0
        else:
            pr_resistance = 50
        
        # è´¡çŒ®è€…æµå¤±
        if 'inactive_contributors' in od_df.columns and 'participants' in od_df.columns:
            recent_inactive = od_df['inactive_contributors'].tail(6).mean()
            recent_participants = od_df['participants'].tail(6).mean()
            if recent_participants > 0:
                churn_rate = recent_inactive / recent_participants
                churn_resistance = min(100, churn_rate * 200)
            else:
                churn_resistance = 0
        else:
            churn_resistance = 50
        
        # ç»¼åˆé˜»åŠ›
        total = issue_resistance * 0.4 + pr_resistance * 0.3 + churn_resistance * 0.3
        
        return {
            'score': round(total, 1),
            'issue_resistance': round(issue_resistance, 1),
            'pr_resistance': round(pr_resistance, 1),
            'churn_resistance': round(churn_resistance, 1),
            'description': self._resistance_description(total)
        }
    
    def _analyze_potential(self, od_df: pd.DataFrame, tier: str, gh_info: Dict) -> Dict:
        """æ½œåŠ›åˆ†æï¼ˆåŸºäºå½“å‰çŠ¶æ€ä¸å±‚çº§åŸºå‡†çš„å·®è·ï¼‰"""
        # è·å–å½“å‰çŠ¶æ€
        if 'openrank' in od_df.columns:
            current_openrank = od_df['openrank'].iloc[-1]
        else:
            current_openrank = 0
        
        # è·å–å±‚çº§åŸºå‡†
        tier_benchmark = TIER_BENCHMARKS.get(tier, TIER_BENCHMARKS['EMERGING'])
        benchmark_openrank = tier_benchmark['openrank']
        
        # è®¡ç®—ä¸ä¸‹ä¸€å±‚çº§çš„å·®è·
        tiers = ['EMERGING', 'GROWING', 'MATURE', 'GIANT']
        current_idx = tiers.index(tier) if tier in tiers else 0
        
        if current_idx < len(tiers) - 1:
            next_tier = tiers[current_idx + 1]
            next_benchmark = TIER_BENCHMARKS[next_tier]['openrank']
            gap_to_next = next_benchmark - current_openrank
            max_gap = next_benchmark - tier_benchmark['openrank']
            
            if max_gap > 0:
                potential_score = (gap_to_next / max_gap) * 100
            else:
                potential_score = 0
        else:
            # å·²ç»æ˜¯æœ€é«˜å±‚çº§
            potential_score = 0
        
        # é™åˆ¶èŒƒå›´
        potential_score = max(0, min(100, potential_score))
        
        return {
            'score': round(potential_score, 1),
            'current_openrank': round(current_openrank, 2),
            'tier_benchmark': benchmark_openrank,
            'description': self._potential_description(potential_score, tier)
        }
    
    def _momentum_description(self, score: float) -> str:
        if score >= 70:
            return 'å¼ºåŠ²å¢é•¿åŠ¨åŠ›'
        elif score >= 50:
            return 'ç¨³å®šå‘å±•åŠ¨åŠ›'
        elif score >= 30:
            return 'åŠ¨åŠ›ä¸è¶³'
        else:
            return 'å¢é•¿åœæ»'
    
    def _resistance_description(self, score: float) -> str:
        if score >= 70:
            return 'é˜»åŠ›è¾ƒå¤§ï¼Œéœ€å…³æ³¨'
        elif score >= 50:
            return 'ä¸­ç­‰é˜»åŠ›'
        elif score >= 30:
            return 'é˜»åŠ›è¾ƒå°'
        else:
            return 'å‘å±•é¡ºç•…'
    
    def _potential_description(self, score: float, tier: str) -> str:
        if tier == 'GIANT':
            return 'å·²è¾¾é¡¶çº§è§„æ¨¡'
        elif score >= 70:
            return f'é«˜å¢é•¿æ½œåŠ›ï¼Œæœ‰è¾ƒå¤§ç©ºé—´è¾¾åˆ°{TIER_NAMES.get(self._get_next_tier(tier), "ä¸‹ä¸€å±‚çº§")}'
        elif score >= 40:
            return f'ä¸­ç­‰æ½œåŠ›ï¼Œé€æ­¥å‘{TIER_NAMES.get(self._get_next_tier(tier), "ä¸‹ä¸€å±‚çº§")}å‘å±•'
        elif score >= 20:
            return f'æœ‰é™æ½œåŠ›ï¼Œæ¥è¿‘å½“å‰å±‚çº§ä¸Šé™'
        else:
            return f'å·²æ¥è¿‘å½“å‰å±‚çº§å¤©èŠ±æ¿'
    
    def _get_next_tier(self, current_tier: str) -> str:
        tiers = ['EMERGING', 'GROWING', 'MATURE', 'GIANT']
        current_idx = tiers.index(current_tier) if current_tier in tiers else 0
        if current_idx < len(tiers) - 1:
            return tiers[current_idx + 1]
        return current_tier


# ============== æ–°ç‰ˆ AHP å¥åº·è¯„ä¼°å™¨ ==============
class AHPHealthEvaluatorV6:
    """æ–°ç‰ˆAHPå¥åº·è¯„ä¼°å™¨ï¼ˆé™å™ªæƒé‡ï¼‰"""
    
    TIER_WEIGHTS = {
        'GIANT': {
            'momentum': 0.20,    # é™ä½åŠ¨é‡æƒé‡
            'stability': 0.40,   # æé«˜ç¨³å®šæ€§æƒé‡
            'potential': 0.15,
            'safety': 0.25
        },
        'MATURE': {
            'momentum': 0.15,    # é™ä½åŠ¨é‡æƒé‡
            'stability': 0.45,   # æé«˜ç¨³å®šæ€§æƒé‡
            'potential': 0.20,
            'safety': 0.20
        },
        'GROWING': {
            'momentum': 0.20,    # ä»0.35é™åˆ°0.20
            'stability': 0.25,
            'potential': 0.40,   # ä»0.30æé«˜åˆ°0.40
            'safety': 0.15       # ä»0.10æé«˜åˆ°0.15
        },
        'EMERGING': {
            'momentum': 0.25,    # ä»0.30é™åˆ°0.25
            'stability': 0.20,
            'potential': 0.45,   # ä»0.40æé«˜åˆ°0.45
            'safety': 0.10
        }
    }
    
    def calculate_health_score(self, 
                              trend_analysis: Dict,
                              temporal_state: Dict,
                              activity_state: Dict,
                              tier: str) -> Tuple[float, Dict[str, float]]:
        """è®¡ç®—å¥åº·åˆ†"""
        weights = self.TIER_WEIGHTS.get(tier, self.TIER_WEIGHTS['MATURE'])
        
        # å„ç»´åº¦åŸå§‹åˆ†æ•°
        raw_scores = {
            'momentum': trend_analysis['momentum']['score'],
            'stability': 100 - trend_analysis['resistance']['score'],
            'potential': trend_analysis['potential']['score'],
            'safety': self._calculate_safety_score(temporal_state, activity_state)
        }
        
        # åº”ç”¨æƒé‡
        weighted_sum = 0
        for dim, score in raw_scores.items():
            weighted_sum += score * weights[dim]
        
        # åŸºç¡€å¥åº·åˆ†
        base_score = weighted_sum
        
        # æ ¹æ®æ—¶é—´çŠ¶æ€å¾®è°ƒ
        temporal_factor = self._get_temporal_factor(temporal_state['state'])
        base_score *= temporal_factor
        
        # æ ¹æ®æ´»è·ƒçŠ¶æ€å¾®è°ƒ
        activity_factor = self._get_activity_factor(activity_state['state'])
        base_score *= activity_factor
        
        # é™åˆ¶èŒƒå›´
        final_score = max(0, min(100, base_score))
        
        # å„ç»´åº¦è´¡çŒ®
        dimension_contributions = {}
        for dim, score in raw_scores.items():
            dimension_contributions[dim] = round(score * weights[dim] / 100, 3)
        
        return round(final_score, 1), dimension_contributions
    
    def _calculate_safety_score(self, temporal_state: Dict, activity_state: Dict) -> float:
        """è®¡ç®—å®‰å…¨åˆ†æ•°"""
        score = 70  # åŸºç¡€åˆ†
        
        # æ—¶é—´çŠ¶æ€è°ƒæ•´
        if temporal_state['state'] == 'DECLINING':
            score -= 20
        elif temporal_state['state'] == 'INSUFFICIENT_DATA':
            score -= 10
        
        # æ´»è·ƒçŠ¶æ€è°ƒæ•´
        if activity_state['state'] == 'ZOMBIE':
            score -= 30
        elif activity_state['state'] == 'DORMANT':
            score -= 15
        
        return max(0, min(100, score))
    
    def _get_temporal_factor(self, state: str) -> float:
        factors = {
            'GROWING': 1.1,
            'STABLE': 1.0,
            'DECLINING': 0.8,
            'INSUFFICIENT_DATA': 0.9
        }
        return factors.get(state, 1.0)
    
    def _get_activity_factor(self, state: str) -> float:
        factors = {
            'THRIVING': 1.2,
            'ACTIVE': 1.1,
            'STABLE': 1.0,
            'DORMANT': 0.8,
            'ZOMBIE': 0.6
        }
        return factors.get(state, 1.0)


# ============== GitHub API åˆ†æå™¨ ==============
class GitHubAPIAnalyzerV6:
    """æ–°ç‰ˆGitHub APIåˆ†æå™¨"""
    
    def __init__(self, token: str = None):
        self.token = token
        self.base_url = "https://api.github.com"
        self.headers = {'Authorization': f'token {token}'} if token else {}
    
    def fetch_repo_info(self, org: str, repo: str) -> Optional[Dict]:
        """è·å–ä»“åº“åŸºæœ¬ä¿¡æ¯ï¼ˆç”¨äºé”šå®šï¼‰"""
        if not self.token:
            return None
        
        try:
            url = f"{self.base_url}/repos/{org}/{repo}"
            res = requests.get(url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                data = res.json()
                return {
                    'stars': data.get('stargazers_count', 0),
                    'forks': data.get('forks_count', 0),
                    'watchers': data.get('watchers_count', 0),
                    'open_issues': data.get('open_issues_count', 0),
                    'language': data.get('language'),
                    'created_at': data.get('created_at'),
                    'updated_at': data.get('updated_at'),
                    'pushed_at': data.get('pushed_at'),
                    'size': data.get('size'),
                    'license': data.get('license', {}).get('name') if data.get('license') else None,
                    'topics': data.get('topics', []),
                    'archived': data.get('archived', False)
                }
            else:
                print(f"GitHub API è¯·æ±‚å¤±è´¥: {res.status_code}")
                return None
        except Exception as e:
            print(f"GitHub API é”™è¯¯: {e}")
            return None
    
    def fetch_recent_activity(self, org: str, repo: str, days: int = 30) -> Dict:
        """è·å–æœ€è¿‘Nå¤©çš„æ´»è·ƒæ•°æ®"""
        if not self.token:
            return {'error': 'éœ€è¦ GitHub Token'}
        
        since_date = (datetime.now() - timedelta(days=days)).isoformat()
        
        result = {
            'period_days': days,
            'commits': 0,
            'issues_opened': 0,
            'issues_closed': 0,
            'prs_opened': 0,
            'prs_merged': 0,
            'contributors_active': set()
        }
        
        try:
            # è·å–æœ€è¿‘æäº¤
            commits_url = f"{self.base_url}/repos/{org}/{repo}/commits?since={since_date}&per_page=100"
            res = requests.get(commits_url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                commits = res.json()
                result['commits'] = len(commits)
                for c in commits:
                    author = c.get('author', {})
                    if author and author.get('login'):
                        result['contributors_active'].add(author['login'])
            
            # è·å–æœ€è¿‘ Issues
            issues_url = f"{self.base_url}/repos/{org}/{repo}/issues?state=all&since={since_date}&per_page=100"
            res = requests.get(issues_url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                issues = res.json()
                for issue in issues:
                    if 'pull_request' not in issue:
                        created = issue.get('created_at', '')
                        if created >= since_date:
                            result['issues_opened'] += 1
                        if issue.get('state') == 'closed':
                            result['issues_closed'] += 1
            
            # è·å–æœ€è¿‘ PRs
            prs_url = f"{self.base_url}/repos/{org}/{repo}/pulls?state=all&per_page=100"
            res = requests.get(prs_url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                prs = res.json()
                for pr in prs:
                    created = pr.get('created_at', '')
                    if created >= since_date:
                        result['prs_opened'] += 1
                        if pr.get('merged_at'):
                            result['prs_merged'] += 1
            
            result['contributors_active'] = len(result['contributors_active'])
            
        except Exception as e:
            result['error'] = str(e)
        
        return result
    
    def validate_conclusions(self, temporal_state: Dict, activity_state: Dict, gh_recent: Dict) -> Dict:
        """ä½¿ç”¨GitHub 30å¤©æ•°æ®éªŒè¯ç»“è®º"""
        if 'error' in gh_recent:
            return {'error': gh_recent['error']}
        
        validation = {
            'overall_valid': True,
            'confidence': 0,
            'validations': [],
            'warnings': []
        }
        
        # æ´»è·ƒçŠ¶æ€éªŒè¯
        gh_activity = activity_state.get('gh_analysis', {})
        if gh_activity:
            od_state = activity_state['state']
            gh_state = gh_activity.get('state', 'UNKNOWN')
            
            if (od_state == 'THRIVING' and gh_state in ['VERY_ACTIVE', 'ACTIVE']) or \
               (od_state == 'ZOMBIE' and gh_state == 'INACTIVE') or \
               (od_state == 'DORMANT' and gh_state == 'LOW_ACTIVITY'):
                validation['validations'].append({
                    'check': 'æ´»è·ƒçŠ¶æ€',
                    'result': 'PASS',
                    'detail': f'OpenDiggerçŠ¶æ€({od_state})ä¸GitHubçŠ¶æ€({gh_state})ä¸€è‡´'
                })
                validation['confidence'] += 25
            else:
                validation['warnings'].append(f'æ´»è·ƒçŠ¶æ€ä¸ä¸€è‡´: OD={od_state}, GH={gh_state}')
        
        # æ—¶é—´è¶‹åŠ¿éªŒè¯ï¼ˆç®€åŒ–ï¼‰
        temporal_state_val = temporal_state['state']
        gh_commits = gh_recent.get('commits', 0)
        
        if temporal_state_val == 'GROWING' and gh_commits >= 5:
            validation['validations'].append({
                'check': 'å¢é•¿è¶‹åŠ¿',
                'result': 'PASS',
                'detail': f'å¢é•¿è¶‹åŠ¿ä¸è¿‘æœŸæ´»åŠ¨(commits={gh_commits})ä¸€è‡´'
            })
            validation['confidence'] += 25
        elif temporal_state_val == 'DECLINING' and gh_commits <= 2:
            validation['validations'].append({
                'check': 'è¡°é€€è¶‹åŠ¿',
                'result': 'PASS',
                'detail': f'è¡°é€€è¶‹åŠ¿ä¸è¿‘æœŸä½æ´»åŠ¨(commits={gh_commits})ä¸€è‡´'
            })
            validation['confidence'] += 25
        else:
            validation['validations'].append({
                'check': 'è¶‹åŠ¿éªŒè¯',
                'result': 'NEUTRAL',
                'detail': f'è¶‹åŠ¿{temporal_state_val}ä¸è¿‘æœŸæ´»åŠ¨(commits={gh_commits})æ— æ˜æ˜¾å†²çª'
            })
            validation['confidence'] += 15
        
        # è´¡çŒ®è€…éªŒè¯
        gh_contributors = gh_recent.get('contributors_active', 0)
        if gh_contributors >= 3:
            validation['validations'].append({
                'check': 'è´¡çŒ®è€…æ´»è·ƒ',
                'result': 'PASS',
                'detail': f'è¿‘æœŸæœ‰{gh_contributors}åæ´»è·ƒè´¡çŒ®è€…'
            })
            validation['confidence'] += 25
        
        # PRæ•ˆç‡éªŒè¯
        pr_opened = gh_recent.get('prs_opened', 0)
        pr_merged = gh_recent.get('prs_merged', 0)
        if pr_opened > 0:
            merge_rate = pr_merged / pr_opened
            if merge_rate >= 0.5:
                validation['validations'].append({
                    'check': 'PRæ•ˆç‡',
                    'result': 'PASS',
                    'detail': f'PRåˆå¹¶ç‡{merge_rate:.0%}è‰¯å¥½'
                })
                validation['confidence'] += 25
            else:
                validation['warnings'].append(f'PRåˆå¹¶ç‡åä½: {merge_rate:.0%}')
        
        # æœ€ç»ˆç½®ä¿¡åº¦
        validation['confidence'] = min(100, validation['confidence'])
        validation['overall_valid'] = validation['confidence'] >= 50 and len(validation['warnings']) <= 2
        
        return validation


# ============== å…¶ä»–åˆ†æå™¨ï¼ˆä¿æŒåŸæ ·ä½†ä¼˜åŒ–ï¼‰ ==============
class BusFactorCalculatorV6:
    """Bus Factor 2.0"""
    
    def calculate(self, data: pd.DataFrame) -> Dict:
        try:
            if 'participants' not in data.columns:
                return {'effective_bus_factor': 1, 'risk_level': 'UNKNOWN'}
            
            participants = data['participants'].tail(6).mean()
            if participants <= 0:
                return {'effective_bus_factor': 1, 'risk_level': 'CRITICAL'}
            
            n = int(max(1, participants))
            contributions = np.array([1/(i+1) for i in range(n)])
            contributions = contributions / contributions.sum()
            
            entropy = -np.sum(contributions * np.log2(contributions + 1e-10))
            max_entropy = np.log2(n) if n > 1 else 1
            normalized = entropy / max_entropy if max_entropy > 0 else 0
            effective_bf = 2 ** entropy
            
            if effective_bf <= 2:
                risk, desc = 'CRITICAL', 'æé«˜é£é™©ï¼šè´¡çŒ®è¿‡äºé›†ä¸­'
            elif effective_bf <= 4:
                risk, desc = 'HIGH', 'é«˜é£é™©ï¼šéœ€åŸ¹å…»æ›´å¤šè´¡çŒ®è€…'
            elif effective_bf <= 8:
                risk, desc = 'MEDIUM', 'ä¸­ç­‰é£é™©ï¼šè´¡çŒ®è€…å¤šæ ·æ€§å°šå¯'
            else:
                risk, desc = 'LOW', 'ä½é£é™©ï¼šè´¡çŒ®è€…ç”Ÿæ€å¥åº·'
            
            return {
                'raw_entropy': round(entropy, 3),
                'normalized_entropy': round(normalized, 3),
                'effective_bus_factor': round(effective_bf, 1),
                'risk_level': risk,
                'description': desc
            }
        except Exception as e:
            return {'effective_bus_factor': 1, 'risk_level': 'UNKNOWN'}


class ETDAnalyzerV6:
    """ETDåˆ†æå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    def analyze(self, data: pd.DataFrame, activity_state: str, tier: str) -> Dict:
        result = {
            'etd_months': float('inf'),
            'etd_status': 'HEALTHY',
            'is_mature_stable': False,
            'description': '',
            'recommendations': []
        }
        
        if 'activity' not in data.columns or len(data) < 6:
            result['description'] = 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¿å‘½é¢„æµ‹'
            return result
        
        try:
            activity = data['activity'].tail(12)
            slope, _ = np.polyfit(range(len(activity)), activity.values, 1)
            current = activity.iloc[-1]
            
            # æˆç†Ÿç¨³å®šé¡¹ç›®åˆ¤æ–­
            if tier in ['GIANT', 'MATURE'] and activity_state in ['STABLE', 'ACTIVE']:
                if current > activity.mean() * 0.3 and abs(slope) < current * 0.1:
                    result['etd_status'] = 'STABLE_MATURE'
                    result['is_mature_stable'] = True
                    result['description'] = 'é¡¹ç›®è¿›å…¥æˆç†Ÿç¨³å®šæœŸï¼Œä½æ´»è·ƒåº¦æ˜¯æ­£å¸¸ç‰¹å¾'
                    return result
            
            # çœŸæ­£è¡°é€€åˆ¤æ–­
            if slope < 0 and current > 0:
                etd = -current / slope
                result['etd_months'] = max(0, etd)
                
                if etd < 6:
                    result['etd_status'] = 'CRITICAL'
                    result['description'] = f'é«˜å±ï¼šé¢„è®¡{etd:.1f}ä¸ªæœˆåæ´»è·ƒåº¦å½’é›¶'
                elif etd < 12:
                    result['etd_status'] = 'WARNING'
                    result['description'] = f'é¢„è­¦ï¼šé¢„è®¡{etd:.1f}ä¸ªæœˆåå¯èƒ½æ¯ç«­'
                elif etd < 24:
                    result['etd_status'] = 'CAUTION'
                    result['description'] = f'æ³¨æ„ï¼šé¢„è®¡{etd:.1f}ä¸ªæœˆåå¯èƒ½ä½è¿·'
                else:
                    result['etd_status'] = 'HEALTHY'
                    result['description'] = f'å¥åº·ï¼šETD > 24ä¸ªæœˆï¼Œæš‚æ— é£é™©'
            else:
                result['etd_status'] = 'THRIVING'
                result['description'] = 'æ´»è·ƒåº¦ç¨³å®šæˆ–ä¸Šå‡ï¼Œæ— æ¯ç«­é£é™©'
            
            return result
        except Exception as e:
            result['description'] = 'åˆ†æè¿‡ç¨‹å‡ºé”™'
            return result


# ============== æ–°ç‰ˆé¡¹ç›®åˆ†æå™¨ ==============
class ProjectAnalyzerV6:
    """GitHub é¡¹ç›®æ·±åº¦åˆ†æå™¨ v6.0 - æ•°æ®æµé‡æ„ç‰ˆ"""
    
    CORE_METRICS = [
        "openrank", "activity", "stars", "attention",
        "participants", "new_contributors", "inactive_contributors",
        "bus_factor", "issues_new", "issues_closed", "pr_new", "pr_merged"
    ]
    
    def __init__(self, url: str, github_token: Optional[str] = None):
        self.org, self.repo = self._parse_url(url)
        self.od_df = pd.DataFrame()  # OpenDiggeræ•°æ®ï¼ˆæœˆåº¦å˜åŒ–ï¼‰
        self.gh_info = {}            # GitHubå¿«ç…§æ•°æ®
        self.gh_recent = {}          # GitHubè¿‘æœŸæ´»åŠ¨
        
        self.structural_tier = None
        self.tier_probabilities = {}
        self.tier_confidence = 0
        
        self.github_token = github_token or os.getenv('GITHUB_TOKEN')
        
        # åˆå§‹åŒ–å„åˆ†æå™¨
        self.data_reconciliation = DataReconciliation()
        self.gmm_classifier = GMMTierClassifier()
        self.temporal_analyzer = TemporalStateAnalyzer()
        self.activity_analyzer = ActivityStateAnalyzer()
        self.trend_analyzer = TrendAnalyzerV6()
        self.ahp_evaluator = AHPHealthEvaluatorV6()
        self.prophet_predictor = ProphetTrendPredictor()
        self.backtest_validator = DirectionalBacktestValidator()
        self.bus_factor_calculator = BusFactorCalculatorV6()
        self.etd_analyzer = ETDAnalyzerV6()
        self.github_analyzer = GitHubAPIAnalyzerV6(self.github_token) if self.github_token else None
    
    def _parse_url(self, url: str) -> Tuple[str, str]:
        match = re.search(r"github\.com/([^/]+)/([^/]+)", url)
        if match:
            return match.group(1), match.group(2).replace(".git", "")
        if "/" in url and "http" not in url:
            parts = url.split('/')
            return parts[0], parts[1]
        raise ValueError("æ— æ•ˆçš„ GitHub URL")
    
    def fetch_data(self) -> bool:
        """è·å–æ•°æ®"""
        print(f"\n{'='*60}")
        print(f"  GitHub é¡¹ç›®æ·±åº¦åˆ†æå™¨ v6.0 - æ•°æ®æµé‡æ„ç‰ˆ")
        print(f"  é¡¹ç›®: {self.org}/{self.repo}")
        print(f"{'='*60}\n")
        
        # 1. è·å–OpenDiggeræ•°æ®ï¼ˆæœˆåº¦å˜åŒ–ï¼‰
        print("ğŸ“Š æ­£åœ¨è·å– OpenDigger æ•°æ®ï¼ˆæœˆåº¦å˜åŒ–ï¼‰...")
        raw_data = {}
        for metric in self.CORE_METRICS:
            url = f"https://oss.open-digger.cn/github/{self.org}/{self.repo}/{metric}.json"
            try:
                res = requests.get(url, timeout=15)
                if res.status_code == 200:
                    data = res.json()
                    monthly = {k: v for k, v in data.items() if re.match(r'^\d{4}-\d{2}$', str(k))}
                    if monthly:
                        raw_data[metric] = pd.Series(monthly)
            except:
                continue
        
        if not raw_data:
            print("âŒ æ— æ³•è·å– OpenDigger æ•°æ®")
            return False
        
        self.od_df = pd.DataFrame(raw_data).fillna(0)
        if len(self.od_df) == 0:
            print("âŒ OpenDigger æ•°æ®ä¸ºç©º")
            return False
        
        self.od_df.index = pd.to_datetime(self.od_df.index)
        self.od_df = self.od_df.sort_index()
        print(f"âœ… è·å–åˆ° {len(self.od_df)} ä¸ªæœˆåº¦å˜åŒ–æ•°æ®")
        
        # 2. è·å–GitHubå¿«ç…§æ•°æ®ï¼ˆç”¨äºé”šå®šï¼‰
        if self.github_token and self.github_analyzer:
            print("ğŸ”— æ­£åœ¨è·å– GitHub API æ•°æ®ï¼ˆå½“å‰å¿«ç…§ï¼‰...")
            self.gh_info = self.github_analyzer.fetch_repo_info(self.org, self.repo)
            if self.gh_info:
                print("âœ… GitHub å¿«ç…§æ•°æ®è·å–æˆåŠŸ")
                
                # è·å–æœ€è¿‘30å¤©æ´»åŠ¨
                print("ğŸ“ˆ æ­£åœ¨è·å– GitHub æœ€è¿‘30å¤©æ´»åŠ¨æ•°æ®...")
                self.gh_recent = self.github_analyzer.fetch_recent_activity(self.org, self.repo, days=30)
                if 'error' not in self.gh_recent:
                    print(f"âœ… è·å–åˆ° {self.gh_recent.get('commits', 0)} æ¬¡æäº¤ï¼Œ{self.gh_recent.get('contributors_active', 0)} åæ´»è·ƒè´¡çŒ®è€…")
            else:
                print("âš ï¸  GitHub API æ•°æ®è·å–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ OpenDigger æ•°æ®")
        else:
            print("âš ï¸  æœªæä¾› GitHub Tokenï¼Œè·³è¿‡ GitHub æ•°æ®è·å–")
        
        # 3. GMMç»“æ„å±‚çº§åˆ†ç±»ï¼ˆåŸºäºGitHubå¿«ç…§ï¼‰
        print("ğŸ—ï¸  æ­£åœ¨åˆ†æé¡¹ç›®ç»“æ„å±‚çº§...")
        structural_metrics = self.data_reconciliation.get_structural_metrics(self.od_df, self.gh_info)
        self.structural_tier, self.tier_probabilities, self.tier_confidence = self.gmm_classifier.predict_proba(structural_metrics)
        
        print(f"âœ… ç»“æ„å±‚çº§åˆ†æå®Œæˆ: {self.structural_tier} ({TIER_NAMES[self.structural_tier]})")
        print(f"   å±‚çº§æ¦‚ç‡: {self.tier_probabilities}")
        print(f"   ç½®ä¿¡åº¦: {self.tier_confidence:.0%}")
        
        return True
    
    def analyze(self) -> Optional[AnalysisResult]:
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        if not self.fetch_data():
            return None
        
        try:
            print("\n" + "="*60)
            print("ğŸ§  å¼€å§‹æ·±åº¦åˆ†æ...")
            print("="*60)
            
            # 1. æ—¶é—´è¶‹åŠ¿çŠ¶æ€åˆ†æ
            print("â° åˆ†ææ—¶é—´è¶‹åŠ¿çŠ¶æ€...")
            temporal_state = self.temporal_analyzer.analyze(self.od_df, 'openrank')
            print(f"   æ—¶é—´çŠ¶æ€: {temporal_state['state']} (ç½®ä¿¡åº¦: {temporal_state['confidence']})")
            
            # 2. æ´»è·ƒçŠ¶æ€åˆ†æ
            print("ğŸ”‹ åˆ†ææ´»è·ƒçŠ¶æ€...")
            activity_state = self.activity_analyzer.analyze(self.od_df, self.gh_recent)
            print(f"   æ´»è·ƒçŠ¶æ€: {activity_state['state']}")
            
            # 3. è¶‹åŠ¿åˆ†æ
            print("ğŸ“ˆ åˆ†æç»¼åˆè¶‹åŠ¿...")
            trend_analysis = self.trend_analyzer.analyze(self.od_df, self.structural_tier, self.gh_info)
            print(f"   è¶‹åŠ¿è¯„åˆ†: {trend_analysis['trend_score']} ({trend_analysis['trend_description']})")
            
            # 4. å¥åº·è¯„åˆ†
            print("â¤ï¸  è®¡ç®—å¥åº·è¯„åˆ†...")
            health_score, dimension_scores = self.ahp_evaluator.calculate_health_score(
                trend_analysis, temporal_state, activity_state, self.structural_tier
            )
            
            # å¥åº·ç­‰çº§
            grades = [(85, 'A+'), (75, 'A'), (65, 'B+'), (55, 'B'), (45, 'C'), (35, 'D'), (0, 'F')]
            health_grade = next(g for t, g in grades if health_score >= t)
            print(f"   å¥åº·è¯„åˆ†: {health_score}/100 ({health_grade})")
            
            # 5. è¶‹åŠ¿é¢„æµ‹
            print("ğŸ”® ç”Ÿæˆè¶‹åŠ¿é¢„æµ‹...")
            trend_predictions = {}
            for metric in ['openrank', 'activity', 'participants']:
                if metric in self.od_df.columns and len(self.od_df[metric]) >= 6:
                    prediction = self.prophet_predictor.prophet_forecast_monthly_trend(
                        self.od_df[metric].dropna(), periods=6
                    )
                    if 'error' not in prediction:
                        trend_predictions[metric] = prediction
            
            # 6. å›æµ‹éªŒè¯
            print("ğŸ” è¿›è¡Œå›æµ‹éªŒè¯...")
            backtest_results = {}
            if 'openrank' in self.od_df.columns:
                backtest = self.backtest_validator.validate(self.od_df['openrank'].dropna())
                if 'error' not in backtest:
                    backtest_results['openrank'] = backtest
            
            # 7. å…¶ä»–åˆ†æ
            print("âš™ï¸  è¿›è¡Œä¸“é¡¹åˆ†æ...")
            bus_factor_2 = self.bus_factor_calculator.calculate(self.od_df)
            etd_analysis = self.etd_analyzer.analyze(self.od_df, activity_state['state'], self.structural_tier)
            
            # 8. GitHubéªŒè¯
            print("âœ… è¿›è¡ŒGitHubæ•°æ®éªŒè¯...")
            github_comparison = {}
            conclusion_validation = {'error': 'æœªæä¾›GitHub Tokenï¼Œè·³è¿‡éªŒè¯'}
            
            if self.github_analyzer and self.gh_info:
                # æ•°æ®åè°ƒåˆ†æ
                github_comparison = {
                    'stars': self.data_reconciliation.split_od_trend_and_gh_snapshot(
                        self.od_df, self.gh_info, 'stars'
                    )['reconciliation']
                }
                
                if self.gh_recent and 'error' not in self.gh_recent:
                    conclusion_validation = self.github_analyzer.validate_conclusions(
                        temporal_state, activity_state, self.gh_recent
                    )
            
            # 9. é£é™©åˆ†æ
            risk_analysis = self._analyze_risk(temporal_state, activity_state, trend_analysis)
            
            # 10. é»‘é©¬åˆ†æ
            dark_horse_analysis = self._analyze_dark_horse(trend_analysis, bus_factor_2)
            
            # 11. å˜ç‚¹æ£€æµ‹
            change_points = []
            if 'openrank' in self.od_df.columns:
                change_points = self._detect_change_points(self.od_df['openrank'])
            
            # 12. ç”Ÿæˆå»ºè®®
            recommendations = self._generate_recommendations(
                self.structural_tier, temporal_state, activity_state,
                trend_analysis, risk_analysis, bus_factor_2, etd_analysis
            )
            
            # æ„å»ºç»“æœ
            result = AnalysisResult(
                project_name=f"{self.org}/{self.repo}",
                structural_tier=self.structural_tier,
                temporal_state=temporal_state['state'],
                activity_state=activity_state['state'],
                tier_probabilities=self.tier_probabilities,
                tier_confidence=self.tier_confidence,
                health_score=health_score,
                health_grade=health_grade,
                dimension_scores=dimension_scores,
                trend_analysis=trend_analysis,
                risk_analysis=risk_analysis,
                bus_factor_2=bus_factor_2,
                etd_analysis=etd_analysis,
                dark_horse_analysis=dark_horse_analysis,
                change_points=change_points,
                github_comparison=github_comparison,
                conclusion_validation=conclusion_validation,
                trend_predictions=trend_predictions,
                backtest_results=backtest_results,
                recommendations=recommendations,
                detailed_report=""
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            result.detailed_report = self.generate_report(result)
            
            # è¾“å‡ºç»“æœ
            print("\n" + "="*60)
            print("ğŸ‰ åˆ†æå®Œæˆ!")
            print("="*60)
            print(result.detailed_report)
            
            # ä¿å­˜æŠ¥å‘Š
            self._save_reports(result)
            
            # ç”Ÿæˆå›¾è¡¨
            self.plot_dashboard(result)
            
            return result
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _analyze_risk(self, temporal_state: Dict, activity_state: Dict, trend_analysis: Dict) -> Dict:
        """é£é™©åˆ†æ"""
        risk_score = 0
        alerts = []
        
        # æ—¶é—´çŠ¶æ€é£é™©
        if temporal_state['state'] == 'DECLINING':
            risk_score += 30
            alerts.append('æ—¶é—´è¶‹åŠ¿æ˜¾ç¤ºè¡°é€€')
        
        # æ´»è·ƒçŠ¶æ€é£é™©
        if activity_state['state'] == 'ZOMBIE':
            risk_score += 40
            alerts.append('é¡¹ç›®å¤„äºåƒµå°¸çŠ¶æ€')
        elif activity_state['state'] == 'DORMANT':
            risk_score += 20
        
        # è¶‹åŠ¿é£é™©
        if trend_analysis['trend_class'] == 'STRONG_DOWN':
            risk_score += 30
            alerts.append('å¼ºçƒˆä¸‹é™è¶‹åŠ¿')
        elif trend_analysis['trend_class'] == 'MODERATE_DOWN':
            risk_score += 15
        
        # é˜»åŠ›é£é™©
        if trend_analysis['resistance']['score'] >= 70:
            risk_score += 25
            alerts.append('æŠ€æœ¯é˜»åŠ›è¾ƒé«˜')
        
        # é£é™©ç­‰çº§
        if risk_score >= 60:
            level = 'CRITICAL'
        elif risk_score >= 40:
            level = 'HIGH'
        elif risk_score >= 20:
            level = 'MEDIUM'
        else:
            level = 'LOW'
        
        return {'score': risk_score, 'level': level, 'alerts': alerts}
    
    def _analyze_dark_horse(self, trend_analysis: Dict, bus_factor: Dict) -> Dict:
        """é»‘é©¬åˆ†æ"""
        if self.structural_tier in ['GIANT', 'MATURE']:
            return {'is_dark_horse': False, 'score': 0, 'reasons': ['å·²è¶…å‡ºé»‘é©¬èŒƒç•´']}
        
        score = 0
        reasons = []
        
        # å¼ºåŠ²åŠ¨é‡
        if trend_analysis['momentum']['score'] >= 70:
            score += 30
            reasons.append('å¼ºåŠ²å¢é•¿åŠ¨é‡')
        
        # é«˜æ½œåŠ›
        if trend_analysis['potential']['score'] >= 60:
            score += 25
            reasons.append('é«˜å¢é•¿æ½œåŠ›')
        
        # ä½é˜»åŠ›
        if trend_analysis['resistance']['score'] <= 30:
            score += 20
            reasons.append('å‘å±•é˜»åŠ›å°')
        
        # å¥åº·çš„è´¡çŒ®è€…ç”Ÿæ€
        if bus_factor.get('risk_level') in ['LOW', 'MEDIUM']:
            score += 15
            reasons.append('è´¡çŒ®è€…ç”Ÿæ€å¥åº·')
        
        return {
            'is_dark_horse': score >= 55,
            'score': min(100, max(0, score)),
            'reasons': reasons
        }
    
    def _detect_change_points(self, series: pd.Series) -> List[Dict]:
        """å˜ç‚¹æ£€æµ‹"""
        if len(series) < 12:
            return []
        
        results = []
        window = 6
        
        for i in range(window, len(series) - window):
            before = series.iloc[i-window:i].mean()
            after = series.iloc[i:i+window].mean()
            change_rate = (after - before) / (before + 0.1)
            
            if abs(change_rate) > 0.3:
                if change_rate > 0.3:
                    cp_type, desc = 'ACCELERATION', 'è¿›å…¥å¿«é€Ÿå¢é•¿æœŸ'
                else:
                    cp_type, desc = 'DECELERATION', 'æ´»è·ƒåº¦æ˜¾è‘—ä¸‹é™'
                
                results.append({
                    'index': i,
                    'date': str(series.index[i])[:7],
                    'type': cp_type,
                    'change_rate': round(change_rate, 3),
                    'description': desc
                })
        
        return results[:3]
    
    def _generate_recommendations(self, structural_tier: str, temporal_state: Dict,
                                  activity_state: Dict, trend_analysis: Dict,
                                  risk_analysis: Dict, bus_factor: Dict,
                                  etd_analysis: Dict) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recs = []
        
        # é£é™©ç›¸å…³å»ºè®®
        if risk_analysis['level'] in ['CRITICAL', 'HIGH']:
            recs.append(f"âš ï¸ é¡¹ç›®é£é™©è¾ƒé«˜({risk_analysis['level']})ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†ï¼š{', '.join(risk_analysis['alerts'][:2])}")
        
        # æ—¶é—´è¶‹åŠ¿å»ºè®®
        if temporal_state['state'] == 'DECLINING':
            recs.append("â¬ æ—¶é—´è¶‹åŠ¿æ˜¾ç¤ºè¡°é€€ï¼Œå»ºè®®åˆ†æåŸå› å¹¶é‡‡å–æ¿€æ´»æªæ–½")
        
        # æ´»è·ƒçŠ¶æ€å»ºè®®
        if activity_state['state'] == 'ZOMBIE':
            recs.append("ğŸ’€ é¡¹ç›®å¤„äºåƒµå°¸çŠ¶æ€ï¼Œå»ºè®®é‡æ–°è¯„ä¼°é¡¹ç›®ä»·å€¼æˆ–è€ƒè™‘å½’æ¡£")
        elif activity_state['state'] == 'DORMANT':
            recs.append("ğŸ˜´ é¡¹ç›®æ´»è·ƒåº¦è¾ƒä½ï¼Œå»ºè®®å¢åŠ ç¤¾åŒºè¿è¥å’ŒæŠ€æœ¯åšå®¢æ›å…‰")
        
        # è¶‹åŠ¿å»ºè®®
        if trend_analysis['trend_class'] in ['STRONG_DOWN', 'MODERATE_DOWN']:
            recs.append("ğŸ“‰ å½“å‰è¶‹åŠ¿ä¸‹è¡Œï¼Œå»ºè®®å…³æ³¨å¹¶é‡‡å–åº”å¯¹æªæ–½")
        
        # é˜»åŠ›å»ºè®®
        if trend_analysis['resistance']['score'] >= 60:
            recs.append("ğŸ›‘ å‘å±•é˜»åŠ›è¾ƒå¤§ï¼Œå»ºè®®ç»„ç»‡ä¸“é¡¹æ¸…ç†æ´»åŠ¨")
        
        # Bus Factorå»ºè®®
        if bus_factor.get('risk_level') in ['CRITICAL', 'HIGH']:
            recs.append("ğŸ‘¥ è´¡çŒ®è€…è¿‡äºé›†ä¸­ï¼Œå»ºè®®åŸ¹å…»æ›´å¤šæ ¸å¿ƒè´¡çŒ®è€…")
        
        # ETDå»ºè®®
        if etd_analysis['etd_status'] in ['CRITICAL', 'WARNING']:
            recs.append(f"â³ {etd_analysis['description']}")
        
        # å±‚çº§ç‰¹å®šå»ºè®®
        if structural_tier == 'EMERGING':
            recs.append("ğŸŒ± æ–°å…´é¡¹ç›®ï¼Œå»ºè®®åŠ å¼ºæ–‡æ¡£å»ºè®¾å’Œç¤¾åŒºå¼•å¯¼")
        elif structural_tier == 'GROWING':
            recs.append("ğŸ“ˆ æˆé•¿å‹é¡¹ç›®ï¼Œå»ºè®®ä¿æŒå½“å‰å‘å±•èŠ‚å¥ï¼Œå…³æ³¨è§„æ¨¡åŒ–æŒ‘æˆ˜")
        elif structural_tier == 'MATURE':
            recs.append("ğŸ¢ æˆç†Ÿé¡¹ç›®ï¼Œå»ºè®®å…³æ³¨æŠ€æœ¯å€ºåŠ¡å’Œå®‰å…¨æ›´æ–°")
        elif structural_tier == 'GIANT':
            recs.append("ğŸ›ï¸ å·¨å‹é¡¹ç›®ï¼Œå»ºè®®å…³æ³¨ç”Ÿæ€æ²»ç†å’Œç¤¾åŒºå¥åº·")
        
        # é»˜è®¤å»ºè®®
        if not recs:
            recs.append("âœ… é¡¹ç›®çŠ¶æ€å¥åº·ï¼Œä¿æŒå½“å‰è¿è¥èŠ‚å¥å³å¯")
        
        return recs[:5]  # æœ€å¤š5æ¡å»ºè®®
    
    def generate_report(self, result: AnalysisResult) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report = f"""
{'â•'*70}
                    {result.project_name} æ·±åº¦è¯Šæ–­æŠ¥å‘Š (v6.0)
                    GitHub é¡¹ç›®åˆ†æå™¨ - æ•°æ®æµé‡æ„ç‰ˆ
{'â•'*70}

ã€ä¸‰å±‚çŠ¶æ€åˆ†ç¦»ã€‘
  ç»“æ„å±‚çº§: {result.structural_tier} ({TIER_NAMES[result.structural_tier]})
  æ—¶é—´è¶‹åŠ¿: {result.temporal_state}
  æ´»è·ƒçŠ¶æ€: {result.activity_state}

{'â”€'*70}

ã€GMMç»“æ„å±‚çº§åˆ†æã€‘
  æœ€ä½³å±‚çº§: {result.structural_tier} (ç½®ä¿¡åº¦: {result.tier_confidence:.0%})
  å±‚çº§æ¦‚ç‡åˆ†å¸ƒ:
"""
        for tier, prob in result.tier_probabilities.items():
            bar_length = int(prob * 30)
            bar = 'â–ˆ' * bar_length + 'â–‘' * (30 - bar_length)
            report += f"    {tier:10s} {bar} {prob:.1%}\n"
        
        report += f"""
{'â”€'*70}

ã€å¥åº·è¯„ä¼°ã€‘
  ç»¼åˆè¯„åˆ†: {result.health_score}/100 ({result.health_grade})
  å„ç»´åº¦è´¡çŒ®:
"""
        for dim, contrib in result.dimension_scores.items():
            report += f"    â€¢ {dim}: {contrib:.1%}\n"
        
        report += f"""
{'â”€'*70}

ã€è¶‹åŠ¿åˆ†æã€‘
  ç»¼åˆè¶‹åŠ¿: {result.trend_analysis['trend_score']}åˆ† ({result.trend_analysis['trend_description']})
  
  â”Œ åŠ¨é‡ (Momentum): {result.trend_analysis['momentum']['score']}/100
  â”‚   {result.trend_analysis['momentum']['description']}
  â”‚
  â”œ é˜»åŠ› (Resistance): {result.trend_analysis['resistance']['score']}/100
  â”‚   {result.trend_analysis['resistance']['description']}
  â”‚
  â”” æ½œåŠ› (Potential): {result.trend_analysis['potential']['score']}/100
      {result.trend_analysis['potential']['description']}

{'â”€'*70}

ã€é£é™©åˆ†æã€‘
  é£é™©ç­‰çº§: {result.risk_analysis['level']} ({result.risk_analysis['score']}åˆ†)
"""
        if result.risk_analysis['alerts']:
            for alert in result.risk_analysis['alerts']:
                report += f"    âš ï¸  {alert}\n"
        
        report += f"""
{'â”€'*70}

ã€è¶‹åŠ¿é¢„æµ‹ã€‘
"""
        if result.trend_predictions.get('openrank'):
            pred = result.trend_predictions['openrank']
            report += f"""
  OpenRank è¶‹åŠ¿é¢„æµ‹:
    å½“å‰å€¼: {pred['current_value']:.2f}
    é¢„æµ‹æ–¹å‘: {pred['direction']} (ç½®ä¿¡åº¦: {pred['direction_confidence']:.0%})
    æœªæ¥6ä¸ªæœˆå¹³å‡: {pred['future_avg']:.2f}
"""
        
        report += f"""
{'â”€'*70}

ã€ä¸“é¡¹åˆ†æã€‘
  Bus Factor 2.0: {result.bus_factor_2.get('effective_bus_factor', 'N/A')}
    {result.bus_factor_2.get('description', '')}
  
  ETDå¯¿å‘½åˆ†æ: {result.etd_analysis['etd_status']}
    {result.etd_analysis.get('description', '')}
  
  é»‘é©¬æ½œåŠ›: {'æ˜¯' if result.dark_horse_analysis.get('is_dark_horse') else 'å¦'} 
    (å¾—åˆ†: {result.dark_horse_analysis.get('score', 0)}/100)

{'â”€'*70}

ã€GitHubæ•°æ®éªŒè¯ã€‘
"""
        if 'error' in result.conclusion_validation:
            report += f"    {result.conclusion_validation['error']}\n"
        else:
            report += f"    æ•´ä½“éªŒè¯: {'é€šè¿‡' if result.conclusion_validation.get('overall_valid') else 'éœ€å¤æ ¸'}\n"
            report += f"    éªŒè¯ç½®ä¿¡åº¦: {result.conclusion_validation.get('confidence', 0)}%\n"
        
        report += f"""
{'â”€'*70}

ã€æ”¹è¿›å»ºè®®ã€‘
"""
        for i, rec in enumerate(result.recommendations, 1):
            report += f"  {i}. {rec}\n"
        
        report += f"""
{'â•'*70}
                         æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'â•'*70}
"""
        return report
    
    def _save_reports(self, result: AnalysisResult):
        """ä¿å­˜æŠ¥å‘Š"""
        try:
            # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
            txt_file = f"{self.org}_{self.repo}_v6_report.txt"
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(result.detailed_report)
            print(f"ğŸ“ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {txt_file}")
            
            # ä¿å­˜JSONæ•°æ®
            json_data = {
                'project': result.project_name,
                'generated_at': datetime.now().isoformat(),
                'structural_tier': result.structural_tier,
                'temporal_state': result.temporal_state,
                'activity_state': result.activity_state,
                'health': {
                    'score': result.health_score,
                    'grade': result.health_grade
                },
                'trend_analysis': result.trend_analysis,
                'risk_analysis': result.risk_analysis,
                'recommendations': result.recommendations
            }
            
            json_file = f"{self.org}_{self.repo}_v6_data.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š JSONæ•°æ®å·²ä¿å­˜: {json_file}")
            
        except Exception as e:
            print(f"ä¿å­˜æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
    
    def plot_dashboard(self, result: AnalysisResult):
        """ç»˜åˆ¶ä»ªè¡¨æ¿"""
        try:
            fig = plt.figure(figsize=(16, 12))
            fig.suptitle(f'{self.org}/{self.repo} é¡¹ç›®åˆ†æä»ªè¡¨æ¿', fontsize=16, fontweight='bold')
            
            # åˆ›å»ºå­å›¾å¸ƒå±€
            gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
            
            # 1. ä¸‰å±‚çŠ¶æ€å›¾
            ax1 = fig.add_subplot(gs[0, 0])
            self._plot_three_layer_status(ax1, result)
            
            # 2. å¥åº·è¯„åˆ†ä»ªè¡¨ç›˜
            ax2 = fig.add_subplot(gs[0, 1])
            self._plot_health_gauge(ax2, result)
            
            # 3. GMMæ¦‚ç‡åˆ†å¸ƒ
            ax3 = fig.add_subplot(gs[0, 2])
            self._plot_gmm_probabilities(ax3, result)
            
            # 4. è¶‹åŠ¿åˆ†æé›·è¾¾å›¾
            ax4 = fig.add_subplot(gs[1, 0], polar=True)
            self._plot_trend_radar(ax4, result)
            
            # 5. é£é™©ç­‰çº§
            ax5 = fig.add_subplot(gs[1, 1])
            self._plot_risk_gauge(ax5, result)
            
            # 6. æ—¶é—´åºåˆ—è¶‹åŠ¿
            ax6 = fig.add_subplot(gs[1, 2])
            self._plot_time_series(ax6, result)
            
            # 7. Bus Factoråˆ†æ
            ax7 = fig.add_subplot(gs[2, 0])
            self._plot_bus_factor(ax7, result)
            
            # 8. æ½œåŠ›åˆ†æ
            ax8 = fig.add_subplot(gs[2, 1])
            self._plot_potential(ax8, result)
            
            # 9. å»ºè®®å…³é”®è¯
            ax9 = fig.add_subplot(gs[2, 2])
            self._plot_recommendations(ax9, result)
            
            plt.tight_layout()
            plt.savefig(f"{self.org}_{self.repo}_v6_dashboard.png", dpi=150, bbox_inches='tight')
            print(f"ğŸ“ˆ ä»ªè¡¨æ¿å›¾è¡¨å·²ä¿å­˜: {self.org}_{self.repo}_v6_dashboard.png")
            plt.show()
            
        except Exception as e:
            print(f"ç»˜å›¾å¤±è´¥: {e}")
    
    def _plot_three_layer_status(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶ä¸‰å±‚çŠ¶æ€å›¾"""
        layers = ['ç»“æ„å±‚çº§', 'æ—¶é—´è¶‹åŠ¿', 'æ´»è·ƒçŠ¶æ€']
        statuses = [result.structural_tier, result.temporal_state, result.activity_state]
        colors = [COLORS['primary'], COLORS['warning'], COLORS['success']]
        
        y_pos = np.arange(len(layers))
        ax.barh(y_pos, [1, 1, 1], color='lightgray', alpha=0.3)
        bars = ax.barh(y_pos, [0.8, 0.8, 0.8], color=colors, alpha=0.8)
        
        ax.set_yticks(y_pos)
        ax.set_yticklabels(layers, fontsize=10)
        ax.set_xlim(0, 1)
        ax.set_xticks([])
        
        for i, (bar, status) in enumerate(zip(bars, statuses)):
            ax.text(0.4, i, status, va='center', ha='center', fontsize=9, fontweight='bold', color='white')
        
        ax.set_title('ä¸‰å±‚çŠ¶æ€åˆ†ç¦»', fontsize=11, fontweight='bold', pad=10)
        ax.grid(False)
    
    def _plot_health_gauge(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶å¥åº·è¯„åˆ†ä»ªè¡¨ç›˜"""
        score = result.health_score
        
        # æ¸…ç©ºåæ ‡è½´
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        
        # ç»˜åˆ¶ä»ªè¡¨ç›˜
        center_x, center_y = 0.5, 0.5
        radius = 0.4
        
        # å±é™©åŒºåŸŸ
        danger_angle = np.linspace(np.pi, np.pi + np.pi * 0.35, 50)
        x_danger = center_x + radius * np.cos(danger_angle)
        y_danger = center_y + radius * np.sin(danger_angle)
        ax.fill_between(x_danger, center_y, y_danger, color=COLORS['danger'], alpha=0.3)
        
        # è­¦å‘ŠåŒºåŸŸ
        warning_angle = np.linspace(np.pi + np.pi * 0.35, np.pi + np.pi * 0.65, 50)
        x_warning = center_x + radius * np.cos(warning_angle)
        y_warning = center_y + radius * np.sin(warning_angle)
        ax.fill_between(x_warning, center_y, y_warning, color=COLORS['warning'], alpha=0.3)
        
        # å®‰å…¨åŒºåŸŸ
        safe_angle = np.linspace(np.pi + np.pi * 0.65, 2*np.pi, 50)
        x_safe = center_x + radius * np.cos(safe_angle)
        y_safe = center_y + radius * np.sin(safe_angle)
        ax.fill_between(x_safe, center_y, y_safe, color=COLORS['success'], alpha=0.3)
        
        # æŒ‡é’ˆ
        angle = np.pi + np.pi * (score / 100)
        x_tip = center_x + radius * 0.8 * np.cos(angle)
        y_tip = center_y + radius * 0.8 * np.sin(angle)
        ax.plot([center_x, x_tip], [center_y, y_tip], 'k-', lw=2)
        
        # ä¸­å¿ƒç‚¹
        ax.add_patch(plt.Circle((center_x, center_y), 0.02, color='black'))
        
        # åˆ†æ•°å’Œç­‰çº§
        ax.text(center_x, center_y - 0.1, f'{score:.0f}/100', 
                ha='center', va='center', fontsize=14, fontweight='bold')
        ax.text(center_x, center_y - 0.2, result.health_grade, 
                ha='center', va='center', fontsize=12, fontweight='bold')
        
        ax.axis('off')
        ax.set_title('å¥åº·è¯„åˆ†', fontsize=11, fontweight='bold', pad=10)
    
    def _plot_gmm_probabilities(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶GMMæ¦‚ç‡åˆ†å¸ƒ"""
        tiers = list(result.tier_probabilities.keys())
        probs = list(result.tier_probabilities.values())
        colors = [TIER_BENCHMARKS[t].get('color', COLORS['primary']) for t in tiers]
        
        bars = ax.bar(tiers, probs, color=colors, alpha=0.8)
        ax.set_ylim(0, 1)
        
        for bar, prob in zip(bars, probs):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                   f'{prob:.1%}', ha='center', va='bottom', fontsize=8)
        
        ax.set_ylabel('æ¦‚ç‡', fontsize=9)
        ax.set_title(f'GMMå±‚çº§æ¦‚ç‡ (æœ€ä½³: {result.structural_tier})', fontsize=11, fontweight='bold')
        ax.tick_params(axis='x', rotation=45)
    
    def _plot_trend_radar(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶è¶‹åŠ¿é›·è¾¾å›¾"""
        categories = ['åŠ¨é‡', 'é˜»åŠ›', 'æ½œåŠ›']
        values = [
            result.trend_analysis['momentum']['score'],
            100 - result.trend_analysis['resistance']['score'],
            result.trend_analysis['potential']['score']
        ]
        values = values + [values[0]]
        
        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
        angles += [angles[0]]
        
        ax.set_theta_offset(np.pi / 2)
        ax.set_theta_direction(-1)
        
        # ç»˜åˆ¶ç½‘æ ¼
        for i in [20, 40, 60, 80, 100]:
            ax.plot(angles, [i] * len(angles), color='gray', linestyle='-', linewidth=0.5, alpha=0.3)
        
        # ç»˜åˆ¶æ•°æ®
        ax.plot(angles, values, 'o-', color=COLORS['primary'], linewidth=2)
        ax.fill(angles, values, color=COLORS['primary'], alpha=0.2)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=10)
        ax.set_ylim(0, 100)
        ax.grid(True)
        
        ax.set_title('è¶‹åŠ¿ä¸‰ç»´åˆ†æ', fontsize=11, fontweight='bold', pad=20)
    
    def _plot_risk_gauge(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶é£é™©ç­‰çº§"""
        risk_levels = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']
        risk_scores = [80, 60, 40, 20]
        colors = [COLORS['danger'], COLORS['warning'], COLORS['info'], COLORS['success']]
        
        current_risk = result.risk_analysis['level']
        current_idx = risk_levels.index(current_risk) if current_risk in risk_levels else 0
        
        # ç»˜åˆ¶æ‰€æœ‰é£é™©ç­‰çº§
        for i, (level, score, color) in enumerate(zip(risk_levels, risk_scores, colors)):
            alpha = 0.8 if i == current_idx else 0.3
            ax.barh(level, score, color=color, alpha=alpha)
            ax.text(score + 2, i, f'{score}åˆ†', va='center', fontsize=9)
        
        ax.set_xlim(0, 100)
        ax.set_title(f'é£é™©ç­‰çº§: {current_risk}', fontsize=11, fontweight='bold')
        ax.grid(True, axis='x', alpha=0.3)
    
    def _plot_time_series(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶æ—¶é—´åºåˆ—"""
        if 'openrank' in self.od_df.columns:
            data = self.od_df['openrank']
            ax.plot(data.index, data.values, color=COLORS['primary'], lw=2)
            ax.fill_between(data.index, 0, data.values, color=COLORS['primary'], alpha=0.2)
            
            # æ ‡è®°å˜ç‚¹
            for cp in result.change_points[:2]:
                idx = cp['index']
                if idx < len(data):
                    ax.axvline(x=data.index[idx], color=COLORS['warning'], linestyle='--', alpha=0.7)
                    ax.text(data.index[idx], data.max() * 0.9, cp['type'][:3], 
                           rotation=90, fontsize=8, ha='right')
            
            ax.set_xlabel('æ—¶é—´', fontsize=9)
            ax.set_ylabel('OpenRank', fontsize=9)
            ax.tick_params(axis='x', rotation=30)
            ax.grid(True, alpha=0.3)
            ax.set_title('OpenRankæ—¶é—´åºåˆ—', fontsize=11, fontweight='bold')
    
    def _plot_bus_factor(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶Bus Factoråˆ†æ"""
        bf = result.bus_factor_2.get('effective_bus_factor', 1)
        risk = result.bus_factor_2.get('risk_level', 'UNKNOWN')
        
        colors = {'CRITICAL': COLORS['danger'], 'HIGH': COLORS['warning'], 
                 'MEDIUM': COLORS['info'], 'LOW': COLORS['success'], 'UNKNOWN': 'gray'}
        
        ax.barh(['Bus Factor'], [min(10, bf)], color=colors.get(risk, 'gray'), alpha=0.8)
        ax.set_xlim(0, 10)
        ax.text(min(10, bf) + 0.2, 0, f'{bf:.1f}', va='center', fontsize=10, fontweight='bold')
        
        ax.set_title(f'Bus Factor: {risk}', fontsize=11, fontweight='bold')
        ax.grid(True, axis='x', alpha=0.3)
    
    def _plot_potential(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶æ½œåŠ›åˆ†æ"""
        current = result.trend_analysis['potential']['current_openrank']
        tier_benchmark = result.trend_analysis['potential']['tier_benchmark']
        next_tier = 'N/A'
        
        # è®¡ç®—ä¸ä¸‹ä¸€å±‚çº§çš„å·®è·
        tiers = ['EMERGING', 'GROWING', 'MATURE', 'GIANT']
        if result.structural_tier in tiers:
            idx = tiers.index(result.structural_tier)
            if idx < len(tiers) - 1:
                next_tier = tiers[idx + 1]
        
        categories = ['å½“å‰', 'å½“å‰å±‚çº§åŸºå‡†', 'ä¸‹ä¸€å±‚çº§ç›®æ ‡']
        values = [current, tier_benchmark, TIER_BENCHMARKS.get(next_tier, {}).get('openrank', current * 1.5)]
        
        colors = [COLORS['primary'], COLORS['warning'], COLORS['success']]
        bars = ax.bar(categories, values, color=colors, alpha=0.8)
        
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + max(values)*0.02,
                   f'{value:.1f}', ha='center', va='bottom', fontsize=9)
        
        ax.set_ylabel('OpenRank', fontsize=9)
        ax.set_title('å¢é•¿æ½œåŠ›åˆ†æ', fontsize=11, fontweight='bold')
        ax.grid(True, axis='y', alpha=0.3)
    
    def _plot_recommendations(self, ax, result: AnalysisResult):
        """ç»˜åˆ¶å»ºè®®å…³é”®è¯"""
        if not result.recommendations:
            ax.text(0.5, 0.5, 'æ— å»ºè®®', ha='center', va='center')
            return
        
        # æå–å…³é”®è¯
        keywords = []
        for rec in result.recommendations:
            # ç®€å•çš„å…³é”®è¯æå–
            if 'é£é™©' in rec:
                keywords.append('é£é™©ç®¡ç†')
            if 'å¢é•¿' in rec or 'å‘å±•' in rec:
                keywords.append('å¢é•¿ç­–ç•¥')
            if 'ç¤¾åŒº' in rec:
                keywords.append('ç¤¾åŒºå»ºè®¾')
            if 'è´¡çŒ®è€…' in rec:
                keywords.append('è´¡çŒ®è€…åŸ¹å…»')
            if 'æŠ€æœ¯' in rec:
                keywords.append('æŠ€æœ¯ä¼˜åŒ–')
            if 'å®‰å…¨' in rec:
                keywords.append('å®‰å…¨ç»´æŠ¤')
        
        # å»é‡
        keywords = list(set(keywords))[:5]
        
        if not keywords:
            keywords = ['ä¿æŒç¨³å®š', 'æŒç»­è§‚å¯Ÿ']
        
        # ç»˜åˆ¶è¯äº‘æ ·å¼
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        
        n = len(keywords)
        for i, keyword in enumerate(keywords):
            size = 12 - i * 2
            x = 0.1 + (i % 3) * 0.3
            y = 0.8 - (i // 3) * 0.4
            ax.text(x, y, keyword, fontsize=size, fontweight='bold', 
                   ha='center', va='center', alpha=0.8)
        
        ax.axis('off')
        ax.set_title('å»ºè®®å…³é”®è¯', fontsize=11, fontweight='bold', pad=10)


# ============== ä¸»å…¥å£ ==============
if __name__ == "__main__":
    print("\n" + "="*60)
    print("  GitHub é¡¹ç›®æ·±åº¦åˆ†æå™¨ v6.0 - æ•°æ®æµé‡æ„ç‰ˆ")
    print("  OpenDigger(è¶‹åŠ¿) + GitHub(é”šå®š) + ä¸‰å±‚çŠ¶æ€åˆ†ç¦»")
    print("="*60 + "\n")
    
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–URL
    import sys
    if len(sys.argv) > 1:
        url = sys.argv[1]
        print(f"ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°: {url}")
    else:
        url = input("è¯·è¾“å…¥ GitHub é¡¹ç›®åœ°å€ (ä¾‹å¦‚: facebook/react): ").strip()
    
    if not url:
        print("ä½¿ç”¨é»˜è®¤é¡¹ç›®: facebook/react")
        url = "facebook/react"
    
    # è·å–GitHub Token
    token = os.getenv('GITHUB_TOKEN')
    if not token and len(sys.argv) > 2:
        token = sys.argv[2]
    
    if not token:
        use_token = input("æ˜¯å¦ä½¿ç”¨ GitHub Token? (y/n, æ¨èä½¿ç”¨): ").strip().lower()
        if use_token == 'y':
            token = input("è¯·è¾“å…¥ GitHub API Token: ").strip()
    
    analyzer = ProjectAnalyzerV6(url, github_token=token)
    result = analyzer.analyze()
    
    if result:
        print("\n" + "="*60)
        print("âœ… åˆ†æå®Œæˆï¼æ€»ç»“:")
        print(f"   ç»“æ„å±‚çº§: {result.structural_tier} ({TIER_NAMES[result.structural_tier]})")
        print(f"   æ—¶é—´è¶‹åŠ¿: {result.temporal_state}")
        print(f"   æ´»è·ƒçŠ¶æ€: {result.activity_state}")
        print(f"   å¥åº·è¯„åˆ†: {result.health_score}/100 ({result.health_grade})")
        print(f"   é£é™©ç­‰çº§: {result.risk_analysis['level']}")
        print("="*60)
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥å’Œç½‘ç»œè¿æ¥")